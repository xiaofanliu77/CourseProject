[SOUND]This lecture is the first oneabout the text clustering. In this lecture, we are going totalk about the text clustering. This is a very important technique fordoing topic mining and analysis. In particular, in this lecture we're going to start withsome basic questions about the clustering. And that is, what is text clustering andwhy we are interested in text clustering. In the following lectures, we are goingto talk about how to do text clustering. How to evaluate the clustering results? So what is text clustering? Well, clustering actually isa very general technique for data mining as you might havelearned in some other courses. The idea is to discover naturalstructures in the data. In another words,we want to group similar objects together. In our case, these objects are of course,text objects. For example, they can be documents,terms, passages, sentences, or websites, and then I'llgo group similar text objects together. So let's see an example, well, hereyou don't really see text objects, but I just used some shapes to denoteobjects that can be grouped together. Now if I ask you, what are some naturalstructures or natural groups where you, if you look at it and you might agree thatwe can group these objects based on chips, or their locations on thistwo dimensional space. So we got the three clusters in this case. And they may not be somuch this agreement about these three clusters but it really dependson the perspective to look at the objects. Maybe some of you have also seenthing in a different way, so we might get different clusters. And you'll see another exampleabout this ambiguity more clearly. But the main point of here is, the problemis actually not so well defined. And the problem lies inhow to define similarity. And what do you mean by similar objects? Now this problem has to be clearly defined in order to havea well defined clustering problem. And the problem is in general that any two objects can be similardepending on how you look at them. So for example, this will keptthe two words like car and horse. So are the two words similar? Well, it depends on how ifwe look at the physical properties of car andhorse they are very different but if you look at them functionally,a car and a horse, can both be transportation tools. So in that sense, they may be similar. So as we can see, it really depends onour perspective, to look at the objects. And so it ought to makethe clustering problem well defined. A user must define the perspective forassessing similarity. And we call this perspectivethe clustering bias. And when you define a clustering problem,it's important to specify your perspective forsimilarity or for defining the similarity that will beused to group similar objects. because otherwise,the similarity is not well defined and one can have differentways to group objects. So let's look at the example here. You are seeing some objects,or some shapes, that are very similar to what youhave seen on the first slide, but if I ask you to group these objects,again, you might feel there is more than herethan on the previous slide. For example, you might think, well, we can steer a group by ships, so that wouldgive us cluster that looks like this. However, you might also feel that, well, maybe the objects can begrouped based on their sizes. So that would give us a different wayto cluster the data if we look at the size andlook at the similarity in size. So as you can see clearly here,depending on the perspective, we'll get different clustering result. So that also clearly tells us that inorder to evaluate the clustering without, we must use perspective. Without perspective, it's very hard todefine what is the best clustering result. So there are many examplesof text clustering setup. And so for example, we can clusterdocuments in the whole text collection. So in this case,documents are the units to be clustered. We may be able to cluster terms. In this case, terms are objects. And a cluster of terms can be used todefine concept, or theme, or a topic. In fact, there's a topic models that youhave seen some previous lectures can give you cluster of terms in somesense if you take terms with high probabilities from word distribution. Another example is just to cluster anytext segments, for example, passages, sentences, or any segments that you canextract the former larger text objects. For example, we might extract the ordertext segments about the topic, let's say, by using a topic model. Now once we've got thosetext objects then we can cluster the segments that we've got to discover interesting clusters thatmight also ripple in the subtopics. So this is a case of combining textclustering with some other techniques. And in general you willsee a lot of text mining can be accurate combined ina flexible way to achieve the goal of doing more sophisticatedmining and analysis of text data. We can also cluster fairlylarge text objects and by that, I just mean text objects maycontain a lot of documents. So for example, we might cluster websites. Each website is actuallycompose of multiple documents. Similarly, we can also cluster articleswritten by the same author, for example. So we can trigger all the articlespublished by also as one unit for clustering. In this way, we might group authorstogether based on whether they're published papers or similar. For the more text clusters will be forthe cluster to generate a hierarchy. That's because we can in general clusterany text object at different levels. So more generally why istext clustering interesting? Well, it's because it's a veryuseful technique for text mining, particularly exploratory text analysis. And so a typical scenario is thatyou were getting a lot of text data, let's say all the email messagesfrom customers in some time period, all the literature articles, etc. And then you hope to get a senseabout what are the overall content of the connection, so for example,you might be interested in getting a sense about major topics,or what are some typical or representative documentsin the connection. And clustering to helpus achieve this goal. We sometimes also want to linka similar text objects together. And these objects might beduplicated content, for example. And in that case, such a technique can help us removeredundancy and remove duplicate documents. Sometimes they are aboutthe same topic and by linking them together we can havemore complete coverage of a topic. We may also used text clustering to createa structure on the text data and sometimes we can create a hierarchy of structuresand this is very useful for problems. We may also use text clustering to induceadditional features to represent text data when we cluster documents together,we can treat each cluster as a feature. And then we can say whena document is in this cluster and then the feature value would be one. And if a document is not in this cluster,then the feature value is zero. And this helps provide additionaldiscrimination that might be used for text classification aswe will discuss later. So there are, in general,many applications of text clustering. And I just thought oftwo very specific ones. One is to cluster search results, for example, [INAUDIBLE] search enginecan cluster such results so that the user can see overall structureof the results of return the fall query. And when the query's ambiguous thisis particularly useful because the clusters likely representdifferent senses of ambiguous word. Another application is to understand themajor complaints from customers based on their emails, right. So in this case,we can cluster email messages and then find in the majorclusters from there, we can understand what are the majorcomplaints about them. [MUSIC]
[SOUND] This lecture is about the Evaluation of Text Categorization. So we've talked about many differentmethods for text categorization. But how do you know whichmethod works better? And for a particular application, how do you know this is the bestway of solving your problem? To understand these, we have to how to we have to know how toevaluate categorization results. So first some general thoughtsabout the evaluation. In general, for evaluation of this kind ofempirical tasks such as categorization, we use methodology thatwas developed in 1960s by information retrieval researchers. Called a Cranfield Evaluation Methodology. The basic idea is to havehumans create test correction, where, we already know, every documentis tagged with the desired categories. Or, in the case of search, for whichquery, which documents that should have been retrieved, andthis is called, a ground truth. Now, with this groundtruth test correction, we can then reuse the collection totest the many different systems and then compare different systems. We can also turn off some components inthe system to see what's going to happen. Basically it provides a way to do controlexperiments to compare different methods. So this methodology hasbeen virtually used for all the tasks that involveempirically defined problems. So in our case, then, we are going tocompare our systems categorization results with the categorization,ground truth, created by humans. And we're going to compareour systems decisions, which documents should getwhich category with what categories have been assignedto those documents by humans. And we want to quantifythe similarity of these decisions or equivalently, to measure the differencebetween the system output and the desired ideal outputgenerated by the humans. So obviously, the highest similarityis the better results are. The similarity could bemeasured in different ways. And that would lead to different measures. And sometimes it's desirable also to matchthe similarity from different perspectives just to have a better understandingof the results in detail. For example, we might be also interestedin knowing which category performs better and which which categoryis easy to categorize, etc. In general,different categorization mistakes however, have different costs forspecific applications. So some areas might bemore serious than others. So ideally, we would like to modelsuch differences, but if you read many papers in categorization you willsee that they don't generally do that. Instead, they will use a simplifiedmeasure and that's because it's often okay not to consider such a costvariation when we compare methods and when we are interested in knowingthe relative difference of these methods. So it's okay to introduce some bias,as long as the bias is not already with a particular method and then we shouldexpect the more effective method to perform better than a less effective one,even though the measure is not perfect. So the first measure that we'll introduceis called classification accuracy and this is a basic into measurethe percentage of correct decisions. So here you see that thereare categories denoted by c1 through ck and there are n documents,denoted by d1 through d N. And for each pair of category and the document,we can then look at the situation. And see if the system hassaid yes to this pair, basically has assigned thiscategory to this document. Or no, so this is denoted by Y or M,that's the systems of the decision. And similarly, we can look at the human'sdecisions also, if the human has assigned a category to the document of thatthere will be a plus sign here. That just means that a human. We think of this assignment is correct andincorrect then it's a minus. So we'll see all combinations of this Ns,yes and nos, minus and pluses. There are four combinations in total. And two of them are correct, andthat's when we have y(+) or n(-), and then there are alsotwo kinds of errors. So the measure of classificationaccuracy is simply to count how many of these decisions are correct. And normalize that by the totalnumber of decisions we have made. So, we know that the total numberof decisions is n, multiplied by k. And, the number of correct decisionsare basically of two kinds. One is y plusses. And the other is n minus this n. We just put together the count. Now, this is a very convenientmeasure that will give us one number to characterize performance of a method. And the higher, the better, of course. But the method also has some problems. First it has treated allthe decisions equally. But in reality, some decision errorsare more serious than others. For example, it may be more important toget the decisions right on some documents, than others. Or maybe, more important to getthe decisions right on some categories, than others, and this would call for some detailed evaluation of thisresults to understand the strands and of different methods, and to understandthe performance of these methods. In detail in a per category or per document basis. One example that shows clearlythe decision errors are having different causes is spam filtering that could beretrieved as two category categorization problem. Missing a legitimate email result,is one type of error. But letting spam to come into yourfolder is another type of error. The two types of errorsare clearly very different, because it's very important notto miss a legitimate email. It's okay to occasionally let a spamemail to come into your inbox. So the error of the first, missing alegitimate email is very, is of high cost. It's a very serious mistake and classification error, classificationaccuracy does not address this issue. There's also another problemwith imbalance to test set. Imagine there's a skew to test set wheremost instances are category one and 98% of instances are category one. Only 2% are in category two. In such a case, we can have a verysimple baseline that accurately performs very well and that baseline. Sign with similar,I put all instances in the major category. That will get us 98%accuracy in this case. It's going to be appearing to bevery effective, but in reality, this is obviously not a good result. And so, in general, when we useclassification accuracy as a measure, we want to ensure thatthe causes of balance. And one above equal number of instances,for example in each class the minoritycategories or causes tend to be overlooked in the evaluationof classification accuracy. So, to address these problems,we of course would like to also evaluate the results in other ways andin different ways. As I said, it's beneficial to lookat after multiple perspectives. So for example, we can look atthe perspective from each document as a perspective based on each document. So the question here is, how goodare the decisions on this document? Now, as in the general cases of alldecisions, we can think about four combinations of possibilities, dependingon whether the system has said yes and depending on whether the human has said itcorrect or incorrect or said yes or no. And so the four combinations are firstwhen both the human systems said yes, and that's the true positives, when the systemsays, yes, and it's after the positive. So, when the system says,yes, it's a positive. But, when the human confirmthat it is indeed correct, that becomes a true positive. When the system says, yes,but the human says, no, that's incorrect,that's a false positive, have FP. And when the system says no, but the humansays yes, then it's a false negative. We missed one assignment. When both the system and human says no, then it's also correctlyto assume that's true negatives. All right, sothen we can have some measures to just better characterize the performanceby using these four numbers and so two popular measures are precision andrecall. And these were also proposed byinformation retrieval researchers 1960s for evaluating search results, but now they have become standard measures,use it everywhere. So when the system says yes, we can askthe question, how many are correct? What's the percent of correctdecisions when the system says yes? That's called precision. It's true positive divided by allthe cases when the system says yes, all the positives. The other measure is called recall,and this measures whether the document has allthe categories it should have. So in this case it's divide the truepositive by true positives and the false negatives. So these are all the cases where this human Says the documentshould have this category. So this represents both categoriesthat it should have got, and so recall tells us whetherthe system has actually indeed assigned all the categories thatit should have to this document. This gives us a detailedview of the document, then we can aggregate them later. And if we're interested in some documents,and this will tell us how well we did onthose documents, the subsets of them. It might be more interesting than others,for example. And this allows us to analyzeerrors in more detail as well. We can separate the documents of certaincharacteristics from others, and then look at the errors. You might see a pattern A forthis kind of document, this long document. It doesn't as well for shock documents. And this gives you some insight forinputting the method. Similarly, we can look atthe per-category evaluation. In this case, we're going to look at the how good arethe decisions on a particular category. As in the previous case we candefine precision and recall. And it would just basically answer thequestions from a different perspective. So when the system says yes,how many are correct? That means looking at this categoryto see if all the documents that are assigned with this categoryare indeed in this category, right? And recall, would tell us,has the category been actually assigned to all the documents Thatshould have this category. It's sometimes also useful to combineprecision and recall as one measure, and this is often done by using f measure. And this is just a harmonicmean of precision. Precision andrecall defined on this slide. And it's also controlledby a parameter beta to indicate whether precision ismore important or recall is more. When beta is set to 1,we have measure called F1, and in this case, we just take equalweight upon both procedure and recall. F1 is very often used as a measure forcategorization. Now, as in all cases, when we combineresults, you always should think about the best way of combining them, so in thiscase I don't know if you have thought about it and we could have combinedthem just with arithmetic mean, right. So that would still give usthe same range of values, but obviously there's a reason why we didn'tdo that and why f1 is more popular, and it's actually usefulto think about difference. And we think about that, you'll seethat there is indeed some difference and some undesirable propertyof this arithmatic. Basically, it will be obviousto you if you think about a case when the system says yes forall the category and document pairs. And then try the compute the precision andrecall in that case. And see what would happen. And basically, this kind of measure, the arithmetic mean, is not going to be as reasonable as F1 minus one[INAUDIBLE] trade off, so that the two values are equal. There is an extreme case where you have0 for one letter and one for the other. Then F1 will be low, butthe mean would still be reasonably high. [MUSIC]
[MUSIC] This lecture is about the Latent AspectRating Analysis for Opinion Mining and Sentiment Analysis. In this lecture, we're going to continue discussingOpinion Mining and Sentiment Analysis. In particular, we're going to introduceLatent Aspect Rating Analysis which allows us to perform detailedanalysis of reviews with overall ratings. So, first is motivation. Here are two reviews that you oftensee in the net about the hotel. And you see some overall ratings. In this case,both reviewers have given five stars. And, of course,there are also reviews that are in text. Now, if you just look at these reviews, it's not very clear whether the hotel isgood for its location or for its service. It's also unclear whya reviewer liked this hotel. What we want to do is todecompose this overall rating into ratings on different aspects such asvalue, rooms, location, and service. So, if we can decomposethe overall ratings, the ratings on these different aspects,then, we can obtain a more detailed understandingof the reviewer's opinionsabout the hotel. And this would also allow us to rankhotels along different dimensions such as value or rooms. But, in general, such detailedunderstanding will reveal more information about the user's preferences,reviewer's preferences. And also, we can understand betterhow the reviewers view this hotel from different perspectives. Now, not only do we want toinfer these aspect ratings, we also want to infer the aspect weights. So, some reviewers may care more aboutvalues as opposed to the service. And that would be a case. like what's shown on the left forthe weight distribution, where you can see a lot ofweight is places on value. But others care more for service. And therefore, they might placemore weight on service than value. The reason why this isalso important is because, do you think about a five star on value, it might still be very expensive if thereviewer cares a lot about service, right? For this kind of service,this price is good, so the reviewer might give it a five star. But if a reviewer really caresabout the value of the hotel, then the five star, most likely,would mean really cheap prices. So, in order to interpret the ratingson different aspects accurately, we also need to know these aspect weights. When they're combined together, we can have a more detailedunderstanding of the opinion. So the task here is to get these reviewsand their overall ratings as input, and then,generate both the aspect ratings, the compose aspect ratings, andthe aspect rates as output. And this is a problem calledLatent Aspect Rating Analysis. So the task, in general,is given a set of review articles about the topic with overall ratings, andwe hope to generate three things. One is the major aspectscommented on in the reviews. Second is ratings on each aspect,such as value and room service. And third is the relative weights placedon different aspects by the reviewers. And this task has a lot of applications,and if you can do this, and it will enable a lot of applications. I just listed some here. And later, I will show you some results. And, for example,we can do opinion based entity ranking. We can generate an aspect-levelopinion summary. We can also analyze reviewers preferences,compare them or compare their preferenceson different hotels. And we can do personalizedrecommendations of products. So, of course, the question ishow can we solve this problem? Now, as in other cases ofthese advanced topics, we wonâ€™t have time to reallycover the technique in detail. But Iâ€™m going to give you a brisk, basic introduction to the techniquedevelopment for this problem. So, first step, weâ€™re going to talk abouthow to solve the problem in two stages. Later, weâ€™re going to also mention thatwe can do this in the unified model. Now, take this review withthe overall rating as input. What we want to do is, first,we're going to segment the aspects. So we're going to pick out what wordsare talking about location, and what words are talkingabout room condition, etc. So with this, we would be ableto obtain aspect segments. In particular, we're going toobtain the counts of all the words in each segment, andthis is denoted by C sub I of W and D. Now this can be done by using seedwords like location and room or price to retrievethe [INAUDIBLE] in the segments. And then, from those segments,we can further mine correlated words with these seed words andthat would allow us to segmented the text into segments,discussing different aspects. But, of course, later, as we will see, we can also use[INAUDIBLE] models to do the segmentation. But anyway, that's the first stage, where the obtain the councilof words in each segment. In the second stage, which is called Latent Rating Regression,we're going to use these words and their frequencies in differentaspects to predict the overall rate. And this predicting happens in two stages. In the first stage,we're going to use the [INAUDIBLE] and the weights of these words in eachaspect to predict the aspect rating. So, for example, if in your discussionof location, you see a word like, amazing, mentioned many times,and it has a high weight. For example, here, 3.9. Then, it will increasethe Aspect Rating for location. But, another word like, far,which is an acted weight, if it's mentioned many times,and it will decrease the rating. So the aspect ratings, assume that itwill be a weighted combination of these word frequencies where the weightsare the sentiment weights of the words. Of course, these sentimental weightsmight be different for different aspects. So we have, for each aspect, a set ofterm sentiment weights as shown here. And that's in order by beta sub I and W. In the second stage or second step,we're going to assume that the overall rating is simply a weightedcombination of these aspect ratings. So we're going to assume we have aspectweights to the [INAUDIBLE] sub i of d, and this will be used to take a weightedaverage of the aspect ratings, which are denoted by r sub i of d. And we're going to assume the overallrating is simply a weighted average of these aspect ratings. So this set up allows us to predictthe overall rating based on the observable frequencies. So on the left side, you will see all these observedinformation, the r sub d and the count. But on the right side, you see all the information inthat range is actually latent. So, we hope to discover that. Now, this is a typical case ofa generating model where would embed the interesting variablesin the generated model. And then, we're going to set upa generation probability for the overall rating giventhe observed words. And then, of course, we can adjust theseparameter values including betas Rs and alpha Is in order to maximizethe probability of the data. In this case, the conditional probabilityof the observed rating given the document. So we have seen such cases before in, for example, PISA,where we predict a text data. But here, we're predicting the rating,and the parameters, of course, are very different. But we can see, if we can uncoverthese parameters, it would be nice, because r sub i of d is precise asthe ratings that we want to get. And these are the composerratings on different aspects. [INAUDIBLE] sub I D is preciselythe aspect weights that we hope to get as a byproduct,that we also get the beta factor, and these are the [INAUDIBLE] factor,the sentiment weights of words. So more formally, the data we are modeling here is a set ofreview documents with overall ratings. And each review document denote by a d,and the overall ratings denote by r sub d. And d pre-segments turninto k aspect segments. And we're going to use ci(w,d) to denotethe count of word w in aspect segment i. Of course, it's zero if the worddoesn't occur in the segment. Now, the model is going topredict the rating based on d. So, we're interested in the provisionalproblem of r sub-d given d. And this model is set up as follows. So r sub-d is assumed the twofollow a normal distribution doesn't mean that denotesactually await the average of the aspect of ratings rSub I of d as shown here. This normal distribution isa variance of data squared. Now, of course,this is just our assumption. The actual rating is not necessarilyanything thing this way. But as always, when we make thisassumption, we have a formal way to model the problem and that allows usto compute the interest in quantities. In this case, the aspect ratings andthe aspect weights. Now, the aspect rating asyou see on the [INAUDIBLE] is assuming that will bea weight of sum of these weights. Where the weight is justthe [INAUDIBLE] of the weight. So as I said, the overall rating is assumed to bea weighted average of aspect ratings. Now, these other values, r forsub I of D, or denoted together by other vector that depends on D isthat the token of specific weights. And weâ€™re going to assume thatthis vector itself is drawn from another Multivariate Gaussiandistribution, with mean denoted by a Mu factor,and covariance metrics sigma here. Now, so this means, when we generate ouroverall rating, we're going to first draw a set of other values from thisMultivariate Gaussian Prior distribution. And once we get these other values,we're going to use then the weighted average of aspect ratings asthe mean here to use the normal distribution to generatethe overall rating. Now, the aspect rating, as I just said,is the sum of the sentiment weights of words in aspect, note that here thesentiment weights are specific to aspect. So, beta is indexed by i,and that's for aspect. And that gives us a way to modeldifferent segment of a word. This is neither becausethe same word might have positive sentiment for another aspect. It's also used for see what parameterswe have here beta sub i and w gives us the aspect-specificsentiment of w. So, obviously,that's one of the important parameters. But, in general, we can see we have theseparameters, beta values, the delta, and the Mu, and sigma. So, next, the question is, how canwe estimate these parameters and, so we collectively denote allthe parameters by lambda here. Now, we can, as usual,use the maximum likelihood estimate, and this will give us the settingsof these parameters, that with a maximized observed ratingscondition of their respective reviews. And of, course,this would then give us all the useful variables that weare interested in computing. So, more specifically, we can now,once we estimate the parameters, we can easily compute the aspect rating,for aspect the i or sub i of d. And that's simply to take all of the wordsthat occurred in the segment, i, and then take their counts and then multiply that by the center ofthe weight of each word and take a sum. So, of course, this time would be zero forwords that are not occurring in and that's why were going to take the sumof all the words in the vocabulary. Now what about the s factor weights? Alpha sub i of d, well,it's not part of our parameter. Right? So we have to use that to compute it. And in this case, we can use the Maximum a Posteriori to compute this alpha value. Basically, we're going to maximize theproduct of the prior of alpha according to our assumed Multivariate GaussianDistribution and the likelihood. In this case,the likelihood rate is the probability of generating this observed overall ratinggiven this particular alpha value and some other parameters, as you see here. So for more details about this model,you can read this paper cited here. [MUSIC]
[SOUND]In this lecture we give an overviewof Text Mining and Analytics. First, let's define the term text mining,and the term text analytics. The title of this course iscalled Text Mining and Analytics. But the two terms text mining, and textanalytics are actually roughly the same. So we are not really going toreally distinguish them, and we're going to use them interchangeably. But the reason that we have chosen to use both terms in the title is becausethere is also some subtle difference, if you look at the two phrases literally. Mining emphasizes more on the process. So it gives us a error ratemedical view of the problem. Analytics, on the other handemphasizes more on the result, or having a problem in mind. We are going to look at textdata to help us solve a problem. But again as I said, we can treatthese two terms roughly the same. And I think in the literatureyou probably will find the same. So we're not going to reallydistinguish that in the course. Both text mining andtext analytics mean that we want to turn text data into high qualityinformation, or actionable knowledge. So in both cases, we have the problem of dealing witha lot of text data and we hope to. Turn these text data into something moreuseful to us than the raw text data. And here we distinguishtwo different results. One is high-quality information,the other is actionable knowledge. Sometimes the boundary betweenthe two is not so clear. But I also want to say a little bit about these two different angles ofthe result of text field mining. In the case of high quality information,we refer to more concise information about the topic. Which might be much easier forhumans to digest than the raw text data. For example, you might facea lot of reviews of a product. A more concise form of informationwould be a very concise summary of the major opinions aboutthe features of the product. Positive about,let's say battery life of a laptop. Now this kind of results are very usefulto help people digest the text data. And so this is to minimize a human effortin consuming text data in some sense. The other kind of outputis actually more knowledge. Here we emphasize the utilityof the information or knowledge we discover from text data. It's actionable knowledge for somedecision problem, or some actions to take. For example, we might be able to determinewhich product is more appealing to us, or a better choice fora shocking decision. Now, such an outcome could becalled actionable knowledge, because a consumer can take the knowledgeand make a decision, and act on it. So, in this case text mining suppliesknowledge for optimal decision making. But again, the two are not soclearly distinguished, so we don't necessarily haveto make a distinction. Text mining is alsorelated to text retrieval, which is a essential componentin many text mining systems. Now, text retrieval refers tofinding relevant information from a large amount of text data. So I've taught another separate MOOCon text retrieval and search engines. Where we discussed various techniques fortext retrieval. If you have taken that MOOC,and you will find some overlap. And it will be useful To knowthe background of text retrieval of understanding some ofthe topics in text mining. But, if you have not taken that MOOC, it's also fine because in this MOOCon text mining and analytics, we're going to repeat some of the key conceptsthat are relevant for text mining. But they're at the high level and they also explain the relation betweentext retrieval and text mining. Text retrieval is very useful fortext mining in two ways. First, text retrieval can bea preprocessor for text mining. Meaning that it can helpus turn big text data into a relatively small amountof most relevant text data. Which is often what's needed forsolving a particular problem. And in this sense, text retrievalalso helps minimize human effort. Text retrieval is also needed forknowledge provenance. And this roughly correspondsto the interpretation of text mining as turning text datainto actionable knowledge. Once we find the patterns in text data, or actionable knowledge, we generallywould have to verify the knowledge. By looking at the original text data. So the users would have to have some textretrieval support, go back to the original text data to interpret the pattern orto better understand an analogy or to verify whether a patternis really reliable. So this is a high level introductionto the concept of text mining, and the relationship betweentext mining and retrieval. Next, let's talk about textdata as a special kind of data. Now it's interesting toview text data as data generated by humans as subjective sensors. So, this slide shows an analogybetween text data and non-text data. And between humans assubjective sensors and physical sensors,such as a network sensor or a thermometer. So in general a sensor wouldmonitor the real world in some way. It would sense some signalfrom the real world, and then would report the signal as data,in various forms. For example, a thermometer would watchthe temperature of real world and then we report the temperaturebeing a particular format. Similarly, a geo sensor would sensethe location and then report. The location specification, for example, in the form of longitudevalue and latitude value. A network sends overthe monitor network traffic, or activities in the network andare reported. Some digital format of data. Similarly we can think ofhumans as subjective sensors. That will observe the real world andfrom some perspective. And then humans will express what theyhave observed in the form of text data. So, in this sense, human is actuallya subjective sensor that would also sense what's happening in the world and then express what's observed in the formof data, in this case, text data. Now, looking at the text data inthis way has an advantage of being able to integrate alltypes of data together. And that's indeed needed inmost data mining problems. So here we are looking atthe general problem of data mining. And in general we would Bedealing with a lot of data about our world thatare related to a problem. And in general it will be dealing withboth non-text data and text data. And of course the non-text dataare usually produced by physical senses. And those non-text data canbe also of different formats. Numerical data, categorical,or relational data, or multi-media data like video or speech. So, these non text data are oftenvery important in some problems. But text data is also very important, mostly because they containa lot of symmetrical content. And they often containknowledge about the users, especially preferences andopinions of users. So, but by treating text data asthe data observed from human sensors, we can treat all this datatogether in the same framework. So the data mining problem isbasically to turn such data, turn all the data in your actionableknowledge to that we can take advantage of it to change the realworld of course for better. So this means the data mining problem is basically taking a lot of data as inputand giving actionable knowledge as output. Inside of the data mining module,you can also see we have a number of differentkind of mining algorithms. And this is because, fordifferent kinds of data, we generally need different algorithms formining the data. For example, video data might require computervision to understand video content. And that would facilitatethe more effective mining. And we also have a lot of generalalgorithms that are applicable to all kinds of data and those algorithms,of course, are very useful. Although, for a particular kind of data, we generally want to alsodevelop a special algorithm. So this course will coverspecialized algorithms that are particularly useful formining text data. [MUSIC]
[SOUND]. This lecture is about the syntagmaticrelation discovery, and entropy. In this lecture, we're going to continuetalking about word association mining. In particular, we're going to talk abouthow to discover syntagmatic relations. And we're going to start withthe introduction of entropy, which is the basis for designing somemeasures for discovering such relations. By definition, syntagmatic relations hold between wordsthat have correlated co-occurrences. That means,when we see one word occurs in context, we tend to see the occurrenceof the other word. So, take a more specific example, here. We can ask the question, whenever eats occurs,what other words also tend to occur? Looking at the sentences on the left,we see some words that might occur together with eats, like cat,dog, or fish is right. But if I take them out andif you look at the right side where we only show eats and some other words,the question then is. Can you predict what other wordsoccur to the left or to the right? Right sothis would force us to think about what other words are associated with eats. If they are associated with eats,they tend to occur in the context of eats. More specifically ourprediction problem is to take any text segment which can be a sentence,a paragraph, or a document. And then ask I the question,is a particular word present or absent in this segment? Right here we ask about the word W. Is W present or absent in this segment? Now what's interesting is that some words are actually easierto predict than other words. If you take a look at the threewords shown here, meat, the, and unicorn, which one do youthink is easier to predict? Now if you think about it fora moment you might conclude that the is easier to predict becauseit tends to occur everywhere. So I can just say,well that would be in the sentence. Unicorn is also relatively easybecause unicorn is rare, is very rare. And I can bet that it doesn'toccur in this sentence. But meat is somewhere inbetween in terms of frequency. And it makes it harder to predict becauseit's possible that it occurs in a sentence or the segment, more accurately. But it may also not occur in the sentence,so now let's study thisproblem more formally. So the problem can be formally defined as predicting the value ofa binary random variable. Here we denote it by X sub w,w denotes a word, so this random variable is associatedwith precisely one word. When the value of the variable is 1,it means this word is present. When it's 0, it means the word is absent. And naturally, the probabilities for1 and 0 should sum to 1, because a word is either present orabsent in a segment. There's no other choice. So the intuition with this concept earliercan be formally stated as follows. The more random this random variable is,the more difficult the prediction will be. Now the question is how does onequantitatively measure the randomness of a random variable like X sub w? How in general, can we quantifythe randomness of a variable and that's why we need a measurecalled entropy and this measure introduced in informationtheory to measure the randomness of X. There is also some connectionwith information here but that is beyond the scope of this course. So forour purpose we just treat entropy function as a function definedon a random variable. In this case, it is a binary randomvariable, although the definition can be easily generalized fora random variable with multiple values. Now the function form looks like this, there's the sum of all the possiblevalues for this random variable. Inside the sum for each value wehave a product of the probability that the random variable equals thisvalue and log of this probability. And note that there is alsoa negative sign there. Now entropy in general is non-negative. And that can be mathematically proved. So if we expand this sum, we'll see thatthe equation looks like the second one. Where I explicitly pluggedin the two values, 0 and 1. And sometimes when we have 0 log of 0, we would generally define that as 0,because log of 0 is undefined. So this is the entropy function. And this function willgive a different value for different distributionsof this random variable. And it clearly depends on the probability that the random variabletaking value of 1 or 0. If we plot this function against the probability that the randomvariable is equal to 1. And then the function looks like this. At the two ends,that means when the probability of X equals 1 is very small or very large,then the entropy function has a low value. When it's 0.5 in the middlethen it reaches the maximum. Now if we plot the functionagainst the probability that X is taking a value of 0 and the function would show exactly the same curve here,and you can imagine why. And so that's because the two probabilities are symmetric,and completely symmetric. So an interesting question youcan think about in general is for what kind of X does entropyreach maximum or minimum. And we can in particular thinkabout some special cases. For example, in one case,we might have a random variable that always takes a value of 1. The probability is 1. Or there's a random variable that is equally likely taking a value of one orzero. So in this case the probabilitythat X equals 1 is 0.5. Now which one has a higher entropy? It's easier to look at the problemby thinking of a simple example using coin tossing. So when we think about randomexperiments like tossing a coin, it gives us a random variable,that can represent the result. It can be head or tail. So we can define a random variableX sub coin, so that it's 1 when the coin shows up as head,it's 0 when the coin shows up as tail. So now we can compute the entropyof this random variable. And this entropy indicates howdifficult it is to predict the outcome of a coin toss. So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head ortail equally likely. So the two probabilities would be a half. Right?So both are equal to one half. Another extreme case iscompletely biased coin, where the coin always shows up as heads. So it's a completely biased coin. Now let's think aboutthe entropies in the two cases. And if you plug in these values you cansee the entropies would be as follows. For a fair coin we see the entropyreaches its maximum, that's 1. For the completely biased coin,we see it's 0. And that intuitively makes a lot of sense. Because a fair coin ismost difficult to predict. Whereas a completely biasedcoin is very easy to predict. We can always say, well, it's a head. Because it is a head all the time. So they can be shown onthe curve as follows. So the fair coin corresponds to the middlepoint where it's very uncertain. The completely biased coincorresponds to the end point where we have a probabilityof 1.0 and the entropy is 0. So, now let's see how we can useentropy for word prediction. Let's think about our problem isto predict whether W is present or absent in this segment. Again, think about the three words,particularly think about their entropies. Now we can assume high entropywords are harder to predict. And so we now have a quantitative way totell us which word is harder to predict. Now if you look at the three words meat,the, unicorn, again, and we clearly would expect meat to havea higher entropy than the unicorn. In fact if you look at the entropy of the,it's close to zero. Because it occurs everywhere. So it's like a completely biased coin. Therefore the entropy is zero. [MUSIC]
This lecture is aboutthe mixture model estimation. In this lecture, we'regoing to continue discussing probabilistictopic models. In particular, we're goingto talk about the how to estimate the parametersof a mixture model. So let's first lookat our motivation for using a mixture model, and we hope to effect out the background words fromthe topic word distribution. So the idea is to assume that the text data actuallycontain two kinds of words. One kind is fromthe background here, so the "is", "we" etc. The other kind is from our topic word distributionthat we're interested in. So in order to solvethis problem of factoring out background words, we can set up our mixturemodel as follows. We are going to assume that we already know the parameters of all the values for all the parameters inthe mixture model except for the word distribution ofTheta sub d which is our target. So this is a case ofcustomizing probably some model so that we embedded the unknown variablesthat we are interested in, but we're going tosimplify other things. We're going to assumewe have knowledge about others and this is a powerful way of customizing a modelfor a particular need. Now you can imagine, wecould have assumed that we also don't know thebackground word distribution, but in this case,our goal is to affect out precisely those high probabilityin the background words. So we assume the backgroundmodel is already fixed. The problem here is, how can we adjust the Theta subd in order to maximize the probability ofthe observed document here and we assume all theother parameters are known? Now, although wedesigned the modal heuristically to try to factor out thesebackground words, it's unclear whether if we use maximumlikelihood estimator, we will actually end up havinga word distribution where the common words like "the" will be indeed having smallerprobabilities than before. So now, in this case, it turns out thatthe answer is yes. When we set up the probabilisticmodeling this way, when we use maximumlikelihood estimator, we will end up havinga word distribution where the common wordswould be factored out by the use ofthe background distribution. So to understand why this is so, it's useful to examinethe behavior of a mixture model. So we're going to lookat a very simple case. In order to understand some interesting behaviorsof a mixture model, the observed patternshere actually are generalizable to mixturemodel in general, but it's much easier tounderstand this behavior when we use a very simple caselike what we're seeing here. So specifically in this case, let's assume thatthe probability of choosing each of the two modelsis exactly the same. So we're going to flip a fair coin to decidewhich model to use. Furthermore, we are goingto assume there are precisely to words,"the" and "text." Obviously, this isa very naive oversimplification of the actual text, but again, it is useful to examine the behaviorin such a special case. So we further assume that, the background model givesprobability of 0.9 to the word "the" and "text" 0.1. Now, let's also assume thatour data is extremely simple. The document has just two words"text" and then "the." So now, let's write down the likelihood functionin such a case. First, what's the probability of "text" and what's theprobability of "the"? I hope by this point, you will be ableto write it down. So the probability of "text" is basically a sum oftwo cases where each case corresponds to each of the water distribution and it accounts for the two waysof generating text. Inside each case, we have the probability of choosingthe model which is 0.5 multiplied by the probability of observing "text"from that model. Similarly, "the" wouldhave a probability of the same form just as it was different exactlyprobabilities. So naturally,our likelihood function is just the product of the two. So it's very easy to see that, once you understandwhat's the probability of each word and whichis also why it's so important to understand what's exactly the probability of observing each word fromsuch a mixture model. Now, the interestingquestion now is, how can we then optimizethis likelihood? Well, you will notice that, there are only two variables. They are preciselythe two probabilities of the two words"text" and "the" given by Theta sub d. This isbecause we have assumed that, all the otherparameters are known. So now, the question isa very simple algebra question. So we have a simple expression with two variables and we hope to choose the values of these two variables tomaximize this function. It's exercises that we have seen some simplealgebra problems, and note that the twoprobabilities must sum to one. So there's some constraint. If there wereno constraint of course, we will set both probabilities to their maximum value whichwould be one to maximize this, but we can't do that because "text" and"the" must sum to one. We can't give those aprobability of one. So now the question is, how should we allocatethe probability in the mass between the two words?What do you think? Now, it will be useful to look at this formula formoment and to see intuitively whatwe do in order to set these probabilities to maximize the valueof this function. If we look into this further, then we'll seesome interesting behavior of the two componentmodels in that, they will becollaborating to maximize the probability ofthe observed data which is dictated by the maximumlikelihood estimator, but they're alsocompeting in some way. In particular, theywould be competing on the words and theywill tend to bet high probabilities ondifferent words to avoid this competition in some sense or to gain advantagein this competition. So again, looking at thisobjective function and we have a constraint onthe two probabilities, now if you look atthe formula intuitively, you might feel thatyou want to set the probability of "text" to be somewhat larger than "the". This intuition canbe well-supported by mathematical fact which is, when the sum oftwo variables is a constant then the product of them which is maximumthen they are equal, and this is a fact thatwe know from algebra. Now, if we plug that in, we will would meanthat we have to make the two probabilities equal. When we make them equaland then if we consider the constraint that we caneasily solve this problem, and the solution is theprobability of "text" would be 0.9 and probabilityof "the" is 0.1. As you can see indeed, the probability of textis not much larger than probability of "the" and this is not the case when wehave just one distribution. This is clearly because of the use of thebackground model which assign a very high probability to "the" lowprobability to "text". If you look at the equation, you will see obviously some interaction ofthe two distributions here. In particular, you will see in order to make them equal and then the probability assignedby Theta sub d must be higher for a word that has a smaller probabilitygiven by the background. This is obvious fromexamining this equation because "the" background part is weak for "text" it's a small. So in order tocompensate for that, we must make the probabilityof "text" that's given by Theta sub d somewhat larger so that the two sidescan be balanced. So this is in fact a very general behaviorof this mixture model. That is, if one distribution assigns a high probabilityto one word than another, then the other distribution would tend to do the opposite. Basically, it would discourage other distributions to do the same and this is tobalance them out so that, we can account for all words. This also means that, by using a backgroundmodel that is fixed to assign high probabilitiesto background words, we can indeed encourage the unknown topicword distribution to assign smaller probabilitiesfor such common words. Instead, put more probabilitymass on the content words that cannot be explained well by the backgroundmodel meaning that, they have a verysmall probability from the backgroundmodel like "text" here.
Hello welcome to CS410 DSO Text Information Systems. This is an online course offered by University of Illinois at Urbana-Champaign. My name is ChengXiang Zhai. I also have a nickname, Cheng. I'm a professor of Computer Science at the University of Illinois at Urbana-Champaign. I'm the instructor of this course. This first lecture is just an introduction to the course. Let's first to start with some motivation. The problem we are trying to address in this course is how to harness big text data. Text data is all kinds of data in the form of natural languages such as English and Chinese. Now, this kind of data is everywhere and also growing very quickly. For example, you can find all kinds of web pages on the Internet and the number of pages is growing quickly. You can also find the blogs articles, news articles, or Emails and other kind of documents and enterprise environment. Of course, we will also have a lot of scientific literature in text form. And nowadays, social media has been growing quickly. So we now see, tweets and other social media data also in the form of text. All such text data encode a lot of useful knowledge about the world because it's in some sense the data reported by human census about the observe the world. So we can analyzed this kind of data to discover a lot of useful knowledge. Especially the knowledge about the human opinions or preferences. So this kind of data is very useful and we can use computational methods to turn such data into useful knowledge, which can then be further used in many applications. The main techniques for making this happen include the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and that's due to a large set of text data into a smaller but much more relevant set of data, that we actually need for a particular problem. And this step, is usually implemented by using text retrieval techniques that involve humans in the loop to find and locate the most relevant documents to a particular problem. Once we find the relevant documents, the next step is to do text mining, which is to further analyze the found of relevant documents to discover useful knowledge to extract the knowledge that can be directly used in application, especially in a application such as decision making. These two steps corresponding to Text Retrieval and Text Mining Techniques, that we will cover in this course. Based on this picture that I show you, this course is designed to leverage to corresponding books that I've offered on Coursera, which cover Text Retrieval and Text Mining respectively. The first book is called, Text Retrieval and Search Engines. The second book is called, the Text Mining and Analytics. These two books have comprehensive lecture videos that cover basic concepts and principles and methods for text retrieval and text mining respectively. So, this online course, will leverage all those lecture videos and also the online quizzes that are provided through the Coursera platform. But in addition to that, we also need to add additional components in order to explore applications because those books have covered a general techniques without a necessary discussing in depths how those techniques are using applications. So, to make the coverage more complete in CS410, we were adding two additional components as shown here. That is of course Products and Technology Review. Both are meant to give the students freedom to choose a particular way to apply some of the techniques that you have learned in those two books to solve a real world problem or to further learn about a particular topic. In the project, you will have opportunities to apply the knowledge that you have learned in those two books to solve a real world problem in an integrated manner. In technical review, you can leverage what you have learned to learn even more about either tool kit or a particular technology that can be used to extend your knowledge about how to solve a problem. The format of the course thus is a mixture of online videos plus some high engagement module which mostly consists of our interactions through forums as I will explain later. And also, we will have a lot of interactions to help you finish the course Project and Technology Review. There are a number of goals that we have kept in mind when designing this course and I want to share with you these goals because they would help you understand why the course has this particular format and why you are asked to do certain components of the tasks. The first goal is we want to emphasize both theory and practice. Theory is obviously very important because the basic concepts and general principles covering the theoretical part would be applicable to all applications. That means, they are not just used for today's applications but they can be used for solving future problems as well so, they are more general. Therefore, they have long lasting impact and utility value. And this is achieved by having you to watch all the lecture videos, and then to use quizzes and exams to make sure that you have mastered all the basic ideas, basic concepts and general principles and those general methods. Practical skills are also very important, and because of specific practical skills can be immediately useful for solving the problems today and maybe you will be able to resolve the problems that you encountered in your job. And this is achieved by having you to do Programming assignments, where you would be able to use existing tool kits, to look into some algorithms in depths and to use some of the algorithms to have some sense about how they work or how to improve them. And finally, of course it's very important to integrate the theory and practice. And this will be done by, having you to work on Course projects. The second goal is to Personalized Learning, because every one of you has a different in need and different in preference, and you will have a different in schedules as well, because many of you are full time workers, perhaps. And so, Personalized Learning is very important to ensure everyone to receive the best education. So to achieve this goal, we have intentionally designed the deadlines and the format of the course to be flexible so that, you can have a lot of freedom to do self-paced learning. For example, you can watch over the lecture videos at anytime that you want to. Watch it, watch that. And you can also finish all the quizzes pretty much anytime and you can do the assignment, program assigns particularly again in a flexible way. In addition to self paced learning, you would also have a choices of topics for project and the technology review. And you will be able to work on a topic that's most interesting to you, for both course project and technology review. Finally, we also have the goal of collaborative learning because this would maximize the efficiency of learning. Our common goal is to help everyone learn maximum amount of knowledge, while spending hopefully minimum amount of effort. So, this is a common goal that we should all work toward, and we should help each other achieve this goal. To promote collaborative learning and to facilitate collaborative learning, we will use forum-based interactions to enable all of us, including many of you from different time zones, to interact with each other effectively. Another way to support a collaborative learning is to have you to work in groups, to finish cost projects and technology reviews. So, both technology reviews and products can be done by working together with others in a group. So, overall the format is as follows: first, you will have to watch the lecture videos of those two MOOCs, and this is also shown here on this slide, and then you also need to take quizzes. And these quizzes are given in a weekly manner. But as I said, the deadlines are actually flexible. So, you should be able to finish quizzes as soon as you finish the corresponding lecture videos. These quizzes are two. There are also two kinds of quizzes. There are practice quizzes that you can use to help you understand some concepts. And there are also test quizzes. Those are of course to make sure that you have indeed mastered the materials. Then there are two exams. Those two exams will be given at the end of each MOOC corresponding. So, the first exam will be given at the end of the first MOOC on Test Retrieval, and the second one will be given at the end of the second MOOC on Test Mining. And then, during this period of watching these videos, you will be also working on programming assignments. Then at the end of the semester, we'll leave about two weeks time for you to work intensively on the course project, although you will be working on the course project throughout the semester. But most of the time will be in the end of the semester. Now, the course project will be down sequentially by following multiple steps. The first is for you to select a topic. You can select the topic from a list of topics that we provide or you can propose your topic, and then we'll ask you to submit a short proposal to be more specific about what you want to work on. And then, in the middle of the semester, you will be asked to provide a short progress report so that we can check your progress and we can provide help in a timely manner. And then, at the end of the semester, you will deliver two things: one is a software and its documentation. You upload that to a public website so that it's available to people. The second one is a tutorial presentation, a short presentation to explain how your software can be used by people. There is also a technology review component which is mandatory and this component will be correlated based on completion. Everyone is required to finish this component. Now, this component is designed to give you an opportunity to explore further any topic that you're interested in. This can be in that examination of a tool kit that you are interested in, perhaps a tool that you have used in the course project, or a comparison of multiple tools that can do similar things, and you might have looked into multiple tools, and you have some opportunity to compare them and to figure out which one is the best. Such a writing would be helpful for others to understand how to use a tool kit or which one to choose. Of course, those of you who are interested in the methods, you can also go in-depth to examine a certain type of methods and write a brief review of them, or looking to a cutting edge topic in your research and write a review of the recent papers about the topic. Now, we hope that you might be able to align the technology review with your course project so that the two will be kind of synergistic in that technology review will help you learn more about the topic related to your course project, while the course project will give you also a motivation for doing an in-depth study of the topic via technology review. Of course, we will try to help you finish all these tasks, and we'll do our best to help you. That means, the TAs and I will interact with you in various ways to help you, and one way is a synchronous question answering and discussing via forums. And the other way is to have synchronous weekly office hours and that will be down by using video teleconferencing. The grading will be done as follows: 25% of your grade will be based on the quizzes, all the quizzes for the two MOOCs; 30% will be based on the two exams, those two exams will be proctored exams; 25% will be based on the programming assignments, there will be multiple programming assignments throughout the semester. The remaining 20% will be based on your course project and that's for the distribution as follows: 5% is for topic selection, 5% is for proposal, and another 5% for progress report, 65% in most of the grade of the course project will be based on software deposit, your deliverable for the project. And finally, 20% is based on the tutorial presentation. Now, most of these components in the course project will be graded based on completion. Indeed, a lot of tasks you see in programming assignments are graded in the same way and that is because we believe we have designed the program assignments and course project in such a way that you will be able to learn a lot by simply going through these tasks. So, this is the effective way of learning by doing and that's why we choose to grade based on completion. That also means you have a lot of control over these grades, because you can ensure that you finish all these tasks and then you will get most of these grades. The only part that will be based on the quality of solution is the tutorial presentation. This is actually not just based on the quality of your tutorial presenting but rather based on whether you are software actually work as you propose. So, does it provide all the functions that you propose? Does it really work? And this will likely affect some of the grade for tutorial presentation, meaning that if a software for some reason doesn't really pass our test, then we would deduct the points from here. As I said, the technology review is not actually contributing to you grade, but it's a metric component and it will be graded based on completion. We also provide up to 5% of extra credit based on your participation in the forum discussions. And this is to encourage you to help each other and particularly by answering questions posed by others. We will be able to use log data from the forum to give you credit, extra credit, and these extra credit points will be actually added to the regular points. So that will allow you to actually increase your grade as will be shown here in more detail. And this is how we are going to determine your final letter grades. They are based on the grade points you have collected over the semester using this map shown here on the slide. Mostly it's five points for each bracket but not always. And as you can see, if you have earned 5% extra credit and when adding this extra credit, will likely help you move your grades up by one bracket, although it won't be more than one bracket. So, we feel that this fixed mapping would give you complete control over your grade, so you can monitor your grade over the semester and kind of assess your progress and also to adjust your schedule accordingly. So, this is a visualization of your workload. Horizontally, we show the timeline, from the first day of instruction to the last day of instruction. And then vertically, you can see there are mainly six tasks for you over the semester. First, you will spend most of your time on watching the lecture videos, and this is why we have a thick black line there that shows that most of your effort probably will be spent on watching lecture videos. And then, you will be taking 12 quizzes, and these will be spreading over most of the semester, corresponding to about 12 weeks when you are expected to watch those lecture videos. Now, we want to emphasize that you should spend most of your time to watch videos and make sure you understand the materials before you take quizzes, and this will ensure that you don't leave any holes. If you do it the other way and by using quizzes to guide you through the lecture videos, then you might leave some holes, and that might hurt your performance on exams. There will be two proctored exams that will be given in the middle of the semester and also later in the semester, at the end of the two box. So, this is your third task. The fourth task is programming assignments, and this will be, again, through the entire semester, actually not really all the weeks in the semester, but most of the weeks, because we don't want you to use up the last two weeks, which we reserve for you to work on course project. And the course project is the fifth component. And finally, you need to finish technology review. Now, although most of the work of the project is expected to be done at the end of the semester, you will actually start working on it from the very beginning. And, soon after the semester starts, we will ask you to think about the topics and you will discuss the topics and then form teams to submit the proposal. But of course, during the first 12 weeks, you will be most occupied by the lecture videos, quizzes, and your assignments for programming. So, that's why you will likely have more time to work on the project in the end. The technology review is kind of designed to extend your knowledge based on your project. But of course, you have the complete freedom to choose whatever topic that you want to review. So, you don't have to tie it to the project. And so, we imagine that you would start working on it a little bit after you have decided the project, the topic, so that you can decide whether you want to tie the technology review with your project. So, it's good to keep this picture in mind throughout the semester because you might have irregular schedule sometimes. For example, you might find that you are very busy in the middle of the semester, then this picture can help you adjust your schedule and the degree you probably want to then work more on some of the tasks at the beginning of the semester, so that you don't overwhelm yourself in the middle of the semester. And similarly, if you expect to be very busy the end of the semester, you might want to start working on the project much earlier. So, I hope this picture will be always in your mind, and you will be able to adjust your schedule accordingly and to particularly work on tasks proactively, in case you anticipate any busy time period. Some of you have already taken maybe both MOOCs. Now, if you are one of them, then I think naturally, you will have more time to work on other problems in this course. So, you should take advantage of this to proactively finish some of the tasks much more quickly. In particular, since you have already watched those videos and then you can simply review them and then you try to work on the quizzes so that you can finish all the quizzes much more quickly. You may be able to finish most of the programming assignments quickly too. This is not only going to be helpful for you in the sense that you will have a lot of time to work on course project, but also, it would allow you to help others by answering their questions on forums or helping them in other way, like teaming up with them to work on the project. However, I should also say that there are a few tasks that are, unfortunately, have very fixed time that you cannot really work on in advance. For example, the two exams will be scheduled on some particular dates that would have no flexibility for you to work on earlier. There may be some tasks in the programming assignments that have to be synchronized. For example, we might run competition of some tasks and there may be some synchronization that's needed, but we would like to minimize the dependency so that you could hopefully work on many of those tasks as early as you can. And of course, we encourage you to use more time to finish a more challenging course project or to finish a higher quality technology review. Now, we rely a lot on forum discussion, and this is because we are in different time zones and it's very hard to find a time that works well for everyone. But forum has important advantages of being able to accommodate everyone in the discussion. So, this will be the primary way of our interactions and engagement, and in particular, we will be using Piazza, which is a forum that we have used for many other courses, and it has proven to be a useful forum with a lot of useful functions. The second advantage of forum is it would also enable you to ask your questions as soon as you have a question. And therefore, we can hopefully accelerate question answering and so that you can have your question answered quickly on the forum without waiting until office hour. Finally, we hope that the forum discussions would help us identify difficult concepts in those lectures, so that we can focus on discussing these concepts or explaining these concepts that are hard to understand in our office hours. This would make better use of our office hours to help all of you. Because of these reasons, so the protocol of question answering will be as follows, and we want to emphasize that it's important to follow this protocol so that we can make effective use of our office hours, so that you can get your questions answered more quickly, so that you can all help each other in learning. So, first, as soon as you have a question or issue to discuss, post it immediately on the forum. And this has a number of advantages, and first is, it will give you the opportunity to have the question answered quickly because often, your question may be answered by your peers or may be answered by a teacher or by me. So, by posting the question immediately, you have a better chance of getting the question answered quickly, so that you won't have to wait. And secondly, your question might also help others because sometimes other students may have a similar question or may not realize that they have encountered this question, but you just articulated this question. So, this is helpful for your peers as well. And discussing of this question is often also a good way to learn. However, if your question is not answered in a timely manner on the forum, or addressed adequately from your perspective, then you should email the question to all of us including me and TAs. Please use a subject line that contains the keyword CS410DSO all together. Of course, your subject line, you can contain other keywords based on your question. And then, if you don't receive a reply from us by email in a timely manner, join an office-hour. Now, we would do our best to reply to your emails but then, depending on the number of emails, depending on our own schedule, we may not be able to always reply to your emails in a timely manner. Of course, we would do our best to respond quickly but if we can't, then you should come to one of our office hours. We would try to schedule our office hours in different time slots during the day. For example, we would have some time slot in the morning, or late morning, or early afternoon, and then another time slot in the evening. And this is so that, we can hopefully accommodate different time zones, because the same slot of time may not be equally convenient with all of you. So, we'll do our best again, to diversify the time slots, both in terms of the time in a day, and also in terms of the days in a week. The format of office hours is as follows, and this again, is based on our need to make effective use and the efficient use of our limited office hours to help you in the best way. So, we will hold weekly office hours, as I said, and we'll publish those time slots. And the office hours will be given by using video-teleconference, and particularly using Zoom, and its used for system that has worked well. And you can join or leave an office hour at any time and that means you can join late, or you can leave early, and it's very flexible. So, don't feel that you always have to come at the beginning of the office hour. Feel free to stop by in the last ten minutes, if that's the best time for you. And again, by taking advantage of the forum discussion, hopefully, by the time of going to office hour, we will only need to deal with some of the relatively difficult questions. And the priority we will use is to give the highest priority to any issues that have already been posted on forums, but have not been resolved even after some email communications. Those are clearly the toughest issues because it has been posted on forum without a good answer, and then was emailed to us and still not satisfactory solved. So, we would have to give those issues the highest priority, that means if there are such issues being raised during office hour, those issues would be given the first priority. After that, we'll look at the any other unresolved issues on forums. That's why it's very important for you to post the issue on the forum first, and only after resolving those issues that have already been posted on forums would we take other questions or issues that have not yet been posted on forums. And of course, you should not hesitate to bring any questions that you have or any issues you want to discuss. It's just that, we want to have a policy to prioritize the issues that we want to handle, so that we can provide the maximum benefit to all of you by using our office hours efficiently. Finally, I want to just say something in general about how to get the most out of this course. Perhaps, the most important advice is to plan ahead based on your own schedule, because many of you are very busy. So, you want to kind of take a look at the picture that I showed you earlier about the tasks, and then consider your own schedule. Try to imagine which periods will be a relatively busy period for you and identify what tasks you're supposed to work on in that period, and try to finish those tasks earlier, so that you don't overlap with yourself during that busy period. And of course, at any time, please let us know how we can help, again, by using the forums as the first step. And another thing to mention is also to allocate a sufficient time for the preparation of two proctored exams because they will be given only once. That means, you only have one chance to take each exam, so you want to really prepare well for them. However, those exams are mostly to confirm that you have indeed mastered the materials. So, they will have similar questions to the quiz questions that you have already seen. And in fact, some questions may be exactly the same as the questions in the quizzes. And we do that because this would allow you to have some sense about what the questions might look like in the exams. So, they should not be much surprise if you have actually worked on all the questions in those quizzes and have made sure that you have understood the answers to those questions. Of course, if you can, try to complete the quizzes and program assignments ahead of time. This would be to your advantage because you can now raise your questions earlier that would allow you to have more time to get your questions answered. Therefore, by the time when you take the quiz, then you would have all the questions resolved. And it also would allow you to actively help others and to discuss any of the problems that you have encountered that would help you earn the extra credit also. The second advice is to post questions on forum immediately and whenever you have difficulty understanding any part of the course materials. And again, I want to emphasize the immediate action of posting any issue, any question on the forums. We would do our best to resolve those issues through the forums. And it's the effective way to also engage the peers to help each other. So, do not hesitate to post questions and we won't to penalize you for posting many questions. In fact, we'll reward that perhaps, because that's one way to contribute to the forum discussion. Finally, you should leverage collaborative learning, this is kind of related to you posting your questions on forums, and again, to actively participate in the forum discussion. You will actually learn a lot from reading other people's posts, even if you know the answers because they are often opinions expressed about those materials. So, we do hope that you have active discussion on forums to help each other and to help each other, particularly, to save time, to understand the difficult concepts, to do well in the quizzes and exams, and to finish assignments smoothly. And finally, we do encourage you to help each other understand materials too. So, we encourage you to answer others' questions, and the system will record those answers. We'll have a statistics about that, and then we'll use that to give extra credit. So, this was just the overall introduction to the course. For more information, you can visit the course website on Coursera. We hope you enjoy this course, and I look forward to working with you. Thank you.
###  **Introduction**  The course project is to give the students hands-on experience on developing some novel information retrieval and/or text mining tools. It would allow the students to potentially apply all the knowledge and skills learned in the course to solve a real-world problem. Group work is encouraged, but not required (i.e., you can have a one-person team). The maximum size of a team is 5 members to avoid challenges in efficient coordination of the work by too many team members. However, a team of a larger size is also possible subject to the approval of the instructor. A typical reason for a larger team is because the project has a very natural task division among the team members so that the need for frequent interactions and coordination of team members may be minimum despite the large size of the team. Whenever possible, collaboration of multiple project teams is strongly encouraged to minimize the amount of work of each team via expertise or resource sharing, as well as to generate “combined impact” (e.g., one team may develop a crawler that can be used by another team that develops a search engine).  More details (what to submit at each step) can be found here:  https://docs.google.com/document/d/1ubzdWekH2WLzft- IaSnkYflKrR7DqW-X7WsWiG30loI/edit#heading=h.wfalxc53x421  ###  **Grading criteria**  Your project will be graded based on the following weighting scheme, corresponding to three stages of work including 1) team search and formation; 2) proposal development; 3) project result submission. All team members will receive the same grade provided that every member has made sufficient effort (at least 20 hours of quality time to work on the project).    * **Team formation (5%):** Every student is required to form a team by the **beginning of** **Week 9 (Oct 18, 2021)** . The team shall need to designate one of the team members as the team leader who will be responsible for submitting and/or uploading the course project deliverables.     * **Project proposal (5%):** Each project team is required to submit a roughly one-page project proposal by the **end of** **Week 9 (Oct 24, 2021)** , and graded based on completion.     * **Progress report (5%):** Each project team is required to submit a short progress report by the **beginning of Week 13 (Nov 15, 2021)** listing the progress made, remaining tasks, any challenges/issues being faced, etc. It will be graded based on completion.     * **Software code submission with documentation (65%):** Due **end of Week 15 (Dec 09, 2021)** , each project team will be asked to submit the produced source code with reasonable documentation. The documentation should cover both how to use the software and how the software is implemented. The 65% of the grade would be distributed as follows: **45% for source code submission** ; **20% for documentation submission** . Both would be graded based on completion.     * **Software usage tutorial presentation (20%):** Due **end of** **Week 15 (Dec 09, 2021).** 10% of the grading is based on completion and the remaining 10% is based on result of testing the software by graders   Thus, your course project work would be graded primarily based on your effort. If you have followed our guidelines and completed all the required tasks, you should receive at least 90% of the total points for the project. This is to encourage the students to pay attention to time management and set realistic goals that can actually be completed by the end of the semester. The remaining 10% is based on how well your software works; a fully functioning software would be given the whole 10%, whereas a buggy software or a software with missing functions would result in losing some of the 10% of the grade. Completion of a functioning software is emphasized also due to the potential dependency between multiple projects when they are all contributing to a larger project (e.g., one team may produce a crawler to crawl data for use by another team to build a search engine).  _The proposal, progress report and final submission will be peer-graded by you. The TAs will review all peer reviews and make sure that (1) the reviews are fair, and (2) the submissions satisfy all requirements._  **Peer-Grading Instructions**  We'll be using peer-grading for grading project proposals, progress reports and final submissions. Later in the semester, you will receive an email from CMT inviting you as a reviewer, and we will make an announcement when the emails are sent.  Peer grading will begin soon after the submission deadlines have passed. **The proposal peer-grading should be done between Oct 24 (12:00 am CST)-Oct 29 (12:00am CST).** **The progress report peer-grading should be done between Nov 15 (12:00 am CST)-Nov 19 (11:59 pm CST). The final submissions peer-grading should be done between Dec 09 (12:00 am CST)-Dec 17 (11:59 pm CST). Please finish the gradings on time.**  Each project will be reviewed by at least 2 students and each student will review ~1-2 projects. We will be using your comments and scores as a guide to assign the final scores to the projects. Please grade them carefully and honestly. We appreciate your help with managing the grading of such a large class. We hope the process would also be a good learning experience for you. More specific instructions will be announced and sent later.  ###  **Instructions**  **1\. Form a team (5%, due beginning of Week 9, Oct 18 2021)**  By the beginning of Week 9, you should form teams. A team ideally comprises of 1 to 3 members. To form larger groups of up to 5 members, please post a private note on Piazza to get TA or instructor approval. Groups of more than 5 members are highly discouraged. The team also needs to designate a member of the team as team leader. Your team leader will be responsible for submitting and/or uploading the course project deliverables such as: 1) this form, 2) the proposal, 3) the presentation, 4) the code, etc.  **2\. Pick a topic & Write a proposal (5%, due end of Week 9, Oct 24 2021) **  **Picking a project topic**  Students will be able to choose their own topic or select a topic from a list of suggested topics which we will provide by Week 4.  **Writing a project proposal**  Each project team is required to write a one-page proposal before you actually go in depth on a topic. The proposal is due end of Week 9.  In the proposal, you should include the names and email addresses of all the team members. One member must be designated as the project coordinator/leader for the team, so please make sure to indicate that. The project coordinator would be responsible for the coordination of the project work by the team and also communication with the instructor or TA when the team needs help.  Detailed instructions about the required content of the proposal will be provided by Week 4, including a short set of questions that need to be answered in the proposal. As long as those questions are addressed (wherever applicable), the proposal does not have to be very long. A couple of sentences for each question would be sufficient.  **3.** **Peer-review project proposals (due Middle of Week 10, Oct 29 2021)**  Each student will review the progress reports of ~1-2 groups and provide feedback/suggestions. TAs will go through the peer assessments and make sure that the proposals satisfies all requirements listed in the Google Doc.  **4\. Work on the project**  You should try to reuse any existing tools as much as possible so as to minimize the amount of work without sacrificing your goal. Discuss any problems or issues with your teammates or classmates on Campuswire and leverage Campuswire to collaborate with each other. Consistent with our course policy, we strongly encourage you to help each other in all the course work so as to maximize your gain of new knowledge and skills while minimizing your work as much as possible. We will do our best to help you as well. Consider documenting your work regularly. This way, you will already have a lot of things written down by the end of the semester.  **5\. Submit progress report (5%, due beginning of Week 13, Nov 15 2021)**  Each team must submit a short report detailing 1) Progress made thus far, 2) Remaining tasks, 3) Any challenges/issues being faced. The graders, TAs and instructor may provide help and suggestions based on the report. Continue working on the project until the final submission.  **6\. Peer-review progress reports (due end of Week 13, Nov 19 2021)**  Each student will review the progress reports of ~1-2 groups and provide feedback/suggestions.  **7\. Software code submission with documentation (65%, due end of Week 15, Dec 09 2021)**  Each team must submit the software code produced for the project along with a written documentation. The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. 4) Brief description of contribution of each team member in case of a multi-person team. Note that **if you are in a team, it is your responsibility to figure out how to contribute to your group project, so you will need to act proactively and in a timely manner if your group coordinator has not assigned a task to you.** There will be no opportunity to make up for any task that you failed to accomplish. **In general, all the members of a team will get the same grade for the project unless the documentation submission indicates that some member(s) only superficially participated in the project without doing much actual work; in that case, we will discount the grade.** Everyone is expected to spend at least 20 hours to seriously work on your course project as a minimum, not including the time spent for preparing the documentation.  The 65% of the grade would be distributed as follows: **45% for source code submission** ; **20% for documentation submission.** The 20% for the documentation submission includes 5% for overview of functions, 10% for implementation documentation, 5% for usage documentation. There is no strict length requirement for the documentation.  **8\. Software usage tutorial presentation (20%, due end of Week 15, Dec 09 2021)**  At the end of the semester, every project team will be asked to submit a short tutorial presentation (e.g., a voiced ppt presentation) to explain how the developed software is to be used. The presentation must include (1) sufficient instructions on how to install the software if applicable, (2) sufficient instructions on how to use the software, and (3) at least one example of use case so as to allow a grader to use the provided use case to test the software. There is no strict length requirement for this video submission, but you should target at 5~10 minutes. A presentation shorter than 5 minutes is unlikely detailed enough to help users understand how to use the software, whereas a longer video than 10 minutes might be too long for impatient users. However, feel free to produce a longer presentation if needed.  The tutorial presentation would be graded based on  1) completion of the presentation (10%); and  2) result of testing the software by graders (10%).  If the software passes the test (i.e., is working as expected), full points will be given; otherwise, points will be deducted from the 10% allocated to the “result of testing the software by graders.”  **9\. Peer-review project code, documentation, and presentations (due Dec 17 2021)**  Each student will review the final project submissions of ~1-2 groups and provide feedback/suggestions.  
##  Exam Instructions    * A password quiz precedes and unlocks the proctored exam. The proctor will enter the password for you.     * Under no circumstances should you type in any password or make an attempt in the password quiz.     * **Note: If the exam does not immediately unlock after the Proctor enters the password, please let the Proctor know that you are going to refresh the page, which should resolve the issue. Then refresh the page.**  ##  How to Schedule the Proctored Exam  **Note: you need to schedule the exam on ProctorU website at[ https://go.proctoru.com ](https://go.proctoru.com/) yourself by following the instructions below. **    1. If this is your first time scheduling an exam with ProctorU, click on the **Sign Up** link on the [ University of Illinois ProctorU portal page ](http://proctoru.com/portal/illinois) to create a new user account and ProctorU log-in password. It is recommended to use the University of Illinois email address to register for the account. If you have a ProctorU account already, you can skip this step and go to [ https://go.proctoru.com ](https://go.proctoru.com/) to schedule the exam.     2. Once account is created, from the **My Exams** page, click the **Schedule New Exam** button to schedule the exam date and time.     3. Fill in the **institution** , **term** and **exam** from the dropdown menus, and click the **Find Reservations** button.     4. On the **Schedule Exam** page, select a reservation time from the calendar at the bottom of the screen. **IMPORTANT: If you do not see any reservations listed, please select the "View All" radio button next to "Filter Results" to display exam times that are outside your specified preference.**    5. Click the **BOOK IT** button next to your desired exam appointment time.     6. Confirm your selection on the next page and click the **Proceed to Cart** button when you are ready to continue.     7. In your cart, click the **Proceed to Checkout** button.     8. Enter the appropriate credit card information and click **Make Payment** .     9. You will see an exam confirmation page and will receive an email message with your scheduled exam information.   ##  How to Take the Proctored Exam    1. At the date and time of the exam appointment, login to the [ University of Illinois ProctorU portal page ](http://proctoru.com/portal/illinois/) .     2. After logging in, you will see a countdown to the proctored exam time at the top of the page. Prior to the exam appointment, you may reschedule using the Reschedule button.     3. At the appointment time, a **Start** button will appear next to the appointment. Click the **Start** button, and you will be connected to a proctor who will guide you through the proctored exam process.     4. You should navigate to the password quiz in the course site and open the password quiz for the proctor.     5. Proctor will type in the password in the password quiz for you. Under no circumstances should you type in a password. **Please turn your head around when the proctor enters the password** .     6. You can submit the password quiz after the proctor enters the password.     7. Once the password is accepted, you can start the proctored exam under the proctor's supervision.     8. Proctor must witness you submit the exam. Make sure to follow the proctor's instructions and remain connected until you are instructed to disconnect by the proctor. If for any reason the session gets disconnected, try to reconnect immediately and if still unable contact ProctorU. If you remain disconnected from the proctor while submitting the exam, an incident report will be submitted to the instructor.   
##  Exam Instructions    * A password quiz precedes and unlocks the proctored exam. The proctor will enter the password for you.     * Under no circumstances should you type in any password or make an attempt in the password quiz.     * **Note: If the exam does not immediately unlock after the Proctor enters the password, please let the Proctor know that you are going to refresh the page, which should resolve the issue. Then refresh the page.**  ##  How to Schedule the Proctored Exam  **Note: you need to schedule the exam on ProctorU website at[ https://go.proctoru.com ](https://go.proctoru.com/) yourself by following the instructions below. **    1. If this is your first time scheduling an exam with ProctorU, click on the **Sign Up** link on the [ University of Illinois ProctorU portal page ](http://proctoru.com/portal/illinois) to create a new user account and ProctorU log-in password. It is recommended to use the University of Illinois email address to register for the account. If you have a ProctorU account already, you can skip this step and go to [ https://go.proctoru.com ](https://go.proctoru.com/) to schedule the exam.     2. Once account is created, from the **My Exams** page, click the **Schedule New Exam** button to schedule the exam date and time.     3. Fill in the **institution** , **term** and **exam** from the dropdown menus, and click the **Find Reservations** button.     4. On the **Schedule Exam** page, select a reservation time from the calendar at the bottom of the screen. **IMPORTANT: If you do not see any reservations listed, please select the "View All" radio button next to "Filter Results" to display exam times that are outside your specified preference.**    5. Click the **BOOK IT** button next to your desired exam appointment time.     6. Confirm your selection on the next page and click the **Proceed to Cart** button when you are ready to continue.     7. In your cart, click the **Proceed to Checkout** button.     8. Enter the appropriate credit card information and click **Make Payment** .     9. You will see an exam confirmation page and will receive an email message with your scheduled exam information.   ##  How to Take the Proctored Exam    1. At the date and time of the exam appointment, login to the [ University of Illinois ProctorU portal page ](http://proctoru.com/portal/illinois/) .     2. After logging in, you will see a countdown to the proctored exam time at the top of the page. Prior to the exam appointment, you may reschedule using the Reschedule button.     3. At the appointment time, a **Start** button will appear next to the appointment. Click the **Start** button, and you will be connected to a proctor who will guide you through the proctored exam process.     4. You should navigate to the password quiz in the course site and open the password quiz for the proctor.     5. Proctor will type in the password in the password quiz for you. Under no circumstances should you type in a password. **Please turn your head around when the proctor enters the password** .     6. You can submit the password quiz after the proctor enters the password.     7. Once the password is accepted, you can start the proctored exam under the proctor's supervision.     8. Proctor must witness you submit the exam. Make sure to follow the proctor's instructions and remain connected until you are instructed to disconnect by the proctor. If for any reason the session gets disconnected, try to reconnect immediately and if still unable contact ProctorU. If you remain disconnected from the proctor while submitting the exam, an incident report will be submitted to the instructor.   
[SOUND] >> This lecture is about Natural Language of Content Analysis. As you see from this picture, this is really the first stepto process any text data. Text data are in natural languages. So computers have to understandnatural languages to some extent, in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is naturallanguage processing, which is the main technique for processingnatural language to obtain understanding. The second is the state ofthe art of NLP which stands for natural language processing. Finally we're going to cover the relationbetween natural language processing and text retrieval. First, what is NLP? Well the best way to explain itis to think about if you see a text in a foreign languagethat you can understand. Now what do you have to do inorder to understand that text? This is basically whatcomputers are facing. So looking at the simple sentence likea dog is chasing a boy on the playground. We don't have any problemsunderstanding this sentence. But imagine what the computer wouldhave to do in order to understand it. Well in general,it would have to do the following. First, it would have to know dogis a noun, chasing's a verb, etc. So this is called lexical analysis,or part-of-speech tagging, and we need to figure out the syntacticcategories of those words. So that's the first step. After that, we're going to figureout the structure of the sentence. So for example, here it shows that A and the dog would go togetherto form a noun phrase. And we won't have dog and is to go first. And there are some structuresthat are not just right. But this structure shows what we mightget if we look at the sentence and try to interpret the sentence. Some words would go together first, and then they will go togetherwith other words. So here we show we have noun phrasesas intermediate components, and then verbal phrases. Finally we have a sentence. And you get this structure. We need to do something calleda semantic analysis, or parsing. And we may have a parseraccompanying the program, and that would automaticallycreated this structure. At this point you would knowthe structure of this sentence, but still you don't knowthe meaning of the sentence. So we have to go furtherto semantic analysis. In our mind we usually can map such a sentence to what we alreadyknow in our knowledge base. For example, you might imaginea dog that looks like that. There's a boy andthere's some activity here. But for a computer would haveto use symbols to denote that. We'd use a symbol (d1) to denote a dog. And (b)1 can denote a boy andthen (p)1 can denote a playground. Now there is also a chasingactivity that's happening here so we have a relationship chasingthat connects all these symbols. So this is how a computer would obtainsome understanding of this sentence. Now from this representation we couldalso further infer some other things, and we might indeed naturally think ofsomething else when we read a text and this is called inference. So for example, if you believethat if someone's being chased and this person might be scared,but with this rule, you can see computers could alsoinfer that this boy maybe scared. So this is some extra knowledgethat you'd infer based on some understanding of the text. You can even go further to understandwhy the person say at this sentence. So this has to do as a use of language. This is called pragmatic analysis. In order to understand the speakactor of a sentence, right? We say something tobasically achieve some goal. There's some purpose there. And this has to do withthe use of language. In this case the person who said this sentence might be remindinganother person to bring back the dog. That could be one possible intent. To reach this level ofunderstanding would require all of these steps anda computer would have to go through all these steps in order to completelyunderstand this sentence. Yet we humans have no troublewith understanding that, we instantly would get everything. There is a reason for that. That's because we have a largeknowledge base in our brain and we can use common sense knowledgeto help interpret the sentence. Computers unfortunately are hardto obtain such understanding. They don't have such a knowledge base. They are still incapable of doingreasoning and uncertainties, so that makes natural languageprocessing difficult for computers. But the fundamental reason why naturallanguage processing is difficult for computers is simply because naturallanguage has not been designed for computers. Natural languages are designed forus to communicate. There are other languages designed forcomputers. For example, programming languages. Those are harder for us, right? So natural languages is designed tomake our communication efficient. As a result,we omit a lot of common sense knowledge because we assume everyoneknows about that. We also keep a lot of ambiguities becausewe assume the receiver or the hearer could know how to decipher an ambiguous wordbased on the knowledge or the context. There's no need to demand differentwords for different meanings. We could overload the same word withdifferent meanings without the problem. Because of these reasons this makes everystep in natural language of processing difficult for computers,ambiguity is the main difficulty. And common sense and reasoning isoften required, that's also hard. So let me give you someexamples of challenges here. Consider the word level ambiguity. The same word can havedifferent syntactic categories. For example design can be a noun ora verb. The word of root mayhave multiple meanings. So square root in math sense orthe root of a plant. You might be able to thinkabout it's meanings. There are also syntactical ambiguities. For example, the main topic of thislecture, natural language processing, can actually be interpreted in twoways in terms of the structure. Think for a moment andsee if you can figure that out. We usually think of this asprocessing of natural language, but you could also think of this as dosay, language processing is natural. So this is an exampleof synaptic ambiguity. What we have different isstructures that can be applied to the same sequence of words. Another common example of an ambiguoussentence is the following. A man saw a boy with a telescope. Now in this case the question is,who had a telescope. This is called a prepositionalphrase attachment ambiguity or PP attachment ambiguity. Now we generally don't have a problem withthese ambiguities because we have a lot of background knowledge to helpus disambiguate the ambiguity. Another example of difficultyis anaphora resolution. So think about the sentence Johnpersuaded Bill to buy a TV for himself. The question here is doeshimself refer to John or Bill? So again this is something thatyou have to use some background or the context to figure out. Finally, presuppositionis another problem. Consider the sentence,he has quit smoking. Now this obviously impliesthat he smoked before. So imagine a computer wants to understandall these subtle differences and meanings. It would have to use a lot ofknowledge to figure that out. It also would have to maintain a largeknowledge base of all the meanings of words and how they are connected to ourcommon sense knowledge of the world. So this is why it's very difficult. So as a result, we are steep not perfect, in fact far from perfect in understandingnatural language using computers. So this slide sort of gains a simplifiedview of state of the art technologies. We can do part of speechtagging pretty well, so I showed 97% accuracy here. Now this number is obviouslybased on a certain dataset, so don't take this literally. This just shows that wecan do it pretty well. But it's still not perfect. In terms of parsing,we can do partial parsing pretty well. That means we can get noun phrasestructures, or verb phrase structure, or some segment of the sentence, and this dude correct them interms of the structure. And in some evaluation results,we have seen above 90% accuracy in terms of partialparsing of sentences. Again, I have to say these numbersare relative to the dataset. In some other datasets,the numbers might be lower. Most of the existing work has beenevaluated using news dataset. And so a lot of these numbers are more orless biased toward news data. Think about social media data,the accuracy likely is lower. In terms of a semantical analysis, we are far from being able to doa complete understanding of a sentence. But we have some techniquesthat would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that canallow us to extract the entities and relations mentioned in text articles. For example,recognizing dimensions of people, locations, organizations, etc in text. So this is called entity extraction. We may be able to recognize the relations. For example,this person visited that place or this person met that person orthis company acquired another company. Such relations can be extracted by using the computer currentNatural Language Processing techniques. They're not perfect butthey can do well for some entities. Some entities are harder than others. We can also do word sensedisintegration to some extend. We have to figure out whether this word inthis sentence would have certain meaning in another context the computer couldfigure out, it has a different meaning. Again, it's not perfect, butyou can do something in that direction. We can also do sentiment analysis, meaning, to figure out whethera sentence is positive or negative. This is especially useful forreview analysis, for example. So these are examplesof semantic analysis. And they help us to obtain partialunderstanding of the sentences. It's not giving us a completeunderstanding, as I showed it before, for this sentence. But it would still help us gainunderstanding of the content. And these can be useful. In terms of inference,we are not there yet, probably because of the general difficultyof inference and uncertainties. This is a general challengein artificial intelligence. Now that's probably also becausewe don't have complete semantical representation fornatural [INAUDIBLE] text. So this is hard. Yet in some domains perhaps,in limited domains when you have a lot of restrictions on the word uses, you may beable to perform inference to some extent. But in general we can notreally do that reliably. Speech act analysis is alsofar from being done and we can only do that analysis forvery special cases. So this roughly gives you someidea about the state of the art. And then we also talk a littlebit about what we can't do, and so we can't even do 100%part of speech tagging. Now this looks like a simple task, but think about the example here,the two uses of off may have different syntactic categories if youtry to make a fine grained distinctions. It's not that easy to figureout such differences. It's also hard to dogeneral complete parsing. And again, the same sentencethat you saw before is example. This ambiguity can be very hard todisambiguate and you can imagine example where you have to use a lot of knowledgein the context of the sentence or from the background, in order to figureout who actually had the telescope. So although the sentence looks verysimple, it actually is pretty hard. And in cases when the sentence isvery long, imagine it has four or five prepositional phrases, and thereare even more possibilities to figure out. It's also harder to do precisedeep semantic analysis. So here's an example. In the sentence "John owns a restaurant."How do we define owns exactly? The word own,it is something that we can understand but it's very hard to precisely describethe meaning of own for computers. So as a result we have a robust anda general Natural Language Processing techniquesthat can process a lot of text data. In a shallow way,meaning we only do superficial analysis. For example, parts of speech tagging or apartial parsing or recognizing sentiment. And those are not deep understanding, because we're not really understandingthe exact meaning of the sentence. On the other hand of the deepunderstanding techniques tend not to scale up well, meaning that they wouldfill only some restricted text. And if you don't restrictthe text domain or the use of words, then thesetechniques tend not to work well. They may work well based on machinelearning techniques on the data that are similar to the training datathat the program has been trained on. But they generally wouldn't work well onthe data that are very different from the training data. So this pretty much summarizes the stateof the art of Natural Language Processing. Of course, within such a short amountof time we can't really give you a complete view of NLP,which is a big field. And I'd expect to see multiple courses onNatural Language Processing topic itself. But because of its relevance to the topicthat we talk about, it's useful for you to know the background in caseyou happen to be exposed to that. So what does that mean for Text Retrieval? Well, in Text Retrieval weare dealing with all kinds of text. It's very hard to restricttext to a certain domain. And we also are often dealingwith a lot of text data. So that means The NLP techniques mustbe general, robust, and efficient. And that just implies today we can onlyuse fairly shallow NLP techniques for text retrieval. In fact, most search engines today use somethingcalled a bag of words representation. Now, this is probably the simplestrepresentation you can possibly think of. That is to turn text datainto simply a bag of words. Meaning we'll keep individual words, butwe'll ignore all the orders of words. And we'll keep duplicatedoccurrences of words. So this is called a bagof words representation. When you represent text in this way,you ignore a lot of valid information. That just makes it harder to understandthe exact meaning of a sentence because we've lost the order. But yet this representation tendsto actually work pretty well for most search tasks. And this was partly because the searchtask is not all that difficult. If you see matching of some ofthe query words in a text document, chances are that that document is aboutthe topic, although there are exceptions. So in comparison of some other tasks, for example, machine translation would requireyou to understand the language accurately. Otherwise the translation would be wrong. So in comparison such tasksare all relatively easy. Such a representation is often sufficientand that's also the representation that the major search engines today,like a Google or Bing are using. Of course, I put in parentheses butnot all, of course there are many queries that are not answered well bythe current search engines, and they do require the replantation thatwould go beyond bag of words replantation. That would require more naturallanguage processing to be done. There was another reason why wehave not used the sophisticated NLP techniques in modern search engines. And that's because someretrieval techniques actually, naturally solved the problem of NLP. So one example is wordsense disintegration. Think about a word like Java. It could mean coffee orit could mean program language. If you look at the word anome,it would be ambiguous, but when the user uses the word in the query,usually there are other words. For example, I'm looking forusage of Java applet. When I have applet there,that implies Java means program language. And that contest can help usnaturally prefer documents which Java is referringto program languages. Because those documents wouldprobably match applet as well. If Java occurs in thatdocuments where it means coffee then you would never match applet orwith very small probability. So this is the case whensome retrieval techniques naturally achieve the goal of word. Another example is some technique called feedback which we will talk aboutlater in some of the lectures. This technique would allow us to addadditional words to the query and those additional words couldbe related to the query words. And these words can help matchingdocuments where the original query words have not occurred. So this achieves, to some extent,semantic matching of terms. So those techniques also helped us bypass some of the difficultiesin natural language processing. However, in the long run we still needa deeper natural language processing techniques in order to improve theaccuracy of the current search engines. And it's particularly needed forcomplex search tasks. Or for question and answering. Google has recently launched a knowledgegraph, and this is one step toward that goal, because knowledge graph wouldcontain entities and their relations. And this goes beyond the simplebag of words replantation. And such technique should help usimprove the search engine utility significantly, although this is the opentopic for research and exploration. In sum, in this lecture wetalked about what is NLP and we've talked about the stateof that techniques. What we can do, what we cannot do. And finally, we also explain whythe bag of words replantation remains the dominant replantationused in modern search engines, even though deeper NLP would be needed forfuture search engines. If you want to know more, you can takea look at some additional readings. I only cited one here andthat's a good starting point. Thanks. [MUSIC]
[SOUND] In this lecture, we are going to talk about howto improve the instantiation of the vector space model. This is a continued discussionof the vector space model. We're going to focus on how to improvethe instantiation of this model. In the previous lecture, you have seen that with simpleinstantiations of the vector space model, we can come up with a simple scoringfunction that would give us basically an account of how many unique queryterms are matched in the document. We also have seen that this functionhas a problem, as shown on this slide. In particular,if you look at these three documents, they will all get the same score becausethey match the three unique query words. But intuitively we would liked4 to be ranked above d3, and d2 is really not relevant. So the problem here is that this functioncouldn't capture the following heuristics. First, we would like to givemore credit to d4 because it matched presidential more times than d3. Second, intuitively, matching presidentialshould be more important than matching about, because about is a verycommon word that occurs everywhere. It doesn't really carry that much content. So in this lecture, let's see how we can improve the modelto solve these two problems. It's worth thinking at this pointabout why do we have these problems? If we look back at assumptions we havemade while instantiating the vector space model,we'll realize that the problem is really coming fromsome of the assumptions. In particular, it has to do with how weplaced the vectors in the vector space. So then naturally,in order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different waysto instantiate the vector space model. In particular, we have to placethe vectors in a different way. So let's see how we can improve this. One natural thought is in order toconsider multiple times of a term in the document, we should consider the term frequencyinstead of just the absence or presence. In order to consider the differencebetween a document where a query term occurred multiple times and onewhere the query term occurred just once, we have to consider the term frequency,the count of a term in the document. In the simplest model, we only modeledthe presence and absence of a term. We ignored the actual number of timesthat a term occurs in a document. So let's add this back. So we're going to thenrepresent a document by a vector with term frequency as element. So that is to say, now the elementsof both the query vector and the document vector will not be 0 or1s, but instead they will be the counts ofa word in the query or the document. So this would bring in additionalinformation about the document, so this can be seen as more accuraterepresentation of our documents. So now let's see what the formulawould look like if we change this representation. So as you'll see on this slide,we still use dot product. And so the formula looksvery similar in the form. In fact, it looks identical. But inside the sum, of course,x i and y i are now different. They are now the counts of word i in the query and in the document. Now at this point I also suggest youto pause the lecture for a moment and just to think about how we can interpretthe score of this new function. It's doing something very similarto what the simplest VSM is doing. But because of the change of the vector, now the new score hasa different interpretation. Can you see the difference? And it has to do with the considerationof multiple occurrences of the same term in a document. More importantly, we would like to knowwhether this would fix the problems of the simplest vector space model. So let's look at this example again. So suppose we change the vectorrepresentation to term frequency vectors. Now let's look at thesethree documents again. The query vector is the samebecause all these words occurred exactly once in the query. So the vector is still a 01 vector. And in fact, d2 is also essentiallyrepresenting the same way because none of these wordshas been repeated many times. As a result,the score is also the same, still 3. The same is true for d3,and we still have a 3. But d4 would be different, becausenow presidential occurred twice here. So the ending for presidential in thedocument vector would be 2 instead of 1. As a result, now the score ford4 is higher. It's a 4 now. So this means by using term frequency, we can now rank d4 above d2 andd3, as we hoped to. So this solved the problem with d4. But we can also see that d2 andd3 are still filtering the same way. They still have identical scores,so it did not fix the problem here. So how can we fix this problem? Intuitively, we would liketo give more credit for matching presidential than matching about. But how can we solvethe problem in a general way? Is there any way to determinewhich word should be treated more importantly andwhich word can be basically ignored? About is such a word which does notreally carry that much content. We can essentially ignore that. We sometimes call sucha word a stock word. Those are generally very frequent andthey occur everywhere. Matching it doesn't really mean anything. But computationally howcan we capture that? So again, I encourage you tothink a little bit about this. Can you came up with any statisticalapproaches to somehow distinguish presidential from about? Now if you think about it for a moment, you'll realize that one difference isthat a word like above occurs everywhere. So if you count the occurrence ofthe word in the whole collection, then we will see that about has muchhigher frequency than presidential, which tends to occuronly in some documents. So this idea suggeststhat we could somehow use the global statistics of terms or some other informationto trying to down-weight the element of about ina vector representation of d2. At the same time,we hope to somehow increase the weight of presidentialin the vector of d3. If we can do that, then we canexpect that d2 will get the overall score to be less than 3 whiled3 will get the score above 3. Then we would be able torank d3 on top of d2. So how can we do this systematically? Again, we can rely onsome statistical count. And in this case, the particular ideais called inverse document frequency. Now we have seen documentfrequency as one signal used in the modern retrieval functions. We discussed this in a previous lecture. So here is the specific way of using it. Document frequency is the count ofdocuments that contain a particular term. Here we say inverse document frequencybecause we actually want to reward a word that doesn't occur in many documents. And so the way to incorporate thisinto our vector representation is then to modify the frequencycount by multiplying it by the IDF of the corresponding word,as shown here. If we can do that,then we can penalize common words, which generally have a lower IDF, and reward rare words,which will have a higher IDF. So more specifically, the IDF can be defined asthe logarithm of M+1 divided by k, where M is the total number of documentsin the collection, k is the DF or document frequency, the total numberof documents containing the word W. Now if you plot thisfunction by varying k, then you would see the curvewould look like this. In general, you can see itwould give a higher value for a low DF word, a rare word. You can also see the maximum valueof this function is log of M+1. It would be interesting for you to thinkabout what's the minimum value for this function. This could be an interesting exercise. Now the specific functionmay not be as important as the heuristic to simplypenalize popular terms. But it turns out that this particularfunction form has also worked very well. Now whether there's a betterform of function here is the open research question. But it's also clear that ifwe use a linear penalization, like what's shown here with this line, then it may not be asreasonable as the standard IDF. In particular, you can seethe difference in the standard IDF, and we somehow havea turning point of here. After this point, we're going to say theseterms are essentially not very useful. They can be essentially ignored. And this makes sense whenthe term occurs so frequently and let's say a term occurs in morethan 50% of the documents, then the term is unlikely very importantand it's basically a common term. It's not very importantto match this word. So with the standard IDF you cansee it's basically assumed that they all have low weights. There's no difference. But if you look atthe linear penalization, at this point that thereis still some difference. So intuitively we'd want tofocus more on the discrimination of low DF words ratherthan these common words. Well, of course,which one works better still has to be validated by using the empiricallycorrelated dataset. And we have to use users tojudge which results are better. So now let's see howthis can solve problem 2. So now let's look atthe two documents again. Now without the IDF weighting before,we just have term frequency vectors. But with IDF weighting wenow can adjust the TF weight by multiplying with the IDF value. For example,here we can see is adjustment and in particular for about there's adjustmentby using the IDF value of about, which is smaller than the IDFvalue of presidential. So if you look at these,the IDF will distinguish these two words. As a result, adjustment here would belarger, would make this weight larger. So if we score with these new vectors,then what would happen is that, of course,they share the same weights for news and campaign, but the matching ofabout will discriminate them. So now as a result of IDF weighting,we will have d3 to be ranked above d2 because it matched a rare word,whereas d2 matched a common word. So this shows that the IDFweighting can solve problem 2. So how effective is this model ingeneral when we used TF-IDF weighting? Well, let's look at all thesedocuments that we have seen before. These are the new scoresof the new documents. But how effective is this new weightingmethod and new scoring function point? So now let's see overall how effectiveis this new ranking function with TF-IDF weighting. Here we show all the five documentsthat we have seen before, and these are their scores. Now we can see the scores for the first four documents hereseem to be quite reasonable. They are as we expected. However, we also see a newproblem because now d5 here, which did not have a very high scorewith our simplest vector space model, now actually has a very high score. In fact, it has the highest score here. So this creates a new problem. This is actually a common phenomenonin designing retrieval functions. Basically, when you tryto fix one problem, you tend to introduce other problems. And that's why it's very tricky howto design effective ranking function. And what's the best ranking functionis their open research question. Researchers are still working on that. But in the next few lectures we're goingto also talk about some additional ideas to further improve this model andtry to fix this problem. So to summarize this lecture, we've talkedabout how to improve the vector space model, andwe've got to improve the instantiation of the vector space modelbased on TD-IDF weighting. So the improvement is mostly onthe placement of the vector where we give high weight to a term thatoccurred many times in a document but infrequently in the whole collection. And we have seen that thisimproved model indeed looks better than the simplestvector space model. But it also still has some problems. In the next lecture we're going to look athow to address these additional problems. [MUSIC]
[MUSIC] This lecture is about Evaluation ofText Retrieval Systems In the previous lectures, we have talked aboutthe a number of Text Retrieval Methods, different kinds of ranking functions. But how do we know whichone works the best? In order to answer this question, we have to compare them and that means wehave to evaluate these retrieval methods. So this is the main topic of this lecture. First, lets think about whydo we have to do evaluation? I already give one reason. That is, we have to use evaluationto figure out which retrieval method works better. Now this is very important foradvancing our knowledge. Otherwise, we wouldn't know whether a newidea works better than an old idea. In the beginning of this course, we talkedabout the problem of text retrieval. We compare it with data base retrieval. There we mentioned that text retrievalis an empirically defined problem. So evaluation must rely on users. Which system works better,would have to be judged by our users. So, this becomes a verychallenging problem because how can we get usersinvolved in the evaluation? How can we do a fair comparisonof different method? So just go back to the reasons forevaluation. I listed two reasons here. The second reason, is basically what Ijust said, but there is also another reason which is to assess the actualutility of a Text Regional system. Imagine you're building yourown such annual applications, it would be interesting knowing how wellyour search engine works for your users. So in this case, matches must reflect the utility tothe actual users in real occasion. And typically, this has to bedone by using user starters and using the real search engine. In the second case, or the second reason, the measures actually all need to collatedwith the utility to actually use this. Thus, they don't have to accuratelyreflect the exact utility to users. So the measure only needs to be goodenough to tell which method works better. And this is usually donethrough a test collection. And this is the main idea that we'llbe talking about in this course. This has been very important forcomparing different algorithms and for improving searchengine system in general. So let's talk about what to measure. There are many aspects of searchingthat we can measure, we can evaluate. And here,I listed the three major aspects. One, is effectiveness or accuracy. How accurate are the search results? In this case, we're measuring a system'scapability of ranking relevant documents on top of non relevant ones. The second, is efficiency. How quickly can you get the results? How much computing resourcesare needed to answer a query? In this case, we need to measure the spaceand time overhead of the system. The third aspect is usability. Basically the question is,how useful is a system for new user tasks. Here, obviously, interfaces and many other things also important andwould typically have to do user studies. Now in this course, we're going totalk mostly about effectiveness and accuracy measures. Because the efficiency and usability dimensions are notreally unique to search engines. And so they are needed forwithout any other software systems. And there is also good coverageof such and other causes. But how to evaluate searchengine's quality or accuracy is something unique to text retrieval andwe're going to talk a lot about this. The main idea that people have proposedbefore using a test set to evaluate the text retrieval algorithm is calledthe Cranfield Evaluation Methodology. This one actually was developeda long time ago, developed in 1960s. It's a methodology forlaboratory test of system components. Its sampling methodology that hasbeen very useful, not just for search engine evaluation. But also for evaluating virtuallyall kinds of empirical tasks, and for example in natural language processingor in other fields where the problem is empirical to find, we typicallywould need to use such a methodology. And today with the big data challenging with the use of machinelearning everywhere. This methodology has been very popular,but it was first developed for a search engine application in the 1960s. So the basic idea of this approach isto build a reusable test collection and define measures. Once such a test collection is built,it can be used again and again to test different algorithms. And we're going to define measuresthat allow you to quantify performance of a system and algorithm. So how exactly will this work? Well we can do have a sample collection ofdocuments and this is adjusted to simulate the real document collectionin the search application. We're going to also have a sampleset of queries, or topics. This is a little simulatorthat uses queries. Then, we'll have to havethose relevance judgments. These are judgments of which documentsshould be returned for which queries. Ideally, they have to be made byusers who formulated the queries. Because those are the people that knowexactly what documents would be used for. And finally, we have to have matches for quantify how well our system's resultmatches the ideal ranked list. That would be constructed baseon user's relevance judgements. So this methodology is very useful forstarting retrieval algorithms, because the test can be reused many times. And it will also provide a faircomparison for all the methods. We have the same criteria or same dataset to be used tocompare different algorithms. This allows us to comparea new algorithm with an old algorithm that was divided manyyears ago, by using the same standard. So this is the illustration of this works,so as I said, we need our queries that are showing here. We have Q1, Q2 etc. We also need the documents andthat's called the document caching and on the right side you will seewe need relevance judgments. These are basically the binary judgmentsof documents with respect to a query. So for example,D1 is judged as being relevant to Q1, D2 is judged as being relevant as well,and D3 is judged as not relevant. And the Q1 etc. These will be created by users. Once we have these, andwe basically have a test collection. And then if you have two systems,you want to compare them, then you can just run eachsystem on these queries and the documents andeach system would then return results. Let's say if the queries Q1 andthen we would have the results here. Here I show R sub A asthe results from system A. So this is, remember we talked about task of computing approximationof the relevant document set. R sub A is system A's approximation here. And R sub B is system B'sapproximation of relevant documents. Now, let's take a look at these results. So which is better? Now imagine if a user,which one would you like? Now let's take a look at the both results. And there are some differences and there are some documents thatare returned by both systems. But if you look at the results,you will feel that maybe A is better in the sense that we don'thave many number element documents. And among the three documents returned,the two of them are relevant. So that's good, it's precise. On the other hand one councilsay maybe B is better, because we've got all ofthem in the documents. We've got three instead of two. So which one is better andhow do we quantify this? Well, obviously this questionhighly depends on a user's task. It depends on users as well. You might even imagine forsome users may be system A is better. If the user is not interested ingetting all the random documents. Right, in this case the user doesn'thave to read a million users will see most of the relevant documents. On the other hand,one can also imagine the user might need to have as many randomdocuments as possible. For example, if you're doing a literaturesurvey you might be in the sigma category, and you might find thatsystem B is better. So in the case, we will have to alsodefine measures that will quantify them. And we might need it to define multiplemeasures because users have different perspectives of looking at the results. [MUSIC]
[SOUND]This lecture is aboutthe Probabilistic Retrieval Model. In this lecture, we're going to continue the discussionof the Text Retrieval Methods. We're going to look at another kind ofvery different way to design ranking functions than the Vector Space Modelthat we discussed before. In probabilistic models,we define the ranking function, based on the probability that thisdocument is relevant to this query. In other words, we introducea binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observationsfrom random variables. Note that in the vector-based models,we assume they are vectors, but here we assume they are the dataobserved from random variables. And so, the problem of retrieval becomesto estimate the probability of relevance. In this category of models,there are different variants. The classic probabilistic model hasled to the BM25 retrieval function, which we discussed in inthe vectors-based model because its a form is actuallysimilar to a backwards space model. In this lecture,we will discuss another sub class in this P class called a languagemodeling approaches to retrieval. In particular, we're going to discussthe query likelihood retrieval model, which is one of the most effectivemodels in probabilistic models. There was also another line calledthe divergence from randomness model which has led to the PL2 function, it's also one of the most effectivestate of the art retrieval functions. In query likelihood, our assumptionis that this probability of relevance can be approximated by the probabilityof query given a document and relevance. So intuitively, this probability justcaptures the following probability. And that is if a user likes document d,how likely would the user enter query q ,inorder to retrieve document d? So we assume that the user likes d,because we have a relevance value here. And then we ask the question about howlikely we'll see this particular query from this user? So this is the basic idea. Now, to understand this idea,let's take a look at the general idea or the basic idea ofProbabilistic Retrieval Models. So here, I listed some imaginedrelevance status values or relevance judgments of queries anddocuments. For example, in this line, it shows that q1 is a querythat the user typed in. And d1 is a documentthat the user has seen. And 1 means the user thinksd1 is relevant to q1. So this R here can be also approximatedby the click-through data that a search engine can collect by watching how youinteracted with the search results. So in this case, let's saythe user clicked on this document. So there's a 1 here. Similarly, the user clicked on d2 also,so there is a 1 here. In other words,d2 is assumed to be relevant to q1. On the other hand,d3 is non-relevant, there's a 0 here. And d4 is non-relevant and then d5 isagain, relevant, and so on and so forth. And this part, maybe,data collected from a different user. So this user typed in q1 and then foundthat the d1 is actually not useful, so d1 is actually non-relevant. In contrast, here we see it's relevant. Or this could be the same query typedin by the same user at different times. But d2 is also relevant, etc. And then here,we can see more data about other queries. Now, we can imagine wehave a lot of such data. Now we can ask the question, how can we then estimatethe probability of relevance? So how can we compute thisprobability of relevance? Well, intuitively that just means if we look at all the entrieswhere we see this particular d and this particular q, how likely we'llsee a one on this other column. So basically that just means thatwe can just collect the counts. We can first count how manytimes we have seen q and d as a pair in this table andthen count how many times we actually have also seen1 in the third column. And then, we just compute the ratio. So let's take a look atsome specific examples. Suppose we are trying to compute thisprobability for d1, d2 and d3 for q1. What is the estimated probability? Now, think about that. You can pause the video if needed. Try to take a look at the table. And try to give yourestimate of the probability. Have you seen that,if we are interested in q1 and d1, we'll be looking at these two pairs? And in both cases, well, actually, in one of the cases, the userhas said this is 1, this is relevant. So R = 1 in only one of the two cases. In the other case, it's 0. So that's one out of two. What about the d1 and the d2? Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1. So it's a two out of two andso on and so forth. So you can see with this approach, we can actually score these documents forthe query, right? We now have a score for d1,d2 and d3 for this query. And we can simply rank thembased on these probabilities and so that's the basic ideaprobabilistic retrieval model. And you can see it makes a lot of sense,in this case, it's going to rank d2 aboveall the other documents. Because in all the cases,when you have c and q1 and d2, R = 1. The user clicked on this document. So this also should show thatwith a lot of click-through data, a search engine can learn a lot fromthe data to improve their search engine. This is a simple examplethat shows that with even with small amount of entries here we canalready estimate some probabilities. These probabilities would give ussome sense about which document might be more relevant or more usefulto a user for typing this query. Now, of course, the problems that wedon't observe all the queries and all the documents andall the relevance values, right? There would be a lot of unseen documents,in general, we have only collected the data from thedocuments that we have shown to the users. And there are even more unseen queriesbecause you cannot predict what queries will be typed in by users. So obviously,this approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic ideaof probabilistic retrieval model and it makes sense intuitively. So what do we do in such a case whenwe have a lot of unseen documents and unseen queries? Well, the solutions that we haveto approximate in some way. So in this particular case calleda query likelihood retrieval model, we just approximate this byanother conditional probability. p(q given d, R=1). So in the condition part, we assume thatthe user likes the document because we have seen that the userclicked on this document. And this part shows thatwe're interested in how likely the user wouldactually enter this query. How likely we will see thisquery in the same row. So note that here, we have madean interesting assumption here. Basically, we're going to do, assume thatwhether the user types in this query has something to do with whetheruser likes the document. In other words,we actually make the following assumption. And that is a user formulates a querybased on an imaginary relevant document. Where if you just look at thisas conditional probability, it's not obvious weare making this assumption. So what I really meant is thatto use this new conditional probability to help us score,then this new conditional probability will have to somehowbe able to estimate this conditional probability withoutrelying on this big table. Otherwise we would be havingsimilar problems as before, and by making this assumption,we have some way to bypass this big table, and try to just model how the userformulates the query, okay? So this is how you cansimplify the general model so that we can derive a specificrelevant function later. So let's look at how this model work forour example. And basically, what we are going to do in this caseis to ask the following question. Which of these documents is mostlikely the imaginary relevant document in the user's mind whenthe user formulates this query? So we ask this question and we quantifythe probability and this probability is a conditional probability of observingthis query if a particular document is in fact the imaginary relevantdocument in the user's mind. Here you can see we've computed allthese query likelihood probabilities. The likelihood of queriesgiven each document. Once we have these values, we can then rank these documentsbased on these values. So to summarize, the general ideaof modern relevance in the proper risk model is to assume the we introducea binary random variable R, here. And then, let the scoring function be definedbased on this conditional probability. We also talked about approximatingthis by using the query likelihood. And in this case we have a rankingfunction that's basically based on the probability ofa query given the document. And this probability should be interpretedas the probability that a user who likes document d, would pose query q. Now, the question of course is, how dowe compute this conditional probability? At this in general has to do with howyou compute the probability of text, because q is a text. And this has to do with a modelcalled a Language Model. And these kind of modelsare proposed to model text. So more specifically, we will bevery interested in the following conditional probabilityas is shown in this here. If the user liked this document,how likely the user would pose this query. And in the next lecture we're going to do, giving introduction to languagemodels that we can see how we can model text that was a probablerisk model, in general. [MUSIC]
[SOUND]This lecture is about the feedbackin text retrieval. So in this lecture, we will continue withthe discussion of text retrieval methods. In particular, we're going to talkabout the feedback in text retrieval. This is a diagram that showsthe retrieval process. We can see the user would type in a query. And then, the query would besent to a retrieval engine or search engine, andthe engine would return results. These results would be issued to the user. Now, after the user hasseen these results, the user can actually make judgements. So for example, the user says,well, this is good and this document is not very useful andthis is good again, etc. Now, this is called a relevance judgmentor relevance feedback because we've got some feedback information fromthe user based on the judgements. And this can be very useful to the system, knowing what exactly isinteresting to the user. So the feedback module wouldthen take this as input and also use the document collectionto try to improve ranking. Typically it would involveupdating the query so the system can now render the resultsmore accurately for the user. So this is called relevance feedback. The feedback is based on relevancejudgements made by the users. Now, these judgements are reliable but the users generally don't want to makeextra effort unless they have to. So the down side is that it involvessome extra effort by the user. There's another form of feedbackcalled pseudo relevance feedback, or blind feedback,also called automatic feedback. In this case, we can see oncethe user has gotten [INAUDIBLE] or in fact we don't have to invoke users. So you can see there'sno user involved here. And we simply assume that the toprank documents to be relevant. Let's say we have assumedtop 10 as relevant. And then, we will then use thisassume the documents to learn and to improve the query. Now, you might wonder, how could this help if we simplyassume the top rank of documents? Well, you can imagine these toprank of documents are actually similar to relevant documentseven if they are not relevant. They look like relevant documents. So it's possible to learn some relatedterms to the query from this set. In fact, you may recall that wetalked about using language model to analyze what association, to learnrelated words to the word of computer. And there, what we did is wefirst use computer to retrieve all the documents that contain computer. So imagine now the queryhere is a computer. And then, the result will be thosedocuments that contain computer. And what we can do then isto take the top n results. They can match computer very well. And we're going to countthe terms in this set. And then, we're going to then usethe background language model to choose the terms that are frequent in this setbut not frequent in the whole collection. So if we make a contrast betweenthese two what we can find is that related to termsto the word computer. As we have seen before. And these related words can then be addedto the original query to expand the query. And this would help us bring the documentsthat don't necessarily match computer but match other words like program andsoftware. So this is very effective forimproving the search result. But of course, pseudo-relevancyvalues are completely unreliable. We have to arbitrarily set a cut off. So there's also something inbetween called implicit feedback. In this case,what we do is we do involve users, but we don't have to askusers to make judgments. Instead, we're going to observe how theuser interacts with the search results. So in this case we'll lookat the clickthroughs. So the user clicked on this one. And the user viewed this one. And the user skipped this one. And the user viewed this one again. Now, this also is a clue about whetherthe document is useful to the user. And we can even assume that we'regoing to use only the snippet here in this document,the text that's actually seen by the user instead of the actualdocument of this entry. The link they are saying web searchmay be broken but it doesn't matter. If the user tries to fetch thisdocument because of the displayed text we can assume these displayedtext is probably relevant is interesting to you sowe can learn from such information. And this is called interesting feedback. And we can, again,use the information to update the query. This is a very importanttechnique used in modern. Now, think about the Google and Bing and they can collect a lot of useractivities while they are serving us. So they would observe what documentswe click on, what documents we skip. And this information is very valuable. And they can use this toimprove the search engine. So to summarize, we talked aboutthe three kinds of feedback here. Relevant feedback where the usermakes explicit judgements. It takes some user effort, butthe judgment information is reliable. We talk about the pseudo feedback wherewe seem to assume top brand marking will be relevant. We don't have to involve the usertherefore we could do that, actually before we returnthe results to the user. And the third is implicit feedbackwhere we use clickthroughs. Where we involve the users, but the user doesn't have to makeit explicitly their fault. Make judgement. [MUSIC]
[MUSIC] This lecture is aboutthe Learning to Rank. In this lecture, we are going tocontinue talking about web search. In particular we're going to talkabout the using machine learning to combine different featuresto improve the ranking function. So the question that we address inthis lecture is how we can combine many features to generate a single rankingfunction to optimize search results? In the previous lectures we have talkedabout a number of ways to rank documents. We have talked about some retrievalmodels like a BM25 or Query Light Code. They can generate a based this course formatching documents with a query. And we also talked about the linkbased approaches like page rank that can give additional scoresto help us improve ranking. Now the question now is,how can we combine all these features and potentially many otherfeatures to do ranking? And this will be very useful forranking webpages, not only just to improve accuracy, but also to improvethe robustness of the ranking function. So that it's not easy fora spammer to just perturb a one or a few features to promote a page. So the general idea of learningto rank is to use machine learning to combine thisfeatures to optimize the weights on different features to generatethe optimal ranking function. So we will assume that the givena query document pair Q and D, we can define a number of features. And these features can vary fromcontent based features such as a score of the document withrespect to the query according to a retrieval function such as BM25 orQuery Light Hold of punitive commands from a machine orPL2 etcetera. It can also be a link based score like orpage rank score like. It can be also application of retrievalmodels to the ink text of the page. Those are the types of descriptionsof links that point to this page. So, these can all the clues whetherthis document is relevant, or not. We can even include a featuresuch as whether the URL has a tilde because this might beindicator of home page or entry page. So all these features can then be combinedtogether to generate a ranking function. The question is, of course. How can we combine them? In this approach,we simply hypothesize that the probability that this document isn't relevant to thisquery is a function of all these features. So we can hypothesize this that the probability of relevanceis related to these features through a particular form ofthe function that has some parameters. These parameters can control the influence of differentfeatures of the final relevance. Now this is of course just an assumption. Whether this assumption reallymakes sense is a big question and that's they have to empiricallyevaluate the function. But by hypothesizing thatthe relevance is related to these features in the particular way, we canthen combine these features to generate the potential more powerful rankingfunction, a more robust ranking function. Naturally the next question is howdo we estimate those parameters? How do we know which featuresshould have a higher weight, and which features will have lower weight? So this is the task of training orlearning, so in this approach what we willdo is use some training data. Those are the data that havebeen charted by users so that we already knowthe relevant judgments. We already know which documents shouldbe ranked high for which queries. And this information can be basedon real judgments by users or this can also be approximated by justusing click through information, where we can assume the clicked documentsare better than the skipped documents clicked documents are relevant andthe skipped documents are non-relevant. So in general with the fitsuch hypothesize ranking function to the training data meaning that we will try to optimize it'sretrieval accuracy on the training data. And we can adjust these parameters to see how we can optimize the performance ofthe functioning on the training data in terms of some measures such as MAP orNDCG. So the training date wouldlook like a table of tuples. Each tuple has three elements, the query,the document, and the judgement. So it looks very much like ourrelevance judgement that we talked about in the evaluationof retrieval systems. [MUSIC]
No office hour this week due to Fall Break  
[![timeanddate.com](//c.tadst.com/gfx/n/logo/logo-2021--horizontal-color- whitebg.svg)](/)  __  [![timeanddate.com](//c.tadst.com/gfx/n/logo/logo-2021--horizontal-inverted- darkbg.svg)](/ "Home page timeanddate.com")  __  __  Sign in __    * [Home](/)     * [Home Page](/)     * [Newsletter](/newsletter/)     * [About Us](/information/)     * [Contact Us](/information/feedback.html)     * [Site Map](/sitemap.html)     * [Our Articles](/topics/)     * [Account/Settings](/custom/)   * [World Clock](/worldclock/)     * [Main World Clock](/worldclock/)     * [Extended World Clock](/worldclock/full.html)     * [Personal World Clock](/worldclock/personal.html)     * [World Time Lookup ](/worldclock/search.html)     * [UTC Time](/worldclock/timezone/utc)   * [Time Zones](/time/)     * [Time Zones Home](/time)     * [Time Zone Converter](/worldclock/converter.html)     * [International Meeting Planner](/worldclock/meeting.html)     * [Event Time Announcer](/worldclock/fixedform.html)     * [Time Zone Map](/time/map/)     * [Time Zone Abbreviations](/time/zones/)     * [Daylight Saving Time](/time/dst/)     * [Time Changes Worldwide](/time/change/)     * [Time Difference](/time/difference/)     * [Time Zone News](/news/time/)   * [Calendar](/calendar/)     * [Calendars Home](/calendar/info.html)     * [Calendar 2021](/calendar/)     * [Calendar 2022](/calendar/?year=2022)     * [Monthly Calendar](/calendar/monthly.html)     * [Printable Calendar (PDF)](/calendar/create.html)     * [Add Your Own Calendar Events](/calendar/events/)     * [Calendar Creator](/calendar/basic.html)     * [Advanced Calendar Creator](/calendar/custommenu.html)     * [Holidays Worldwide](/holidays/)     * [On This Day in History](/on-this-day/)     * [Months of the Year](/calendar/months/)     * [Days of the Week](/calendar/days/)     * [About Leap Years](/date/leapyear.html)   * [Weather](/weather/)     * [Worldwide](/weather/)     * [Local Weather](/scripts/go.php)     * [2-Week Forecast](/scripts/goweather.php?type=ext)     * [Hour-by-Hour](/scripts/go.php?type=hourly)     * [Past Week](/scripts/go.php?type=historic)     * [Climate](/scripts/go.php?type=climate)   * [Sun & Moon](/astronomy/)     * [Sun & Moon Home](/astronomy)     * [Sun Calculator](/sun/)     * [Moon Calculator](/moon/)     * [Moon Phases](/moon/phases/)     * [Night Sky](/astronomy/night/)     * [Meteor Showers](/astronomy/meteor-shower/)     * [Day and Night Map](/worldclock/sunearth.html)     * [Moon Light World Map](/astronomy/moon/light.html)     * [Eclipses](/eclipse/)     * [Live Streams](/live/)     * [Seasons](/calendar/seasons.html)   * [Timers](/counters/)     * [Timers Home](/counters)     * [Stopwatch](/stopwatch/)     * [Timer](/timer/)     * [Countdown to Any Date](/countdown/create)     * [New Year Countdown](/countdown/newyear)   * [Calculators](/date/)     * [Calculators Home](/date/)     * [Date to Date Calculator (duration)](/date/duration.html)     * [Business Date to Date (exclude holidays)](/date/workdays.html)     * [Date Calculator (add / subtract)](/date/dateadd.html)     * [Business Date (exclude holidays)](/date/weekdayadd.html)     * [Weekday Calculator](/date/weekday.html)     * [Week Number Calculator](/date/weeknumber.html)     * [Distance Calculator](/worldclock/distance.html)     * [Distance Signpost](/worldclock/distances.html)   * [Apps & API](/extra/)     * [iOS Apps](/ios/)     * [Android Apps](/android/)     * [Free Clock](/clocks/free.html)     * [Free Countdown](/clocks/freecountdown.html)     * [API for Developers](/services/api/)   * [Free Fun](/fun/)     * [Free & Fun Home](/fun)     * [Free Clock for Your Site](/clocks/free.html)     * [Free Countdown for Your Site](/clocks/freecountdown.html)     * [Word Clock](/wordclock/)     * [Fun Holidays](/holidays/fun/)     * [Alternative Age Calculator](/date/birthday.html)     * [Date Pattern Calculator](/date/pattern.html)     * [Fun Fact Articles](/topics/fun)   * [My Account __](/custom/)     * [My Account](/custom)     * [My Location](/custom/location.html)     * [My Units](/custom/site.html)     * [My Events](/calendar/events/)     * [My World Clock](/worldclock/personal.html)     * [My Privacy](/custom/privacy.html)     * [Paid Services](/services/)     * [Sign in](/custom/login.html)     * [Register](/custom/create.html)  [Home](/)   [Time Zones](/time/)   Time Zone Converter – Time Difference Calculator  # Time Zone Converter – Time Difference Calculator  Provides time zone conversions taking into account Daylight Saving Time (DST), local time zone and accepts present, past, or future dates.  Advertising  __ Sort By: \-- Custom --↑ City (Asc.)↓ City (Des.)↑ Country (Asc.)↓ Country (Des.)↑ Time (Asc.)↓ Time (Des.)  !  , , *   ()    Holiday: [](/),   :   __ __  × delete  __  UTC   __   ***** Observing [Daylight Saving Time](/time/dst/)  You have reached the maximum number of allowed cities (12)  This city has already been added and cannot be added twice.  ### New version! Try, for example:    * When the time in New York is 3 pm, [what is the time in London?](/worldclock/converter.html?hour=15&p1=179&p2=136)   * When the time in London is 9 am, [what is the time in Tokyo and India?](/worldclock/converter.html?hour=9&p1=136&p2=248&p3=176)   * The [Classic Time Zone Converter](/worldclock/converter-classic.html) is still available.  Include UTC Time in Results  Show Time Zone  Show Holidays and Office Hours (  Good,  Not so good,  Not good)  Show Current City Time  Show Time Difference (From 1st City)  [__ Clear all locations & start again ](/worldclock/converter.html)  __ Download / Export Results / Add to Calendar  ×  ## Download / Export Results  #### Plain text output   __Copy to clipboard  #### Shareable link   __Copy  ####  __Download as text file  ####  __ Add to Calendar  [ Need some help?](/worldclock/converter-help.html)  ### [Event Time Announcer ](/worldclock/fixedform.html)  Need to let the world know when your event is occurring in their time zone?  ### [Meeting Planner ](/worldclock/meeting.html)  Find the best meeting time across different time zones worldwide...  ### [Make a Countdown to This Event ](/countdown/create)  Countdown with colorful animations counting down the weeks, days, minutes, hours, and seconds...  ### [Looking for the Old Time Zone Converter? ](/worldclock/converter.html)  You can still use the legacy version of the [classic Time Zone Converter](/worldclock/converter-classic.html) to find the time difference between locations worldwide.  [![](//c.tadst.com/gfx/ios/app/mplanner-100.png)  ####  Meeting Planner App for iOS  Find the best time for a web meeting across time zones.  ](/ios/meetingplanner/)  [![Illustration image](//c.tadst.com/gfx/64x64/time-api.png?1)  ####  Time API  Get the time at any given coordinate on Earth, calculate time zone conversions.  ](https://dev.timeanddate.com/time/)  How was your experience? Thank you for your feedback! [Contact Us ](/information/feedback.html)  [ ![Illustration of a pink shield with a white heart.](//c.tadst.com/gfx/n/i/service__supporter.svg) ](/services/supporter.html)  ####  [ Love Our Site? Become a Supporter ](/services/supporter.html)    * Browse our site **advert free.**   * Sun & Moon times **precise to the second.**   * **Exclusive calendar templates** for PDF Calendar.  [![The timeanddate logo](//c.tadst.com/gfx/n/logo/logo-2021--horizontal-color- whitebg.svg)](/)  [© Time and Date AS 1995–2021](/information/copyright.html)  #### Company    * [About us](/company/)   * [Careers/Jobs](/company/jobs)   * [Contact Us](/information/feedback.html)   * [Contact Details](/information/contact.html)   * [Sitemap](/sitemap.html)   * [Newsletter](/newsletter/)  #### Legal    * [Link policy](/information/copyright.html)   * [Advertising](/information/advertising.html)   * [Disclaimer](/information/disclaimer.html)   * [Terms & Conditions](/information/terms-conditions.html)   * [Privacy Policy](/information/privacy.html)   * [Do Not Sell My Info](/information/privacy.html)   * [My Privacy](/custom/privacy.html)  #### Services    * [World Clock](/worldclock/)   * [Time Zones](/time/)   * [Calendar](/calendar/)   * [Weather](/weather/)   * [Sun & Moon](/astronomy/)   * [Timers](/counters/)   * [Calculators](/date/)   * [API](/services/api/)  #### Sites    * [timeanddate.no](https://www.timeanddate.no)   * [timeanddate.de](https://www.timeanddate.de)  #### Follow Us  [__](https://www.facebook.com/timeanddate/)[__](https://twitter.com/timeanddate)[__](https://www.linkedin.com/company/time- and-date- as/about/)[__](https://www.instagram.com/timeanddatecom/)[__](https://www.youtube.com/c/timeanddate)  © Time and Date AS 1995–2021. [Privacy & Terms](/information/copyright.html)  
In this course, there are two timed exams proctored via [ ProctorU ](https://www.proctoru.com/portal/illinois/index) . Timed 1-hour exams can be taken online at your convenience from **12am to 11:59pm CT during the "Exam window" provided in the weekly overviews.** ( [ **time zone conversion** ](http://www.timeanddate.com/worldclock/converter.html) ).  ##  What is ProctorU?  ProctorU is an online proctoring service that allows students to take exams online while ensuring the integrity of the exam for the institution. The service authenticates your identity and monitors both your computer screen and webcam to ensure academic integrity.  ##  How ProctorU Exam Works    1. Check the [ **Technical Requirements for Online Proctored Exam** ](http://online.illinois.edu/proctoru#tech) to make sure you have the right computer equipment.     2. ProctorU System Test: In order to take the proctored exams, you will need to pass the [ **ProctorU System Test** ](https://test-it-out.proctoru.com/) with the computer that you will use for taking the exams.     3. Sign up for a ProctorU Account: Before you can schedule for an exam appointment, you will first need to create an account with ProctorU at: [ **https://go.proctoru.com/students/users/new?institution=12** ](https://go.proctoru.com/students/users/new?institution=12) . It is recommended to use your **Illinois@edu** email to sign up for the account.     4. Schedule the exams with ProctorU and take the exams at the scheduled date and time with a proctor. For details on how to schedule and take the exam, please read the How to Schedule and Take the ProctorU Exams page carefully (located after this page).   ##  What You Need to Know and Prepare [ **IMPORTANT** ]    * Proctored exams are fee-based exams. For pricing information, see the [ **ProctorU Overview** ](https://citl.illinois.edu/services/for-students/proctoru) .     * Refunds are given only if the exam is canceled within **48 hours** before the scheduled exam time.     * **72-hour** notice is required for scheduling exams online, after which you must schedule by phone with an additional premium rush fee.     * **A web cam** , **microphone** , **computer with an Internet connection** , and **two forms of photo ID** are required.     * The time of your exam will not start until the exam is unlocked by your proctor.     * Clear your work space off any material not authorized for your exam.     * If you have any questions about what's permitted for your exam, the proctor will answer when they connect. Alternatively, you should read the Exam Policies and Technical Support page that's located in the respective exam week before the exam session to understand what resources are permitted for the exams.     * Please prepare a reflective surface such as a mirror (CDs and DVDs are **NOT** allowed) during the exam if your webcam is built into your computer or laptop. If you don't have a mirror, your cellphones and tablets work as well.     * Please be sure you are in a quiet, well-lit, and private (free of distractions and other people) room while taking the exam.     * Position your camera so that the proctor can see your entire face from chin to forehead.     * Please ensure your proctor can see all entrances and exists.     * During the exam, please follow your proctor's instructions and refrain from reading your questions loud.   ##  ProctorU Readiness Quiz:  At the end of this section, the ProctorU Readiness Quiz is designed to make sure that you understand what to prepare before taking the ProctorU exams. It does not count towards the course grade. It is recommended that you pass all questions in the ProctorU Readiness quiz.  
##  **CS410 Technology Review (4-credit students only)**  **CS410 Technology Review**  The Technology Review assignment is designed to provide students with an opportunity to go beyond the materials covered in the course lectures to learn about an interesting course-related cutting-edge technology topic not covered in any lecture. In this assignment, a student is required to write a short review article on a chosen topic by the student from a list of suggested topics by the instructor and TAs. A student can also propose a topic not on the list shttps://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing  The topic of a technology review can be one of the three broad topic categories related to the general topic of “text data retrieval and analysis”:  1) Useful software toolkits for processing text data or building text data applications.  2) Emerging new applications of text retrieval or analysis.  3) New techniques for text retrieval or analysis.  A list of specific topics will be provided for students to choose, but students may also propose additional topics interesting to them subject to the approval of the instructor. A review may cover one toolkit/application/technique in-depth or compare multiple toolkits/applications/techniques. The former is only allowed if the toolkit/application/technique is sufficiently complex to justify devoting an entire review to it. In any case, your review must have novel content that does not exist in any existing literature or webpages so that it would offer unique information/knowledge that others can learn from your review. So please make sure to check whether there is already a review on the topic before you devote time to complete a review. If you find an existing review on the topic, you may still write about the topic, but just need to make sure that you take a somewhat different perspective than the existing review or add new content on top of the existing review (i.e., extending it in some way).  The Technology Review should be completed individually. It will be graded based on completion of the following two tasks  1\. **Topic proposal** : Every student is required to select a topic from a provided topic list or propose a topic by the **end of Week 8 (Oct 17, 2021)** in the signup sheet: [ https://docs.google.com/spreadsheets/d/1v-RYD- E_KgqFnAdt7IvYHb9svNtlTla5HcOEfYSZyQM/edit?usp=sharing ](https://docs.google.com/spreadsheets/d/1v-RYD- E_KgqFnAdt7IvYHb9svNtlTla5HcOEfYSZyQM/edit?usp=sharing "https://docs.google.com/spreadsheets/d/1v-RYD- E_KgqFnAdt7IvYHb9svNtlTla5HcOEfYSZyQM/edit?usp=sharing")  Some sample topics are provided here: [ https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing ](https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing "https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing")  2\. **Review submission** : Each student is required to submit a complete Technology Review by the **end of Week 11 (Nov 7, 2021)** . The review must have a coherent storyline (Intro, Body, Conclusion) and cite relevant references. It must be at least ~2 pages  The deadline is set to an earlier time than that of course project code submission so as to give the students an opportunity to read some relevant reviews (especially those on toolkits) before finishing their projects if they want to.  
#  Week 1 Overview  The first six weeks of the course are based on the content of the Text Retrieval and Search Engines MOOC. During this week's lessons, you will learn the overall design of this MOOC, an overview of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model.  ##  Time  This module should take **approximately 3 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 1 Video Lectures  |  2 hours      **Week 1 Graded Quiz**  |  1 hour      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain some basic concepts in natural language processing, text information access.     * Explain why text retrieval is often defined as a ranking problem.     * Explain the basic idea of the vector space retrieval model and how to instantiate it with the simplest bit-vector representation.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What does a computer have to do in order to understand a natural language sentence?     * What is ambiguity?     * Why is natural language processing (NLP) difficult for computers?     * What is bag-of-words representation? Why do modern search engines use this simple representation of text?     * What are the two modes of text information access? Which mode does a web search engine such as Google support?     * When is browsing more useful than querying to help a user find relevant information?     * Why is a text retrieval task defined as a ranking task?     * What is a retrieval model?     * What are the two assumptions made by the Probability Ranking Principle?     * What is the Vector Space Retrieval Model? How does it work?     * How do we define the dimensions of the Vector Space Model? What does “bag of words” representation mean?     * What does the retrieval function intuitively capture when we instantiate a vector space model with bag of words representation and bit representation for documents and queries?   ##  Additional Readings and Resources  The following readings are optional:    * N. J. Belkin and W. B. Croft. 1992. _Information filtering and information retrieval: Two sides of the same coin?_ Commun. ACM 35, 12 (Dec. 1992), 29-38.     * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. **Chapters 1-6** .   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Part of speech tagging, syntactic analysis, semantic analysis, and ambiguity     * “Bag of words” representation     * Push, pull, querying, browsing     * Probability ranking principle     * Relevance     * Vector space model     * Dot product     * Bag of words representation     * Bit vector representation   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the **[ Learner Help Center ](https://courserahelp.zendesk.com/hc/en-us/) ** to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the **[ Content Issues ](https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) ** forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 10 Overview  During this week's lessons, you will learn text clustering, including the basic concepts, main clustering techniques, and how to evaluate text clustering. You will also start learning text categorization, which is related to text clustering, but with pre-defined categories that can be viewed as pre- defining clusters.  ##  Time  This module should take **approximately 3 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 10 Video Lectures  |  2 hours      **Week 10 Graded Quiz**  |  1 hour      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain the concept of text clustering and why it is useful.     * Explain how Hierarchical Agglomerative Clustering and k-Means clustering work.     * Explain how to evaluate text clustering.     * Explain the concept of text categorization and why it is useful.     * Explain how Naïve Bayes classifier works.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What is clustering? What are some applications of clustering in text mining and analysis?     * How does hierarchical agglomerative clustering work? How do single-link, complete-link, and average-link work for computing group similarity? Which of these three ways of computing group similarity is least sensitive to outliers in the data?     * How do we evaluate clustering results?     * What is text categorization? What are some applications of text categorization?     * What does the training data for categorization look like?     * How does the Naïve Bayes classifier work?     * Why do we often use logarithm in the scoring function for Naïve Bayes?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung, _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ . ACM and Morgan & Claypool Publishers, 2016. Chapters 14 & 15\.     * Manning, Chris D., Prabhakar Raghavan, and Hinrich Schütze. _Introduction to Information Retrieval_ . Cambridge: Cambridge University Press, 2007. Chapters 13-16.     * Yang, Yiming. _An Evaluation of Statistical Approaches to Text Categorization_ . Inf. Retr. 1, 1-2 (May 1999), 69-90. doi: 10.1023/A:1009982220290   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Clustering, document clustering, and term clustering     * Clustering bias     * Perspective of similarity     * Hierarchical Agglomerative Clustering, and k-Means     * Direction evaluation (of clustering), indirect evaluation (of clustering)     * Text categorization, topic categorization, sentiment categorization, email routing     * Spam filtering     * Naïve Bayes classifier     * Smoothing   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issue   
#  Week 11 Overview  During this week's lessons, you will continue learning about various methods for text categorization, particularly discriminative qualifiers, and you will also learn sentiment analysis and opinion mining, including a detailed introduction to a particular technique for sentiment classification (i.e., ordinal regression).  ##  Time  This module should take **approximately 4 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 11 Video Lectures  |  2 hours      **Week 11 Graded Quiz**  |  1 hour      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain the basic ideas of Logistic Regression, K-Nearest Neighbors (k-NN), and how K-NN works.     * Explain how to evaluate categorization results.     * Explain the tasks of opinion mining and sentiment analysis and why they are important tasks from an application perspective.     * Explain how sentiment analysis can be done using text categorization techniques and why a straightforward application of regular text categorization techniques may not be adequate.     * Give examples of both simple and complex features that are used for characterizing text data and explain how NLP can enable complex features to be generated from text.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What’s the general idea of the logistic regression classifier? How is it related to Naïve Bayes? Under what conditions would logistic regression cover Naïve Bayes as a special case for two-category categorization?     * What’s the general idea of the k-Nearest Neighbor classifier? How does it work?     * How do we evaluate categorization results?     * How do we compute classification accuracy, precision, recall, and F score?     * Why is harmonic mean as used in F better than the arithmetic mean of precision and recall?     * What’s the difference between macro and micro averaging?     * Why is it sometimes interesting to frame a categorization problem as a ranking problem?     * What is an opinion? How is it different from a factual statement?     * What’s an opinion holder? What’s an opinion target?     * What’s the goal of opinion mining?     * What is sentiment analysis? How is it similar to and different from a text categorization task such as topic categorization?     * Why are unigram features generally insufficient for accurate sentiment classification?     * What’s the concern of using too many complex features such as frequent substructures of parse trees?     * What are some commonly used features to represent text data?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung, _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining._ ACM and Morgan & Claypool Publishers, 2016. Chapters 15 & 18\.     * Yang, Yiming. An Evaluation of Statistical Approaches to Text Categorization. Inf. Retr. 1, 1-2 (May 1999), 69-90. doi: 10.1023/A:1009982220290     * Bing Liu, _Sentiment analysis and opinion mining._ Morgan & Claypool Publishers, 2012.     * Bo Pang and Lillian Lee, _Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieva_ l 2(1-2), pp. 1–135, 2008.   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Generative classifier vs. discriminative classifier     * Training data     * Logistic regression     * K-Nearest Neighbor classifier     * Classification accuracy, precision, recall, F measure, macro-averaging, and micro-averaging     * Opinion holder, opinion target, sentiment, and opinion representation     * Sentiment classification     * Features, n-grams, frequent patterns, and overfitting   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 2 Overview  During this week's lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query.  ##  Time  This module should take **approximately 6 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 2 Video Lectures  |  2 hours      **Week 2 Graded Quiz**  |  1 hour      **Programming Assignment 1**  |  3 hours      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain what TF-IDF weighting is and why TF transformation and document length normalization are necessary for the design of an effective ranking function.     * Explain what an inverted index is and how to construct it for a large set of text documents that do not fit into the memory.     * Explain how variable-length encoding can be used to compress integers and how unary coding and gamma-coding work.     * Explain how scoring of documents in response to a query can be done quickly by using an inverted index.     * Explain Zipf’s law.   ##  Guiding Questions  Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.    * What are some different ways to place a document as a vector in the vector space?     * What is term frequency (TF)?     * What is TF transformation?     * What is document frequency (DF)?     * What is inverse document frequency (IDF)?     * What is TF-IDF weighting?     * Why do we need to penalize long documents in text retrieval?     * What is pivoted document length normalization?     * What are the main ideas behind the retrieval function BM25?     * What is the typical architecture of a text retrieval system?     * What is an inverted index?     * Why is it desirable to compress an inverted index?     * How can we create an inverted index when the collection of documents does not fit into the memory?     * How can we leverage an inverted index to score documents quickly?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. **Chapter 6 - Section 6.3, and Chapter 8.**    * Ian H. Witten, Alistair Moffat, and Timothy C. Bell. _Managing Gigabytes: Compressing and Indexing Documents and Images_ , Second Edition. Morgan Kaufmann, 1999.   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Term frequency (TF)     * Document frequency (DF) and inverse document frequency (IDF)     * TF transformation     * Pivoted length normalization     * BM25     * Inverted index and postings     * Binary coding, unary coding, gamma-coding, and d-gap     * Zipf’s law   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues2   
#  Week 3 Overview  During this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the average precision (AP) and the normalized discounted cumulative gain (nDCG), and practical issues in evaluation, including statistical significance testing and pooling.  ##  Time  This module should take **approximately 7 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 3 Video Lectures  |  2 hours      **Week 3 Graded Quiz**  |  1 hour      **Programming Assignment 2.1**  |  4 hours      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.     * Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.     * Explain how to evaluate a ranked list of documents.     * Explain how to compute and plot a precision-recall curve.     * Explain how to compute average precision and mean average precision (MAP).     * Explain how to evaluate a ranked list with multi-level relevance judgments.     * Explain how to compute normalized discounted cumulative gain.     * Explain why it is important to perform statistical significance tests.   ##  Guiding Questions  Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.    * Why is evaluation so critical for research and application development in text retrieval?     * How does the Cranfield evaluation methodology work?     * How do we evaluate a set of retrieved documents?     * How do you compute precision, recall, and F1?     * How do we evaluate a ranked list of search results?     * How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)?     * What is mean reciprocal rank?     * Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?     * Why is precision at k documents more meaningful than average precision from a user’s perspective?     * How can we evaluate a ranked list of search results using multi-level relevance judgments?     * How do you compute normalized discounted cumulative gain (nDCG)?     * Why is normalization necessary in nDCG? Does MAP need a similar normalization? Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?   ##  Additional Readings and Resources    * Mark Sanderson. _Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval_ 4, 4 (2010), 247-375.     * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. Chapter 9   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Cranfield evaluation methodology     * Precision and recall     * Average precision, mean average precision (MAP), and geometric mean average precision (gMAP)     * Reciprocal rank and mean reciprocal rank     * F-measure     * Normalized discounted cumulative Gain (nDCG)     * Statistical significance test   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 4 Overview  During this week's lessons, you will learn probabilistic retrieval models and statistical language models, particularly the detail of the query likelihood retrieval function with two specific smoothing methods, and how the query likelihood retrieval function is connected with the retrieval heuristics used in the vector space model.  ##  Time  This module should take **approximately 6 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 4 Video Lectures  |  2 hours      **Week 4 Graded Quiz**  |  1 hour      **Programming Assignment 2.2**  |  3 hours      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain how to interpret p(R=1|q,d) and estimate it based on a large set of collected relevance judgments (or clickthrough information) about query q and document d.     * Explain how to interpret the conditional probability p(q|d) used for scoring documents in the query likelihood retrieval function.     * Explain what a statistical language model and a unigram language model are.     * Explain how to compute the maximum likelihood estimate of a unigram language model.     * Explain how to use unigram language models to discover semantically related words.     * Compute p(q|d) based on a given document language model p(w|d).     * Explain what smoothing does.     * Show that query likelihood retrieval function implements TF-IDF weighting if we smooth the document language model p(w|d) using the collection language model p(w|C) as a reference language model.     * Compute the estimate of p(w|d) using Jelinek-Mercer (JM) smoothing and Dirichlet Prior smoothing, respectively.   ##  Guiding Questions  Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.    * Given a table of relevance judgments in the form of three columns (query, document, and binary relevance judgments), how can we estimate p(R=1|q,d)?     * How should we interpret the query likelihood conditional probability p(q|d)?     * What is a statistical language model? What is a unigram language model? How many parameters are there in a unigram language model?     * How do we compute the maximum likelihood estimate of the unigram language model (based on a text sample)?     * What is a background language model? What is a collection language model? What is a document language model?     * Why do we need to smooth a document language model in the query likelihood retrieval model? What would happen if we don’t do smoothing?     * When we smooth a document language model using a collection language model as a reference language model, what is the probability assigned to an unseen word in a document?     * How can we prove that the query likelihood retrieval function implements TF-IDF weighting if we use a collection language model smoothing?     * How does linear interpolation (Jelinek-Mercer) smoothing work? What is the formula?     * How does Dirichlet prior smoothing work? What is the formula?     * What are the similarities and differences between Jelinek-Mercer smoothing and Dirichlet prior smoothing?   ##  Additional Readings and Resources    * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. **Chapter 6 - Section 6.4**  ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * p(R=1|q,d) ; query likelihood, p(q|d)     * Statistical and unigram language models     * Maximum likelihood estimate     * Background, collection, and document language models     * Smoothing of unigram language models     * Relation between query likelihood and TF-IDF weighting     * Linear interpolation (i.e., Jelinek-Mercer) smoothing     * Dirichlet Prior smoothing   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 5 Overview  During this week's lessons, you will learn feedback techniques in information retrieval, including the Rocchio feedback method for the vector space model, and a mixture model for feedback with language models. You will also learn how web search engines work, including web crawling, web indexing, and how links between web pages can be leveraged to score web pages.  ##  Time  This module should take **approximately 4 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 5 Video Lectures  |  2 hours      **Week 5 Graded Quiz**  |  1 hour      **Programming Assignment 2.3**  |  30 mins      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain the similarity and differences in the three different kinds of feedback, i.e., relevance feedback, pseudo-relevance feedback, and implicit feedback.     * Explain how the Rocchio feedback algorithm works.     * Explain how the Kullback-Leibler (KL) divergence retrieval function generalizes the query likelihood retrieval function.     * Explain the basic idea of using a mixture model for feedback.     * Explain some of the main general challenges in creating a web search engine.     * Explain what a web crawler is and what factors have to be considered when designing a web crawler.     * Explain the basic idea of Google File System (GFS).     * Explain the basic idea of MapReduce and how we can use it to build an inverted index in parallel.     * Explain how links on the web can be leveraged to improve search results.     * Explain how PageRank algorithm works.   ##  Guiding Questions  Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.    * What is relevance feedback? What is pseudo-relevance feedback? What is implicit feedback?     * How does Rocchio work? Why do we need to ensure that the original query terms have sufficiently large weights in feedback?     * What is the KL-divergence retrieval function? How is it related to the query likelihood retrieval function?     * What is the basic idea of the two-component mixture model for feedback?     * What are some of the general challenges in building a web search engine?     * What is a crawler? How can we implement a simple crawler?     * What is focused crawling? What is incremental crawling?     * What kind of pages should have a higher priority for recrawling in incremental crawling?     * What can we do if the inverted index doesn’t fit in any single machine?     * What’s the basic idea of the Google File System (GFS)?     * How does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework?     * How can we use MapReduce to build an inverted index in parallel?     * What is anchor text? Why is it useful for improving search accuracy?     * What is a hub page? What is an authority page?     * What kind of web pages tend to receive high scores from PageRank?     * How can we interpret PageRank from the perspective of a random surfer “walking” on the Web?     * How exactly do you compute PageRank scores?     * How does the HITS algorithm work?   ##  Additional Readings and Resources    * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. **Chapters 7 & 10 **  ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Relevance feedback     * Pseudo-relevance feedback     * Implicit feedback     * Rocchio feedback     * Kullback-Leiber divergence (KL-divergence) retrieval function     * Mixture language model     * Scalability and efficiency     * Spams     * Crawler, focused crawling, and incremental crawling     * Google File System (GFS)     * MapReduce     * Link analysis and anchor text     * PageRank   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 6 Overview  During this week's lessons, you will learn how machine learning can be used to combine multiple scoring factors to optimize ranking of documents in web search (i.e., learning to rank), and learn techniques used in recommender systems (also called filtering systems), including content-based recommendation/filtering and collaborative filtering. You will also have a chance to review all the text retrieval content.  ##  Time  This module should take **approximately 7 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 6 Video Lectures  |  2 hours      **Week 6 Graded Quiz**  |  1 hour      **Programming Assignment 2.4**  |  4 hours      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain how we can extend a retrieval system to perform content-based information filtering (recommendation).     * Explain how we can use a linear utility function to evaluate an information filtering system.     * Explain the basic idea of collaborative filtering.     * Explain how the memory-based collaborative filtering algorithm works.   ##  Guiding Questions  Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.    * What is content-based information filtering?     * How can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function?     * How can we extend a retrieval system to perform content-based information filtering?     * What is the exploration-exploitation tradeoff?     * How does the beta-gamma threshold learning algorithm work?     * What is the basic idea of collaborative filtering?     * How does the memory-based collaborative filtering algorithm work?     * What is the “cold start” problem in collaborative filtering?   ##  Additional Readings and Resources    * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. **Chapters 10 - Section 10.4,Chapters 11**  ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Content-based filtering     * Collaborative filtering     * Beta-gamma threshold learning     * Linear utility     * User profile     * Exploration-exploitation tradeoff     * Memory-based collaborative filtering     * Cold start   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-retrieval/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 7 Overview  From Week 7 to Week 12, the lectures are based on the Text Mining and Analytics MOOC. During this week's lessons, you will receive an overview of natural language processing techniques and text representation, which are the foundation for all kinds of text-mining applications, and word association mining with a particular focus on mining one of the two basic forms of word associations (i.e., paradigmatic relations).  ##  Time  This module should take **approximately 11 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 7 Video Lectures  |  2 hours      **Week 7 Graded Quiz**  |  1 hour      **Exam 1**  |  2 hours      Programming Assignment 3  |  6 hours      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain some basic concepts in natural language processing.     * Explain different ways to represent text data.     * Explain the two basic types of word associations and how to mine paradigmatic relations from text data.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What does a computer have to do in order to understand a natural language sentence?     * What is ambiguity?     * Why is natural language processing (NLP) difficult for computers?     * What is bag-of-words representation?     * Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?     * What is a paradigmatic relation?     * What is a syntagmatic relation?     * What is the general idea for discovering paradigmatic relations from text?     * What is the general idea for discovering syntagmatic relations from text?     * Why do we want to do Term Frequency Transformation when computing similarity of context?     * How does BM25 Term Frequency transformation work?     * Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung, _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining._ ACM and Morgan & Claypool Publishers, 2016. Chapters 1-4, Chapter 13.     * Chris Manning and Hinrich Schütze, _Foundations of Statistical Natural Language Processing._ MIT Press. Cambridge, MA: May 1999. Chapter 5 on collocations.     * Chengxiang Zhai, _Exploiting context to identify lexical atoms: A statistical view of linguistic context_ . Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brazil, Feb. 4-6, 1997, pp. 119-129.     * Shan Jiang and ChengXiang Zhai, _Random walks on adjacency graphs for mining lexical relations from big text data_ . Proceedings of IEEE BigData Conference 2014, pp. 549-554.   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Part of speech tagging     * Syntactic analysis     * Semantic analysis     * Ambiguity     * Text representation, especially bag-of-words representation     * Context of a word; context similarity     * Paradigmatic relation     * Syntagmatic relation   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 8 Overview  During this week's lessons, you will learn more about word association mining with a particular focus on mining the other basic form of word association (i.e., syntagmatic relations), and start learning topic analysis with a focus on techniques for mining one topic from text.  ##  Time  This module should take **approximately 3.5 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 8 Video Lectures  |  2 hours      **Week 8 Graded Quiz**  |  1 hour      **MP2.4 Submission for Grading**  |  30 mins      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain some basic concepts of probability including entropy, conditional entropy and mutual information.     * Explain some ways of discovering syntagmatic and paradigmatic relations.     * Explain the basic idea of Bayesian estimation theory.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What is entropy? For what kind of random variables does the entropy function reach its minimum and maximum, respectively?     * What is conditional entropy?     * What is the relation between conditional entropy H(X|Y) and entropy H(X)? Which is larger?     * How can conditional entropy be used for discovering syntagmatic relations?     * What is mutual information I(X;Y)? How is it related to entropy H(X) and conditional entropy H(X|Y)?     * What’s the minimum value of I(X;Y)? Is it symmetric?     * For what kind of X and Y, does mutual information I(X;Y) reach its minimum? For a given X, for what Y does I(X;Y) reach its maximum?     * Why is mutual information sometimes more useful for discovering syntagmatic relations than conditional entropy?     * What is a topic?     * How can we define the task of topic mining and analysis computationally? What’s the input? What’s the output?     * How can we heuristically solve the problem of topic mining and analysis by treating a term as a topic? What are the main problems of such an approach?     * What are the benefits of representing a topic by a word distribution?     * What is a statistical language model? What is a unigram language model? How can we compute the probability of a sequence of words given a unigram language model?     * What is Maximum Likelihood estimate of a unigram language model given a text article?     * What is the basic idea of Bayesian estimation? What is a prior distribution? What is a posterior distribution? How are they related with each other? What is Bayes rule?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ , ACM and Morgan & Claypool Publishers, 2016. Chapters 13, 17.   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Entropy     * Conditional entropy     * Mutual information     * Topic and coverage of topic     * Language model     * Generative model     * Unigram language model     * Word distribution     * Background language model     * Parameters of a probabilistic model     * Likelihood     * Bayes rule     * Maximum likelihood estimation     * Prior and posterior distributions     * Bayesian estimation & inference     * Maximum a posteriori (MAP) estimate     * Prior model     * Posterior mode   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   
#  Week 9 Overview  During this week's lessons, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA.  ##  Time  This module should take **approximately 3.5 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 9 Video Lectures  |  2 hours      **Week 9 Graded Quiz**  |  1 hour      **Project Proposal and Team Formation Submission for Grading**  |  30 mins      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain what a mixture of unigram language model is and why using a background language in a mixture can help “absorb” common words in English.     * Explain what PLSA is and how it can be used to mine and analyze topics in text.     * Explain the general idea of using a generative model for text mining.     * Explain how to compute the probability of observing a word from a mixture model like PLSA.     * Explain the basic idea of the EM algorithm and how it works.     * Explain the main difference between LDA and PLSA.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What is a mixture model? In general, how do you compute the probability of observing a particular word from a mixture model? What is the general form of the expression for this probability?     * What does the maximum likelihood estimate of the component word distributions of a mixture model behave like? In what sense do they “collaborate” and/or “compete”? Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability on the common (often non-content) words?     * What is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?     * What is PLSA? How many parameters does a PLSA model have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior on a topic word distribution?     * How is LDA different from PLSA? What is shared by the two models?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung, _Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining_ . ACM and Morgan & Claypool Publishers, 2016. Chapter 17.     * Blei, D. 2012. _Probabilistic Topic Models_ . Communications of the ACM 55 (4): 77–84. doi: 10.1145/2133806.2133826.     * Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. _Automatic Labeling of Multinomial Topic Models_ . Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246.     * Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011. _Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA_ . Information Retrieval, 14, 2 (April 2011), 178-203. doi: 10.1007/s10791-010-9141-9.   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Mixture model     * Component model     * Constraints on probabilities     * Probabilistic Latent Semantic Analysis (PLSA)     * Expectation-Maximization (EM) algorithm     * E-step and M-step     * Hidden variables     * Hill climbing     * Local maximum     * Latent Dirichlet Allocation (LDA)   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the **Contact Us!** link available on each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues.   
[SOUND]This lecture is about generating probabilisticmodels for text clustering. In this lecture, we're going to continuediscussing text clustering, and we're going to introducegenerating probabilistic models as a way to do text clustering. So this is the overall plan forcovering text clustering. In the previous lecture, we have talkedabout what is text clustering and why text clustering is interesting. In this lecture, we're going to talkabout how to do text clustering. In general, as you see on this slide,there are two kinds of approaches. One is generating probabilistic models,which is the topic of this lecture. And later, we'll also discusssimilarity-based approaches. So to talk about generating models fortext clustering, it would be useful to revisitthe topic mining problem using topic models,because the two problems are very similar. This is a slide that you have seenearlier in the lecture on topic model. Here we show that we have inputof a text collection C and a number of topics k, and vocabulary V. And we hope to generateas output two things. One is a set of topicsdenoted by Theta i's, each is awarded distribution andthe other is pi i j. These are the probabilities thateach document covers each topic. So this is a topic coverage andit's also visualized here on this slide. You can see that this is what wecan get by using a topic model. Now, the main difference between this andthe text clustering problem is that here, a document is assumed topossibly cover multiple topics. And indeed, in general,a document will be covering more than one topic withnonzero probabilities. In text clustering, however,we only allow a document to cover one topic,if we assume one topic is a cluster. So that means if we change the problemdefinition just slightly by assuming that each document that can onlybe generated by using precisely one topic. Then we'll have a definition ofthe clustering problem as you'll hear. So here the output is changed so that we no longer have the detailedcoverage distributions pi i j. But instead, we're going to havea cluster assignment decisions, Ci. And Ci is a decision for the document i. And C sub i is going to take a valuefrom 1 through k to indicate one of the k clusters. And basically tells us thatd i is in which cluster. As illustrated here, we no longer havemultiple topics covered in each document. It is precisely one topic. Although which topic is still uncertain. There is also a connection with the problem of mining one topicthat we discussed earlier. So here again,it's a slide that you have seen before and here we hope to estimate a topic model or distribution based onprecisely one document. And that's when we assume that thisdocument, it covers precisely one topic. But we can also consider somevariations of the problem. For example,we can consider there are N documents, each covers a different topic, sothat's N documents, and topics. Of course, in this case,these documents are independent, and these topics are also independent. But, we can further allow thesedocuments with share topics, and we can also assume that we are goingto assume there are fewer topics than the number of documents, sothese documents must share some topics. And if we have N documentsthat share k topics, then we'll again have preciselythe document clustering problem. So because of these connections,naturally we can think about how to use a probabilistically generative modelto solve the problem of text clustering. So the question now is what generativemodel can be used to do clustering? As in all cases of designing a generativemodel, we hope the generative model would adopt the output that we hope to generateor the structure that we hope to model. So in this case,this is a clustering structure, the topics andeach document that covers one topic. And we hope to embed suchpreferences in the generative model. But, if you think about the maindifference between this problem and the topic model that wetalked about earlier. And you will see a main requirementis how can we force every document to be generatedfrom precisely one topic, instead of k topics,as in the topic model? So let's revisit the topicmodel again in more detail. So this is a detailed view ofa two component mixture model. When we have k components,it looks similar. So here we see that whenwe generate a document, we generate each word independent. And when we generate each word, but firstmake a choice between these distributions. We decide to use one ofthem with probability. So p of theta 1 is the probability ofchoosing the distribution on the top. Now we first make this decision regardingwhich distribution should be used to generate the word. And then we're going to use thisdistribution to sample a word. Now note that in such a generative model, the decision on which distributionto use for each word is independent. So that means, for example, the here could have generated fromthe second distribution, theta 2 whereas text is more likely generatedfrom the first one on the top. That means the words in the document thatcould have been generated in general from multiple distributions. Now this is not what we want,as we said, for text clustering, for document clustering, where we hoped this document will begenerated from precisely one topic. So now that means weneed to modify the model. But how? Well, let's first think about why thismodel cannot be used for clustering. And as I just said, the reason is because it has allowed multiple topics tocontribute a word to the document. And that causes confusion becausewe're not going to know which cluster this document is from. And it's, more importantlyit's violating our assumption about the partitioning ofdocuments in the clusters. If we really have one topic to correspondit to one cluster of documents, then we would have a document that wegenerate from precisely one topic. That means all the words in the document must have been generated fromprecisely one distribution. And this is not true forsuch a topic model that we're seeing here. And that's why this cannot be used forclustering because it did not ensure that only one distribution has been usedto generate all the words in one document. So if you realize this problem, then we can naturally design alternativemixture model for doing clustering. So this is what you're seeing here. And we again have to make a decisionregarding which distribution to use to generate this document becausethe document could potentially be generated from any of the kword distributions that we have. But this time, once we have madea decision to choose one of the topics, we're going to stay with this regime togenerate all the words in the document. And that means, once we have madea choice of the distribution in generating the first word,we're going to go stay with this distribution in generating all ofthe other words in the document. So, in other words,we only make the choice once for, basically, we make the decision once forthis document and this state was just togenerate all the words. Similarly if I had choosing the seconddistribution, theta sub 2 here, you can see which state was this one. And then generatethe entire document of d. Now, if you compare thispicture with the previous one, you will see the decision ofusing a particular distribution is made just once for this document,in the case of document clustering. But in the case of topic model, we have to make as many decisions asthe number of words in the document. Because for each word, we can makea potentially different decision. And that's the key differencebetween the two models. But this is obviouslyalso a mixed model so we can just group them togetheras one box to show that this is the model that will give usa probability of the document. Now, inside of this model, there is also this switch ofchoosing a different distribution. And we don't observe that sothat's a mixture model. And of course a main problem indocument clustering is to infer which distribution has been usedto generate a document and that would allow us to recoverthe cluster identity of a document. So it will be useful to think aboutthe difference from the topic model as I have also mentioned multiple times. And there are mainly two differences, one is the choice of using that particular distribution ismade just once for document clustering. Whereas in the topic model, it's madeit multiple times for different words. The second is that word distribution,here, is going to be used to regenerateall the words for a document. But, in the case of onedistribution doesn't have to generate all the words in the document. Multiple distribution could have been usedto generate the words in the document. Let's also think about a special case, when one of the probability of choosinga particular distribution is equal to 1. Now that just means wehave no uncertainty now. We just stick with oneparticular distribution. Now in that case, clearly, we willsee this is no longer mixture model, because there's no uncertainty here and we can just use precisely one of thedistributions for generating a document. And we're going back tothe case of estimating one order distribution based on one document. So that's a connectionthat we discussed earlier. Now you can see it more clearly. So as in all cases of usinga generative model to solve a problem, we first look at data andthen think about how to design the model. But once we design the model, the next step is to writedown the likelihood function. And after that we're going to look atthe how to estimate the parameters. So in this case,what's the likelihood function? It's going to be very similar to whatyou have seen before in topic models but it will be also different. Now if you still recall whatthe likelihood function looks like in then you will realize that in general, theprobability of observing a data point from mixture model is going to be a sum of allthe possibilities of generating the data. In this case, so it's going tobe a sum over these k topics, because every one can beuser generated document. And then inside is the sum you can stillrecall what the formula looks like, and it's going to be the productof two probabilities. One is the probability of choosing thedistribution, the other is the probability of observing a particulardatapoint from that distribution. So if you map this kind offormula to our problem here, you will see the probabilityof observing a document d is basically a sum in thiscase of two different distributions because we have a verysimplified situation of just two clusters. And so in this case,you can see it's a sum of two cases. In each case,it's indeed the probability of choosing the distribution either theta 1 ortheta 2. And then, the probability ismultiplied by the probability of observing this document fromthis particular distribution. And if you further expandedthis probability of observing the whole document, we see that it'sa product of observing each word X sub i. And here we made the assumption thateach word is generated independently, so the probability of the wholedocument is just a product of the probability of eachword in the document. So this form should be verysimilar to the topic model. But it's also useful to think aboutthe difference and for that purpose, I am also copying the probability oftopic model of these two components here. So here you can see the formula looks verysimilar or in many ways, they are similar. But there is also some difference. And in particular,the difference is on the top. You see for the mixture model for documentclustering, we first take a product, and then take a sum. And that's correspondingto our assumption of first make a choice ofchoosing one distribution and then stay with the distribution,it'll generate all the words. And that's why we havethe product inside the sum. The sum corresponds to the choice. Now, in topic model, we see thatthe sum is actually inside the product. And that's because we generatedeach word independently. And that's why we havethe product outside, but when we generate each word wehave to make a decision regarding which distribution we use sowe have a sum there for each word. But in general,these are all mixture models and we can estimate these modelsby using the Algorithm, as we will discuss more later. [MUSIC]
[SOUND] This lecture isa continued discussion of evaluation of text categorization. Earlier we have introduced measures thatcan be used with computer provision and recall. For each category and each documentnow in this lecture we're going to further examine how to combine theperformance of the different categories of different documents how to aggregate them,how do we take average? You see on the title here I indicatedit's called a macro average and this is in contrast to micro averagethat we'll talk more about later. So, again, for each category we're goingto compute the precision require an f1 so for example category c1 we haveprecision p1, recall r1 and F value f1. And similarly we can do that for category2 and and all the other categories. Now once we compute that andwe can aggregate them, so for example we can aggregateall the precision values. For all the categories, forcomputing overall precision. And this is often very useful to summarizewhat we have seen in the whole data set. And aggregation can bedone many different ways. Again as I said, in a case when youneed to aggregate different values, it's always good to think about what'sthe best way of doing the aggregation. For example, we can consider arithmeticmean, which is very commonly used, or you can use geometric mean,which would have different behavior. Depending on the way you aggregate,you might have got different conclusions. in terms of which method works better,so it's important to consider these differences and choosing the right one ora more suitable one for your task. So the difference fore examplebetween arithmetically and geometrically is that the arithmeticallywould be dominated by high values whereas geometrically wouldbe more affected by low values. Base and so whether you are wantto emphasis low values or high values would be a questionrelate with all you And similar we can do that forrecal and F score. So that's how we can generate the overallprecision, recall and F score. Now we can do the same for aggregationof other all the document All right. So it's exactly the same situation foreach document on our computer. Precision, recall, and F. And then after we have completedthe computation for all these documents, we're going to aggregate them to generatethe overall precision, overall recall, and overall F score. These are, again, examiningthe results from different angles. Which one's more useful willdepend on your application. In general, it's beneficial to look atthe results from all these perspectives. And especially if you compare differentmethods in different dimensions, it might reveal which methodIs better in which measure or in what situations andthis provides insightful. Understanding the strands of a method ora weakness and this provides further insight forimproving them. So as I mentioned,there is also micro-average in contrast to the macro averagethat we talked about earlier. In this case, what we do is youpool together all the decisions, and then compute the precision and recall. So we can compute the overallprecision and recall by just counting how many cases are in true positive,how many cases in false positive, etc, it's computing the valuesin the contingency table, and then we can compute the precision andrecall just once. In contrast, in macro-averaging, we'regoing to do that for each category first. And then aggregate over these categoriesor we do that for each document and then aggregate all the documents buthere we pooled them together. Now this would be very similar tothe classification accuracy that we used earlier, and one problem here of course to treat allthe instances, all the decisions equally. And this may not be desirable. But it may be a property forsome applications, especially if we associate the, forexample, the cost for each combination. Then we can actually compute for example,weighted classification accuracy. Where you associate the different cost orutility for each specific decision, so there could be variations of thesemethods that would be more useful. But in general macro average tends tobe more information than micro average, just because it might reflect the need forunderstanding performance on each category or performance on eachdocument which are needed in applications. But macro averaging and micro averaging,they are both very common, and you might see both reported inresearch papers on Categorization. Also sometimes categorizationresults might actually be evaluated from ranking prospective. And this is because categorizationresults are sometimes or often indeed passed it to a human forvarious purposes. For example, it might be passedto humans for further editing. For example, news articles can be temptedto be categorized by using a system and then human editors wouldthen correct them. And all the email messages might bethroughout to the right person for handling in the help desk. And in such a case the categorizationswill help prioritizing the task forparticular customer service person. So, in this case the resultshave to be prioritized and if the system can't give a scoreto the categorization decision for confidence then we can use the scoresto rank these decisions and then evaluate the results as a rank list,just as in a search engine. Evaluation where you rankthe documents in responsible query. So for example a discovery ofspam emails can be evaluated based on ranking emails forthe spam category. And this is useful if you want peopleto to verify whether this is really spam, right? The person would then takethe rank To check one by one and then verify whether this is indeed a spam. So to reflect the utility forhumans in such a task, it's better to evaluate Ranking Chris and thisis basically similar to a search again. And in such a case oftenthe problem can be better formulated as a ranking probleminstead of a categorization problem. So for example, ranking documents ina search engine can also be framed as a binary categorization problem,distinguish the relevant documents that are useful to users from those thatare not useful, but typically we frame this as a ranking problem,and we evaluate it as a rank list. That's because people tendto examine the results so ranking evaluation more reflectsutility from user's perspective. So to summarize categorization evaluation, first evaluation is always veryimportant for all these tasks. So get it right. If you don't get it right,you might get misleading results. And you might be misled to believeone method is better than the other, which is in fact not true. So it's very important to get it right. Measures must also reflectthe intended use of the results for a particular application. For example, in spam filtering and news categorization the resultsare used in maybe different ways. So then we would need toconsider the difference and design measures appropriately. We generally need to consider how will theresults be further processed by the user and think from a user's perspective. What quality is important? What aspect of quality is important? Sometimes there are trade offs betweenmultiple aspects like precision and recall and so we need to know for thisapplication is high recall more important, or high precision is more important. Ideally we associate the different costwith each different decision arrow. And this of course has to be designedin an application specific way. Some commonly used measures for relativecomparison methods are the following. Classification accuracy, it's verycommonly used for especially balance. [INAUDIBLE] preceding [INAUDIBLE]Scores are common and report characterizing performances,given angles and give us some [INAUDIBLE] like a [INAUDIBLE] Perdocument basis [INAUDIBLE] And then take a average of all of them, differentways micro versus macro [INAUDIBLE]. In general, you want to look at theresults from multiple perspectives and for particular applications some perspectiveswould be more important than others but diagnoses andanalysis of categorization methods. It's generally useful to look atas many perspectives as possible to see subtle differences between methodsor tow see where a method might be weak from which you can obtain sight forimproving a method. Finally sometimes rankingmay be more appropriate so be careful sometimes categorization hasgot may be better frame as a ranking tasks and there're machine running methods foroptimizing ranking measures as well. So here are two suggested readings. One is some chapters of this book whereyou can find more discussion about evaluation measures. The second is a paper aboutcomparison of different approaches to text categorization and it also has an excellent discussion ofhow to evaluate textual categorization. [MUSIC]
[SOUND] This lecture is a continued discussion ofLatent Aspect Rating Analysis. Earlier, we talked about how to solvethe problem of LARA in two stages. But we first do segmentationof different aspects. And then we use a latent regressionmodel to learn the aspect ratings and then later the weight. Now it's also possible to developa unified generative model for solving this problem, and that is we not only model the generationalover-rating based on text. We also model the generation of text,and so a natural solution wouldbe to use topic model. So given the entity, we can assume there are aspects thatare described by word distributions. Topics. And then we an use a topic model to modelthe generation of the reviewed text. I will assume words in the review textare drawn from these distributions. In the same way as we assumed forgenerating model like PRSA. And then we can then plug inthe latent regression model to use the text to furtherpredict the overrating. And that means when we firstpredict the aspect rating and then combine them with aspect weightsto predict the overall rating. So this would give usa unified generated model, where we model both the generation of textand the overall ready condition on text. So we don't have time to discussthis model in detail as in many other cases in this part of the causewhere we discuss the cutting edge topics, but there's a reference site herewhere you can find more details. So now I'm going to show you somesimple results that you can get by using these kind of generated models. First, it's about rating decomposition. So here, what you seeare the decomposed ratings for three hotels that havethe same overall rating. So if you just look at the overall rating, you can't really tell muchdifference between these hotels. But by decomposing theseratings into aspect ratings we can see some hotels have higherratings for some dimensions, like value, but others might score betterin other dimensions, like location. And so this can give you detailedopinions at the aspect level. Now here, the ground-truth isshown in the parenthesis, so it also allows you to see whetherthe prediction is accurate. It's not always accurate but It's mostlystill reflecting some of the trends. The second result you comparedifferent reviewers on the same hotel. So the table shows the decomposed ratingsfor two reviewers about same hotel. Again their high leveloverall ratings are the same. So if you just look at the overallratings, you don't really get that much information about the differencebetween the two reviewers. But after you decompose the ratings, you can see clearly that they havehigh scores on different dimensions. So this shows that model can reviewdifferences in opinions of different reviewers and such a detailedunderstanding can help us understand better about reviewers and also betterabout their feedback on the hotel. This is something very interesting, because this is in somesense some byproduct. In our problem formulation,we did not really have to do this. But the design of the generatingmodel has this component. And these are sentimental weights forwords in different aspects. And you can see the highly weighted wordsversus the negatively loaded weighted words here foreach of the four dimensions. Value, rooms, location, and cleanliness. The top words clearly make sense, andthe bottom words also make sense. So this shows that with this approach, we can also learn sentimentinformation directly from the data. Now, this kind of lexicon is very usefulbecause in general, a word like long, let's say, may have different sentimentpolarities for different context. So if I say the battery life of thislaptop is long, then that's positive. But if I say the rebooting time forthe laptop is long, that's bad, right? So even forreviews about the same product, laptop, the word long is ambiguous, it couldmean positive or it could mean negative. But this kind of lexicon, that we canlearn by using this kind of generated models, can show whether a word ispositive for a particular aspect. So this is clearly very useful, and infact such a lexicon can be directly used to tag other reviews about hotels or tag comments about hotels insocial media like Tweets. And what's also interesting is that sincethis is almost completely unsupervised, well assuming the reviews whoseoverall rating are available And then this can allow us to learn formpotentially larger amount of data on the internet to reach sentiment lexicon. And here are some results tovalidate the preference words. Remember the model can infer wethera reviewer cares more about service or the price. Now how do we know whetherthe inferred weights are correct? And this poses a very difficultchallenge for evaluation. Now here we show someinteresting way of evaluating. What you see here are the pricesof hotels in different cities, and these are the prices of hotels that arefavored by different groups of reviewers. The top ten are the reviewerswas the highest inferred value to other aspect ratio. So for example value versus location,value versus room, etcetera. Now the top ten of the reviewers thathave the highest ratios by this measure. And that means these reviewerstend to put a lot of weight on value as comparedwith other dimensions. So that means they reallyemphasize on value. The bottom ten on the otherhand of the reviewers. The lowest ratio, what does that mean? Well it means these reviewers haveput higher weights on other aspects than value. So those are people that cared aboutanother dimension and they didn't care so much the value in some sense, at leastas compared with the top ten group. Now these ratios are computer based onthe inferred weights from the model. So now you can see the average pricesof hotels favored by top ten reviewers are indeed much cheaper than thosethat are favored by the bottom ten. And this provides some indirect wayof validating the inferred weights. It just means the weights are not random. They are actually meaningful here. In comparison,the average price in these three cities, you can actually see the top tentend to have below average in price, whereas the bottom half, where they carea lot about other things like a service or room condition tend to have hotelsthat have higher prices than average. So with these results we can builda lot of interesting applications. For example, a direct application would beto generate the rated aspect, the summary, and because of the decomposition wehave now generated the summaries for each aspect. The positive sentences the negativesentences about each aspect. It's more informative than original reviewthat just has an overall rating and review text. Here are some other results about the aspects that's coveredfrom reviews with no ratings. These are mp3 reviews, and these results show that the modelcan discover some interesting aspects. Commented on low overall ratings versusthose higher overall per ratings. And they care more aboutthe different aspects. Or they comment more onthe different aspects. So that can help us discover forexample, consumers' trend in appreciating differentfeatures of products. For example, one might have discoveredthe trend that people tend to like larger screens of cell phones orlight weight of laptop, etcetera. Such knowledge can be useful for manufacturers to design theirnext generation of products. Here are some interesting resultson analyzing users rating behavior. So what you see is average weights along different dimensions bydifferent groups of reviewers. And on the left side you see the weightsof viewers that like the expensive hotels. They gave the expensive hotels 5 Stars,and you can see their average ratestend to be more for some service. And that suggests that people likeexpensive hotels because of good service, and that's not surprising. That's also another way tovalidate it by inferred weights. If you look at the right side where,look at the column of 5 Stars. These are the reviewers thatlike the cheaper hotels, and they gave cheaper hotels five stars. As we expected andthey put more weight on value, and that's why they likethe cheaper hotels. But if you look at the, when they didn'tlike expensive hotels, or cheaper hotels, then you'll see that they tended tohave more weights on the condition of the room cleanness. So this shows that by using this model,we can infer some information that's very hard to obtaineven if you read all the reviews. Even if you read all the reviews it'svery hard to infer such preferences or such emphasis. So this is a case where text miningalgorithms can go beyond what humans can do, to reviewinteresting patterns in the data. And this of course can be very useful. You can compare different hotels, compare the opinions from differentconsumer groups, in different locations. And of course, the model is general. It can be applied to anyreviews with overall ratings. So this is a very usefultechnique that can support a lot of text mining applications. Finally the results of applying thismodel for personalized ranking or recommendation of entities. So because we can infer the reviewersweights on different dimensions, we can allow a user to actuallysay what do you care about. So for example, I have a queryhere that shows 90% of the weight should be on value and 10% on others. So that just means I don'tcare about other aspect. I just care about getting a cheaper hotel. My emphasis is on the value dimension. Now what we can do with such queryis we can use reviewers that we believe have a similar preferenceto recommend a hotels for you. How can we know that? Well, we can infer the weights ofthose reviewers on different aspects. We can find the reviewers whoseweights are more precise, of course inferred ratesare similar to yours. And then use those reviewers torecommend hotels for you and this is what we call personalized orrather query specific recommendations. Now the non-personalizedrecommendations now shown on the top, and you can see the top results generallyhave much higher price, than the lower group and that's because when thereviewer's cared more about the value as dictated by this query they tendedto really favor low price hotels. So this is yetanother application of this technique. It shows that by doing text miningwe can understand the users better. And once we can handle users betterwe can solve these users better. So to summarize our discussionof opinion mining in general, this is a very important topic andwith a lot of applications. And as a text sentimentanalysis can be readily done by using just text categorization. But standard techniquetends to not be enough. And so we need to have enrichedfeature implementation. And we also need to considerthe order of those categories. And we'll talk about ordinalregression for some of these problem. We have also assume thatthe generating models are powerful for mining latent user preferences. This in particular in the generativemodel for mining latent regression. And we embed some interestingpreference information and send the weights of words in the modelas a result we can learn most useful information whenfitting the model to the data. Now most approaches have been proposed andevaluated. For product reviews, and that was becausein such a context, the opinion holder and the opinion target are clear. And they are easy to analyze. And there, of course,also have a lot of practical applications. But opinion mining from news andsocial media is also important, but that's more difficult than analyzing review data,mainly because the opinion holders and opinion targets are all interested. So that calls for natural management processingtechniques to uncover them accurately. Here are some suggested readings. The first two are small books thatare of some use of this topic, where you can find a lot of discussionabout other variations of the problem and techniques proposed forsolving the problem. The next two papers aboutgenerating models for rating the aspect rating analysis. The first one is about solvingthe problem using two stages, and the second one is about a unified modelwhere the topic model is integrated with the regression model to solvethe problem using a unified model. [MUSIC]
[SOUND]So, looking at the text mining problem moreclosely, we see that the problem is similar to general data mining, exceptthat we'll be focusing more on text data. And we're going to have text miningalgorithms to help us to turn text data into actionable knowledge thatwe can use in real world, especially for decision making, or for completing whatever tasks thatrequire text data to support. Because, in general,in many real world problems of data mining we also tend to have other kindsof data that are non-textual. So a more general picture would beto include non-text data as well. And for this reason we might beconcerned with joint mining of text and non-text data. And so in this course we'regoing to focus more on text mining, but we're also going to also touch how doto joint analysis of both text data and non-text data. With this problem definition wecan now look at the landscape of the topics in text mining and analytics. Now this slide shows the process ofgenerating text data in more detail. More specifically, a human sensor or human observer would look atthe word from some perspective. Different people would be looking atthe world from different angles and they'll pay attention to different things. The same person at different times mightalso pay attention to different aspects of the observed world. And so the humans are able to perceivethe world from some perspective. And that human, the sensor,would then form a view of the world. And that can be called the Observed World. Of course, this would be different fromthe Real World because of the perspective that the person has takencan often be biased also. Now the Observed World can berepresented as, for example, entity-relation graphs orin a more general way, using knowledge representation language. But in general, this is basically whata person has in mind about the world. And we don't really know whatexactly it looks like, of course. But then the human wouldexpress what the person has observed using a natural language,such as English. And the result is text data. Of course a person could have useda different language to express what he or she has observed. In that case we might have text data ofmixed languages or different languages. The main goal of text miningIs actually to revert this process of generating text data. We hope to be able to uncoversome aspect in this process. Specifically, we can think about mining,for example, knowledge about the language. And that means by looking at text datain English, we may be able to discover something about English, some usageof English, some patterns of English. So this is one type of mining problems,where the result is some knowledge about language whichmay be useful in various ways. If you look at the picture, we can also then mine knowledgeabout the observed world. And so this has much to do withmining the content of text data. We're going to look at what the textdata are about, and then try to get the essence of it orextracting high quality information about a particular aspect ofthe world that we're interested in. For example, everything that has beensaid about a particular person or a particular entity. And this can be regarded as mining content to describe the observed world inthe user's mind or the person's mind. If you look further,then you can also imagine we can mine knowledge about this observer,himself or herself. So this has also to do withusing text data to infer some properties of this person. And these properties couldinclude the mood of the person or sentiment of the person. And note that we distinguishthe observed word from the person because text data can't describe what theperson has observed in an objective way. But the description can be alsosubjected with sentiment and so, in general, you can imagine the textdata would contain some factual descriptions of the world plussome subjective comments. So that's why it's also possible to do text mining to mineknowledge about the observer. Finally, if you look at the pictureto the left side of this picture, then you can see we can certainly alsosay something about the real world. Right? So indeed we can do text mining toinfer other real world variables. And this is often calleda predictive analytics. And we want to predict the valueof certain interesting variable. So, this picture basically covered multiple types of knowledge thatwe can mine from text in general. When we infer otherreal world variables we could also use some of the results from mining text data as intermediateresults to help the prediction. For example, after we mine the content of text data wemight generate some summary of content. And that summary could be then used to help us predict the variablesof the real world. Now of course this is still generatedfrom the original text data, but I want to emphasize here thatoften the processing of text data to generate some features that can helpwith the prediction is very important. And that's why here we show the results of some other mining tasks, includingmining the content of text data and mining knowledge about the observer,can all be very helpful for prediction. In fact, when we have non-text data,we could also use the non-text data to help prediction, andof course it depends on the problem. In general, non-text data can be veryimportant for such prediction tasks. For example,if you want to predict stock prices or changes of stock prices based ondiscussion in the news articles or in social media, then this is an example of using text data to predictsome other real world variables. But in this case, obviously, the historical stock price data wouldbe very important for this prediction. And so that's an example ofnon-text data that would be very useful for the prediction. And we're going to combine both kindsof data to make the prediction. Now non-text data can be also used foranalyzing text by supplying context. When we look at the text data alone, we'll be mostly looking at the contentand/or opinions expressed in the text. But text data generally alsohas context associated. For example, the time and the locationthat associated are with the text data. And these are useful context information. And the context can provide interestingangles for analyzing text data. For example, we might partition textdata into different time periods because of the availability of the time. Now we can analyze text data in eachtime period and then make a comparison. Similarly we can partition textdata based on locations or any meta data that's associated toform interesting comparisons in areas. So, in this sense,non-text data can actually provide interesting angles orperspectives for text data analysis. And it can help us make context-sensitive analysis of content orthe language usage or the opinions about the observer orthe authors of text data. We could analyze the sentimentin different contexts. So this is a fairly general landscape ofthe topics in text mining and analytics. In this course we're going toselectively cover some of those topics. We actually hope to covermost of these general topics. First we're going to covernatural language processing very briefly because this has to dowith understanding text data and this determines how we can representtext data for text mining. Second, we're going to talk about how tomine word associations from text data. And word associations is a form of use forlexical knowledge about a language. Third, we're going to talk abouttopic mining and analysis. And this is only one way toanalyze content of text, but it's a very useful waysof analyzing content. It's also one of the most usefultechniques in text mining. Then we're going to talk aboutopinion mining and sentiment analysis. So this can be regarded as one exampleof mining knowledge about the observer. And finally we're going tocover text-based prediction problems where we try to predict somereal world variable based on text data. So this slide also serves asa road map for this course. And we're going to usethis as an outline for the topics that we'll coverin the rest of this course. [MUSIC]
[SOUND] This lecture isabout the syntagmatic relation discovery andconditional entropy. In this lecture, we're going to continue the discussionof word association mining and analysis. We're going to talk about the conditionalentropy, which is useful for discovering syntagmatic relations. Earlier, we talked aboutusing entropy to capture how easy it is to predict the presence orabsence of a word. Now, we'll addressa different scenario where we assume that we know somethingabout the text segment. So now the question is, suppose we knowthat eats occurred in the segment. How would that help uspredict the presence or absence of water, like in meat? And in particular, we want toknow whether the presence of eats has helped us predictthe presence of meat. And if we frame this using entrophy, that would mean we are interestedin knowing whether knowing the presence of eats could reduceuncertainty about the meats. Or, reduce the entrophyof the random variable corresponding to the presence orabsence of meat. We can also ask as a question,what if we know of the absents of eats? Would that also help us predictthe presence or absence of meat? These questions can beaddressed by using another concept called a conditioning entropy. So to explain this concept, let's firstlook at the scenario we had before, when we know nothing about the segment. So we have these probabilities indicatingwhether a word like meat occurs, or it doesn't occur in the segment. And we have an entropy function thatlooks like what you see on the slide. Now suppose we know eats is present, so now we know the value of anotherrandom variable that denotes eats. Now, that would change allthese probabilities to conditional probabilities. Where we look at the presence orabsence of meat, given that we know eatsoccurred in the context. So as a result, if we replace these probabilitieswith their corresponding conditional probabilities in the entropy function,we'll get the conditional entropy. So this equation now here would be the conditional entropy. Conditional on the presence of eats. So, you can see this is essentiallythe same entropy function as you have seen before, except that allthe probabilities now have a condition. And this then tells usthe entropy of meat, after we have known eatsoccurring in the segment. And of course, we can also definethis conditional entropy for the scenario where we don't see eats. So if we know it did not occur inthe segment, then this entry condition of entropy would capture the instancesof meat in that condition. So now,putting different scenarios together, we have the completed definitionof conditional entropy as follows. Basically, we're going to consider bothscenarios of the value of eats zero, one, and this gives us a probabilitythat eats is equal to zero or one. Basically, whether eats is present orabsent. And this of course, is the conditional entropy ofmeat in that particular scenario. So if you expanded this entropy, then you have the following equation. Where you see the involvement ofthose conditional probabilities. Now in general, for any discreterandom variables x and y, we have the conditional entropy is no largerthan the entropy of the variable x. So basically, this is upper bound forthe conditional entropy. That means by knowing moreinformation about the segment, we want to be able toincrease uncertainty. We can only reduce uncertainty. And that intuitively makes sensebecause as we know more information, it should always helpus make the prediction. And cannot hurtthe prediction in any case. Now, what's interesting here is also tothink about what's the minimum possible value of this conditional entropy? Now, we know that the maximumvalue is the entropy of X. But what about the minimum,so what do you think? I hope you can reach the conclusion thatthe minimum possible value, would be zero. And it will be interesting to think aboutunder what situation will achieve this. So, let's see how we can use conditionalentropy to capture syntagmatic relation. Now of course,this conditional entropy gives us directly one way to measurethe association of two words. Because it tells us to what extent,we can predict the one word given that we know the presence orabsence of another word. Now before we look at the intuitionof conditional entropy in capturing syntagmatic relations, it's useful tothink of a very special case, listed here. That is, the conditional entropyof the word given itself. So here, we listed this conditionalentropy in the middle. So, it's here. So, what is the value of this? Now, this means we know wherethe meat occurs in the sentence. And we hope to predict whetherthe meat occurs in the sentence. And of course, this is 0 becausethere's no incident anymore. Once we know whether the wordoccurs in the segment, we'll already know the answerof the prediction. So this is zero. And that's also when this conditionalentropy reaches the minimum. So now, let's look at some other cases. So this is a case of knowing the andtrying to predict the meat. And this is a case of knowing eats andtrying to predict the meat. Which one do you think is smaller? No doubt smaller entropy means easier forprediction. Which one do you think is higher? Which one is not smaller? Well, if you at the uncertainty,then in the first case, the doesn't really tellus much about the meat. So knowing the occurrence of the doesn'treally help us reduce entropy that much. So it stays fairly close tothe original entropy of meat. Whereas in the case of eats,eats is related to meat. So knowing presence of eats orabsence of eats, would help us predict whether meat occurs. So it can help us reduce entropy of meat. So we should expect the sigma term, namelythis one, to have a smaller entropy. And that means there is a strongerassociation between meat and eats. So we now also know whenthis w is the same as this meat, then the conditional entropywould reach its minimum, which is 0. And for what kind of wordswould either reach its maximum? Well, that's when this stuffis not really related to meat. And like the for example,it would be very close to the maximum, which is the entropy of meat itself. So this suggests that when youuse conditional entropy for mining syntagmatic relations,the hours would look as follows. For each word W1, we're going toenumerate the overall other words W2. And then, we can computethe conditional entropy of W1 given W2. We thought all the candidate was inascending order of the conditional entropy because we're out of favor,a world that has a small entropy. Meaning that it helps us predictthe time of the word W1. And then, we're going to take the top ringof the candidate words as words that have potential syntagmatic relations with W1. Note that we need to usea threshold to find these words. The stresser can be the numberof top candidates take, or absolute value forthe conditional entropy. Now, this would allow us to mine the most strongly correlated words witha particular word, W1 here. But, this algorithm does nothelp us mine the strongest that K syntagmatical relationsfrom an entire collection. Because in order to do that, we have toensure that these conditional entropies are comparable across different words. In this case of discoveringthe mathematical relations for a targeted word like W1, we only needto compare the conditional entropies for W1, given different words. And in this case, they are comparable. All right. So, the conditional entropy of W1, givenW2, and the conditional entropy of W1, given W3 are comparable. They all measure how hardit is to predict the W1. But, if we think about the two pairs, where we share W2 in the same condition,and we try to predict the W1 and W3. Then, the conditional entropiesare actually not comparable. You can think of about this question. Why? So why are they not comfortable? Well, that was because theyhave a different outer bounds. Right?So those outer bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds. So we cannot reallycompare them in this way. So how do we address this problem? Well later, we'll discuss, we can usemutual information to solve this problem. [MUSIC]
[SOUND] Now lets look at anotherbehaviour of the Mixed Model and in this case lets look atthe response to data frequencies. So what you are seeing now is basicallythe likelihood of function for the two word document andwe now in this case the solution is text. A probability of 0.9 andthe a probability of 0.1. Now it's interesting to think about a scenario where we startadding more words to the document. So what would happen if we addmany the's to the document? Now this would change the game, right? So, how? Well, picture, what wouldthe likelihood function look like now? Well, it start with the likelihoodfunction for the two words, right? As we add more words, we know that. But we have to just multiplythe likelihood function by additional terms to account forthe additional. occurrences of that. Since in this case, all the additional terms are the,we're going to just multiply by this term. Right?For the probability of the. And if we have another occurrence of the,we'd multiply again by the same term, and so on and forth. Add as many terms as the number ofthe's that we add to the document, d'. Now this obviously changesthe likelihood function. So what's interesting is now to thinkabout how would that change our solution? So what's the optimal solution now? Now, intuitively you'd knowthe original solution, pulling the 9 versus pulling the ,will nolonger be optimal for this new function. Right? But, the question is howshould we change it. What general is to sum to one. So he know we must take away someprobability the mass from one word and add the probabilitymass to the other word. The question is which word tohave reduce the probability and which word to have a larger probability. And in particular,let's think about the probability of the. Should it be increasedto be more than 0.1? Or should we decrease it to less than 0.1? What do you think? Now you might want to pause the videoa moment to think more about. This question. Because this has to do with understandingof important behavior of a mixture model. And indeed,other maximum likelihood estimator. Now if you look at the formula fora moment, then you will see it seems like another object Function is moreinfluenced by the than text. Before, each computer. So now as you can imagine,it would make sense to actually assign a smaller probability fortext and lock it. To make room fora larger probability for the. Why?Because the is repeated many times. If we increase it a little bit,it will have more positive impact. Whereas a slight decrease of textwill have relatively small impact because it occurred just one, right? So this means there is anotherbehavior that we observe here. That is high frequency wordsgenerated with high probabilities from all the distributions. And, this is no surprise at all, because after all, we are maximizingthe likelihood of the data. So the more a word occurs, then itmakes more sense to give such a word a higher probability because the impactwould be more on the likelihood function. This is in fact a very general phenomenonof all the maximum likelihood estimator. But in this case, we can see as wesee more occurrences of a term, it also encourages the unknowndistribution theta sub d to assign a somewhat higherprobability to this word. Now it's also interesting to think aboutthe impact of probability of Theta sub B. The probability of choosing oneof the two component models. Now we've been so far assumingthat each model is equally likely. And that gives us 0.5. But you can again look at this likelihoodfunction and try to picture what would happen if we increase the probabilityof choosing a background model. Now you will see these terms for the, we have a different form wherethe probability that would be even larger because the background hasa high probability for the word and the coefficient in front of 0.9 whichis now 0.5 would be even larger. When this is larger,the overall result would be larger. And that also makes thisthe less important for theta sub d to increasethe probability before the. Because it's already very large. So the impact here of increasingthe probability of the is somewhat regulated by this coefficient,the point of i. If it's larger on the background, then it becomes less importantto increase the value. So this means the behavior here, which is high frequency words tend to getthe high probabilities, are effected or regularized somewhat by the probabilityof choosing each component. The more likely a componentis being chosen. It's more important that to have highervalues for these frequent words. If you have a various small probability ofbeing chosen, then the incentive is less. So to summarize,we have just discussed the mixture model. And we discussed that the estimationproblem of the mixture model and particular with this discussed somegeneral behavior of the estimator and that means we can expect ourestimator to capture these infusions. First every component model attempts to assign high probabilities tohigh frequent their words in the data. And this is to collaborativelymaximize likelihood. Second, different component models tend tobet high probabilities on different words. And this is to avoid a competition orwaste of probability. And this would allow them to collaboratemore efficiently to maximize the likelihood. So, the probability of choosing eachcomponent regulates the collaboration and the competition between component models. It would allow some component modelsto respond more to the change, for example, of frequency ofthe theta point in the data. We also talked about the special caseof fixing one component to a background word distribution, right? And this distribution can be estimatedby using a collection of documents, a large collection of English documents,by using just one distribution and then we'll just have normalizedfrequencies of terms to give us the probabilitiesof all these words. Now when we use sucha specialized mixture model, we show that we can effectively get ridof that one word in the other component. And that would make this covertopic more discriminative. This is also an example of imposinga prior on the model parameter and the prior here basically means one modelmust be exactly the same as the background language model and if you recall what wetalked about in Bayesian estimation, and this prior will allow us to favor a modelthat is consistent with our prior. In fact, if it's not consistent we'regoing to say the model is impossible. So it has a zero prior probability. That effectively excludes such a scenario. This is also issue thatwe'll talk more later. [MUSIC]
The following policies will be enforced while taking your exam:    * The exam will be closed notes (i.e., no external material allowed).     * You may use two sheets of scratch paper during the exam.     * No calculators are allowed.     * No bathroom breaks are allowed.   If you are disconnected during the exam for less than 5 minutes, log back in to ProctorU as soon as you're able to resume the exam. If you are disconnected for more than 5 minutes, you must reschedule the exam with ProctorU. Make sure your internet connection is stable before you begin the exam. Also be sure to submit your exam before disconnecting from ProctorU.  ##  For Live Technical Support  If you run into technical issue accessing the exam before taking it, you can contact Coursera Live Chat for technical assistance. If you encounter any issue during the exam, the proctor will seek assistance on your behalf.  To access Coursera Live Chat:    1. Go to Coursera Learner Center: [ https://learner.coursera.help ](https://learner.coursera.help/) .     2. Make sure to log in to your Coursera account.     3. On the bottom right of the page, you will be prompted with a chat live that connects you to a Coursera live assistant.     4. After you are connected with the live chat agent, make sure that you mention to the agent that you are an registered student in the MCS-DS program from University of Illinois.   
##  Exam Instructions    * A password quiz precedes and unlocks the proctored exam. The proctor will enter the password for you.     * Under no circumstances should you type in any password or make an attempt in the password quiz.     * **Note:** **If the exam does not immediately unlock after the Proctor enters the password, please let the Proctor know that you are going to refresh the page, which should resolve the issue. Then refresh the page.**  ##  How to Schedule the Proctored Exam  **Note: you need to schedule the exam on ProctorU website at[ https://go.proctoru.com ](https://go.proctoru.com/) yourself by following the instructions below. **    1. If this is your first time scheduling an exam with ProctorU, click on the **Sign Up** link on the [ University of Illinois ProctorU portal page ](http://proctoru.com/portal/illinois) to create a new user account and ProctorU log-in password. It is recommended to use the University of Illinois email address to register for the account.If you have a ProctorU account already, you can skip this step and go to [ https://go.proctoru.com ](https://go.proctoru.com/) to schedule the exam.     2. Once account is created, from the **My Exams** page, click the **Schedule New Exam** button to schedule the exam date and time.     3. Fill in the **institution** , **term** and **exam** from the dropdown menus, and click the **Find Reservations** button.     4. On the **Schedule Exam** page, select a reservation time from the calendar at the bottom of the screen. **IMPORTANT: If you do not see any reservations listed, please select the "View All" radio button next to "Filter Results" to display exam times that are outside your specified preference.**    5. Click the **BOOK IT** button next to your desired exam appointment time.     6. Confirm your selection on the next page and click the **Proceed to Cart** button when you are ready to continue.     7. In your cart, click the **Proceed to Checkout** button.     8. Enter the appropriate credit card information and click **Make Payment** .     9. You will see an exam confirmation page and will receive an email message with your scheduled exam information.   ##  How to Take the Proctored Exam    1. At the date and time of the exam appointment, login to the [ University of Illinois ProctorU portal page ](http://proctoru.com/portal/illinois/) .     2. After logging in, you will see a countdown to the proctored exam time at the top of the page. Prior to the exam appointment, you may reschedule using the Reschedule button.     3. At the appointment time, a **Start** button will appear next to the appointment. Click the **Start** button, and you will be connected to a proctor who will guide you through the proctored exam process.     4. You should navigate to the password quiz in the course site and open the password quiz for the proctor.     5. Proctor will type in the password in the password quiz for you. Under no circumstances should you type in a password. **Please turn your head around when the proctor enters the password** .     6. You can submit the password quiz after the proctor enters the password.     7. Once the password is accepted, you can start the proctored exam under the proctor's supervision.     8. Proctor must witness you submit the exam. Make sure to follow the proctor's instructions and remain connected until you are instructed to disconnect by the proctor. If for any reason the session gets disconnected, try to reconnect immediately and if still unable contact ProctorU. If you remain disconnected from the proctor while submitting the exam, an incident report will be submitted to the instructor.   
[SOUND] In this lecture,we're going to talk about the text access. In the previous lecture, we talked aboutthe natural language content, analysis. We explained that the state of the arenatural language processing techniques are still not good enough to processa lot of unrestricted text data in a robust manner. As a result, bag of words remains very popular inapplications like a search engine. In this lecture, we're going to talkabout some high-level strategies to help users get access to the text data. This is also important step to convertraw big text data into small random data. That are actually neededin a specific application. So the main question we'll address here,is how can a text information system, help usersget access to the relevant text data? We're going to cover two complimentarystrategies, push versus pull. And then we're going to talk abouttwo ways to implement the pull mode, querying versus browsing. So first push versus pull. These are two different ways connectthe users with the right information at the right time. The difference is whichtakes the initiative, which party takes the initiative. In the pull mode, the users take the initiative tostart the information access process. And in this case, a user typically woulduse a search engine to fulfill the goal. For example,the user may type in the query and then browse the results tofind the relevant information. So this is usually appropriate for satisfying a user's adhoc information need. An ad hoc information need isa temporary information need. For example, you want to buy a product so you suddenly have a need to readreviews about related product. But after you have cracked information,you have purchased in your product. You generally no longerneed such information, so it's a temporary information need. In such a case, it's very hard fora system to predict your need, and it's more proper forthe users to take the initiative, and that's why search engines are very useful. Today because many people have manyinformation needs all the time. So as we're speaking Google is probablyprocessing many queries from this. And those are all, or mostly adequate. Information needs. So this is a pull mode. In contrast in the push mode inthe system would take the initiative to push the information to the user orto recommend information to the user. So in this case this is usuallysupported by a recommender system. Now this would be appropriate if. The user has a stable information. For example you may have a researchinterest in some topic and that interest tends to stay for a while. So, it's rather stable. Your hobby is another example of. A stable information need is such a casethe system can interact with you and can learn your interest, andthen to monitor the information stream. If the system hasn't seen anyrelevant items to your interest, the system could then take the initiativeto recommend the information to you. So, for example, a news filter or news recommended system couldmonitor the news stream and identify interesting news to you andsimply push the news articles to you. This mode of information access may bealso a property that when this system has good knowledge about the users needand this happens in the search context. So for example, when you search forinformation on the web a search engine might infer you might bealso interested in something related. Formation. And they would recommend the informationto you, so that just reminds you, for example, of an advertisementplaced on the search page. So this is about the two high levelstrategies or two modes of text access. Now let's look at the pullmode in more detail. In the pull mode, we can furtherdistinguish it two ways to help users. Querying versus browsing. In querying,a user would just enter a query. Typical the keyword query, and the search engine system wouldreturn relevant documents to use. And this works well when the user knowswhat exactly are the keywords to be used. So if you know exactlywhat you are looking for, you tend to know the right keywords. And then query works very well,and we do that all of the time. But we also know that sometimesit doesn't work so well. When you don't know the rightkeywords to use in the query, or you want to browse informationin some topic area. You use because browsingwould be more useful. So in this case, in the case of browsing,the users would simply navigate it, into the relevant informationby following the paths supported by the structures of documents. So the system would maintainsome kind of structures and then the user could followthese structures to navigate. So this really works well when the userwants to explore the information space or the user doesn't know whatare the keywords to using the query. Or simply because the user finds itinconvenient to type in a query. So even if a user knows what query totype in if the user is using a cellphone to search for information. It's still harder to enter the query. In such a case, again,browsing tends to be more convenient. The relationship between browsing andquerying is best understood by making and imagine you're site seeing. Imagine if you're touring a city. Now if you know the exactaddress of attraction. Taking a taxi there isperhaps the fastest way. You can go directly to the site. But if you don't know the exact address,you may need to walk around. Or you can take a taxi to a nearbyplace and then walk around. It turns out that we do exactlythe same in the information studies. If you know exactly what youare looking for, then you can use the right keywords in your queryto find the information you're after. That's usually the fastest way to do,find information. But what if you don't knowthe exact keywords to use? Well, you clearly probably won't so well. You will not related pages. And then, you need to also walkaround in the information space, meaning by following the links orby browsing. You can then finally getinto the relevant page. If you want to learn about again. You will likely do a lot of browsing so just like you are looking around insome area and you want to see some interesting attractionsrelated in the same. [INAUDIBLE]. So this analogy also tells us thattoday we have very good support for query, but we don't really havegood support for browsing. And this is because in orderto browse effectively, we need a map to guide us,just like you need a map to. Of Chicago, through the city of Chicago, you need atopical map to tour the information space. So how to construct such a topicalmap is in fact a very interesting research question that might bring us more interesting browsing experienceon the web or in applications. So, to summarize this lecture, we've talked about the two high levelstrategies for text access; push and pull. Push tends to be supported bythe Recommender System, and Pull tends to be supportedby the Search Engine. Of course, in the sophisticated[INAUDIBLE] information system, we should combine the two. In the pull mode, we can further this[INAUDIBLE] Querying and Browsing. Again we generally want to combinethe two ways to help you assist, so that you can supportthe both querying nad browsing. If you want to know more aboutthe relationship between pull and push, you can read this article. This give excellent discussion of therelationship between machine filtering and information retrieval. Here informational filtering is similarto information recommendation or the push mode of information access. [MUSIC]
[MUSIC] In this lecture, we continuethe discussion of vector space model. In particular, we're going totalk about the TF transformation. In the previous lecture, we have derived a TF idea of weightingformula using the vector space model. And we have assumed that this modelactually works pretty well for these examples as shown on this slide,except for d5, which has received a very high score. Indeed, it has received the highestscore among all these documents. But this document is intuitive andnon-relevant, so this is not desirable. In this lecture,we're going to talk about, how we're going to use TFtransformation to solve this problem. Before we discuss the details,let's take a look at the formula for this simple TF-IDFweighting ranking function. And see why this document hasreceived such a high score. So this is the formula, andif you look at the formula carefully, then you will see it involves a sumover all the matched query terms. And inside the sum, each matchedquery term has a particular weight. And this weight is TF-IDF weighting. So it has an idea of component,where we see two variables. One is the total number of documentsin the collection, and that is M. The other is the document of frequency. This is the number ofdocuments that are contained. This word w. The other variables involved in the formula includethe count of the query term. W in the query, andthe count of the word in the document. If you look at this document again,now it's not hard to realize that the reason why it hasn'treceived a high score is because it has a very high count of campaign. So the count of campaign in this documentis a 4, which is much higher than the other documents, and has contributedto the high score of this document. So in treating the amountto lower the score for this document, we need to somehowrestrict the contribution of the matching of thisterm in the document. And if you think about the matchingof terms in the document carefully, you actually would realize, we probably shouldn't rewardmultiple occurrences so generously. And by that I mean,the first occurrence of a term says a lot aboutthe matching of this term, because it goes from zerocount to a count of one. And that increase means a lot. Once we see a word in the document, it's very likely that the documentis talking about this word. If we see a extra occurrence ontop of the first occurrence, that is to go from one to two,then we also can say that, well the second occurrence kind of confirmed that it'snot a accidental managing of the word. Now we are more sure that thisdocument is talking about this word. But imagine we have seen, let's say,50 times of the word in the document. Now, adding one extra occurrence is notgoing to test more about the evidence, because we're already sure thatthis document is about this word. So if you're thinking this way, it seemsthat we should restrict the contribution of a high count of a term, andthat is the idea of TF Transformation. So this transformation function isgoing to turn the real count of word into a term frequency weight forthe word in the document. So here I show in x axis that we'll count,and y axis I show the term frequency weight. So in the previous breaking functions, we actually have imprison rateuse some kind of transformation. So for example,in the 0/1 bit vector recantation, we actually use such a transformationfunction, as shown here. Basically if the count is 0,then it has 0 weight, otherwise it would have a weight of 1. It's flat. Now, what about usingterm count as TF weight? Well, that's a linear function, so it hasjust exactly the same weight as the count. Now we have just seen thatthis is not desirable. So what we want is something like this. So for example,with an algorithm function, we can't have a sublineartransformation that looks like this. And this will control the influenceof really high weight, because it's going to lower its inference. Yet, it will retainthe inference of small counts. Or we might want to even bend the curvemore by applying logarithm twice. Now people have tried all these methods. And they are indeed working better thanthe linear form of the transformation. But so far, what works the best seemsto be this special transformation, called a BM25 transformation. BM stands for best matching. Now in this transformation,you can see there's a parameter k here. And this k controls the upperbound of this function. It's easy to see thisfunction has a upper bound, because if you look at the x divided byx + k, where k is a non-active number, then the numerator will never be ableto exceed the denominator, right? So it's upper bounded by k+1. Now, this is also difference betweenthis transformation function and a logarithm transformation. Which it doesn't have upper bound. Furthermore, one interesting propertyof this function is that, as we vary k, we can actually simulate differenttransformation functions. Including the two extremesthat are shown here. That is, the 0/1 bit transformation andthe linear transformation. So for example, if we set k to 0,now you can see the function value will be 1. So we precisely recoverthe 0/1 bit transformation. If you set k to very largenumber on the other hand, it's going to look more likethe linear transformation function. So in this sense,this transformation is very flexible. It allows us to controlthe shape of the transformation. It also has a nice propertyof the upper bound. And this upper bound is useful to controlthe inference of a particular term. And so that we can prevent a spammerfrom just increasing the count of one term to spam all queriesthat might match this term. In other words, this upper boundmight also ensure that all terms would be counted when we aggregatethe weights to compute the score. As I said, this transformationfunction has worked well so far. So to summarize this lecture,the main point is that we need to do Sublinear TF Transformation,and this is needed to capture the intuition of diminishingreturn from higher term counts. It's also to avoid the dominance byone single term over all others. This BM25 transformation that wetalked about is very interesting. It's so far one of the best-performingTF Transformation formulas. It has upper bound, and soit's also robust and effective. Now if we're plugging this function intoour TF-IDF weighting vector space model. Then we'd end up havingthe following ranking function, which has a BM25 TF component. Now, this is alreadyvery close to a state of the odd ranking function called BM25. And we'll discuss how we can furtherimprove this formula in the next lecture. [MUSIC]
[SOUND]This lecture is about the basic measures forevaluation of text retrieval systems. In this lecture,we're going to discuss how we design basic measures to quantitativelycompare two retrieval systems. This is a slide that you have seenearlier in the lecture where we talked about the Granvilleevaluation methodology. We can have a test faction that consistsof queries, documents, and [INAUDIBLE]. We can then run two systems on thesedata sets to contradict the evaluator. Their performance. And we raise the question,about which set of results is better. Is system A better or is system B better? So let's now talk about how toaccurately quantify their performance. Suppose we have a total of 10 relevantdocuments in the collection for this query. Now, the relevant judgments show onthe right in [INAUDIBLE] obviously. And we have only seen 3 [INAUDIBLE] there,[INAUDIBLE] documents there. But, we can imagine there are other Randomdocuments in judging for this query. So now, intuitively,we thought that system A is better because itdid not have much noise. And in particular we have seenthat among the three results, two of them are relevant but in system B, we have five results andonly three of them are relevant. So intuitively it looks likesystem A is more accurate. And this infusion can be capturedby a matching holder position, where we simply compute to what extentall the retrieval results are relevant. If you have 100% position, that would mean that allthe retrieval documents are relevant. So in this case system A hasa position of two out of three System B has somesweet hold of 5 and this shows that systemA is better frequency. But we also talked about System Bmight be prefered by some other units would like to retrieve as manyrandom documents as possible. So in that case we'll have to comparethe number of relevant documents that they retrieve andthere's another method called recall. This method uses the completenessof coverage of random documents In your retrieval result. So we just assume that there are tenrelevant documents in the collection. And here we've got two of them,in system A. So the recall is 2 out of 10. Whereas System B has called a 3,so it's a 3 out of 10. Now we can see by recallsystem B is better. And these two measures turn out tobe the very basic of measures for evaluating search engine. And they are very important becausethey are also widely used in many other test evaluation problems. For example, if you look atthe applications of machine learning, you tend to see precision recall numbersbeing reported and for all kinds of tasks. Okay so, now let's define thesetwo measures more precisely. And these measures are to evaluate a setof retrieved documents, so that means we are considering that approximationof the set of relevant documents. We can distinguish 4 cases dependingon the situation of the documents. A document can be retrieved ornot retrieved, right? Because we are talkingabout a set of results. A document can be also relevant or not relevant depending on whether the userthinks this is a useful document. So we can now have counts of documents in. Each of the four categories againhave a represent the number of documents that have been retrieved andrelevant. B for documents that are not retrieved butrather etc. No with this table thenwe can define precision. As the ratio of the relevant retrieved documents A to the totalof relevant retrieved documents. So, this is just A dividedby The sum of a and c. The sum of this column. Singularly recall is defined bydividing a by the sum of a and b. So that's again to divide a by. The sum of the row instead of the column. All right, so we can see precision andrecall is all focused on looking at the a, that's the number ofretrieved relevant documents. But we're going to usedifferent denominators. Okay, so what would be an ideal result. Well, you can easily see beingthe ideal case would have precision and recall oil to be 1.0. That means We have got 1% ofall the Relevant documents in our results, and all of the resultsthat we returned all Relevant. At least there's no singleNot Relevant document returned. In reality, however, high recall tendsto be associated with low precision. And you can imagine why that's the case. As you go down the to try to get asmany random documents as possible, you tend to encounter a lot of documents,so the precision has to go down. Note that this set can alsobe defined by a cut off. In the rest of this, that's why althoughthese two measures are defined for retrieve the documents, they are actuallyvery useful for evaluating a rank list. They are the fundamental measures intask retrieval and many other tasks. We often are interested in The precisionat ten documents for web search. This means we look at how many documents among the top ten resultsare actually relevant. Now, this is a very meaningful measure, because it tells us how many relevantdocuments a user can expect to see On the first page of where theytypically show ten results. So precision and recallare the basic matches and we need to use them to further evaluate a searchengine, but they are the Building blocks. We just said that there tends to bea trailoff between precision and recall, so naturally it would beinteresting to combine them. And here's one method that's often used,called F-measure And it's a [INAUDIBLE] mean of precision andrecall as defined on this slide. So, you can see at first, compute the. Inverse of R and P here,and then it would interpret the 2 by using coefficientsdepending on parameter beta. And after some transformation you caneasily see it would be of this form. And in any case it just becomesan agent of precision and recall, and beta is a parameter,that's often set to 1. It can control the emphasison precision or recall always set beta to 1 We end up having a specialcase of F-Measure, often called F1. This is a popular measure that's oftenused as a combined precision and recall. And the formula looks very simple. It's just this, here. Now it's easy to see that ifyou have a Larger precision, or larger recall than fmeasure would be high. But, what's interesting is thatthe trade off between precision and recall is capturedan interesting way in f1. So, in order to understand that, we can first look at the naturalWhy not just the combining and using the symbol arithmeticallyas efficient here? That would be likely the most natural wayof combining them So what do you think? If you want to think more,you can pause the video. So why is this not as good as F1? Or what's the problem with this? Now, if you think aboutthe arithmetic mean, you can see this isthe sum of multiple terms. In this case,it's the sum of precision and recall. In the case of a sum, the total valuetends to be dominated by the large values. that means if you have a very high P orvery high R then you really don't care about whether the other valueis low so the whole sum would be high. Now this is not desirable because onecan easily have a perfect recall. We have perfect recall easily. Can we imagine how? It's probably very easy toimagine that we simply retrieve all the documents in the collection andthen we have a perfect recall. And this will give us 0.5 as the average. But such results are clearly notvery useful for the users even though the average using thisformula would be relevantly high. In contrast you can see F 1 wouldreward a case where precision and recall are roughly That seminar, so it would a case where you hadextremely high value for one of them. So this means f one encodesa different trade off between that. Now this example showsactually a very important. Methodology here. But when you try to solve a problem youmight naturally think of one solution, let's say in this it'sthis error mechanism. But it's important not tosettle on this source. It's important to think whether youhave other ways to combine that. And once you think about the multiplevariance It's important to analyze their difference, and then think aboutwhich one makes more sense. In this case, if you think more carefully, you will think that F1probably makes more sense. Than the simple. Although in other cases theremay be different results. But in this case the seems not reasonable. But if you don't pay attentionto these subtle differences you might just take a easy way tocombine them and then go ahead with it. And here later, you will find that,the measure doesn't seem to work well. All right. So this methodology is actually veryimportant in general, in solving problems. Try to think about the best solution. Try to understand the problem very well,and then know why you needed this measure, and whyyou need to combine precision and recall. And then use that to guide you infinding a good way to solve the problem. To summarize, we talked aboutprecision which addresses the question are there retrievableresults all relevant? We also talk about the Recall. Which addresses the question, have all ofthe relevant documents been retrieved. These two, are the two,basic matches in text and retrieval in. They are used formany other tasks, as well. We talk about F measure as a way tocombine Precision Precision and recall. We also talked about the tradeoffbetween precision and recall. And this turns out to dependon the user's search tasks and we'll discuss this pointmore in a later lecture. [MUSIC]
[SOUND] This lecture is aboutthe statistical language model. In this lecture, we're going to give an introductionto statistical language model. This has to do with how do you modeltext data with probabilistic models. So it's related to how we modelquery based on a document. We're going to talk aboutwhat is a language model. And then we're going to talk about thesimplest language model called the unigram language model, which also happens to bethe most useful model for text retrieval. And finally, what this classwill use is a language model. What is a language model? Well, it's just a probabilitydistribution over word sequences. So here, I'll show one. This model gives the sequence Todayis Wednesday a probability of 0.001. It give Today Wednesday is a very, very small probabilitybecause it's non-grammatical. You can see the probabilitiesgiven to these sentences or sequences of words can varya lot depending on the model. Therefore, it's clearly context dependent. In ordinary conversation, probably Today is Wednesday is mostpopular among these sentences. Imagine in the context ofdiscussing apply the math, maybe the eigenvalue is positive,would have a higher probability. This means it can be used torepresent the topic of a text. The model can also be regardedas a probabilistic mechanism for generating text. And this is why it's also oftencalled a generating model. So what does that mean? We can imagine this is a mechanism that's visualised here as a stochastic systemthat can generate sequences of words. So, we can ask for a sequence,and it's to send for a sequence from the device if you want,and it might generate, for example, Today is Wednesday, but it couldhave generated any other sequences. So for example,there are many possibilities, right? So in this sense,we can view our data as basically a sample observed fromsuch a generating model. So, why is such a model useful? Well, it's mainly because it can quantifythe uncertainties in natural language. Where do uncertainties come from? Well, one source is simplythe ambiguity in natural language that we discussed earlier in the lecture. Another source is because we don'thave complete understanding, we lack all the knowledgeto understand the language. In that case,there will be uncertainties as well. So let me show some examples of questionsthat we can answer with a language model that would have interestingapplications in different ways. Given that we see John and feels,how likely will we see happy as opposed to habit as the nextword in a sequence of words? Now, obviously, this would be very usefulfor speech recognition because happy and habit would have similar acoustic sound,acoustic signals. But, if we look at the language model, we know that John feels happy would befar more likely than John feels habit. Another example, given that weobserve baseball three times and game once in a news article,how likely is it about sports? This obviously is related to textcategorization and information retrieval. Also, given that a user isinterested in sports news, how likely would the useruse baseball in a query? Now, this is clearly relatedto the query likelihood that we discussed in the previous lecture. So now,let's look at the simplest language model, called a unigram language model. In such a case, we assume that we generate a text bygenerating each word independently. So this means the probability ofa sequence of words would be then the product ofthe probability of each word. Now normally,they're not independent, right? So if you have single word in likea language, that would make it far more likely to observe model than ifyou haven't seen the language. So this assumption is notnecessarily true, but we make this assumptionto simplify the model. So now the model has precisely Nparameters, where N is vocabulary size. We have one probability for each word, andall these probabilities must sum to 1. So strictly speaking,we actually have N-1 parameters. As I said,text can then be assumed to be assembled, drawn from this word distribution. So for example,now we can ask the device or the model to stochastically generatethe words for us, instead of sequences. So instead of giving a whole sequence, like Today is Wednesday,it now gives us just one word. And we can get all kinds of words. And we can assemble thesewords in a sequence. So that will still allow youto compute the probability of Today is Wednesday as the productof the three probabilities. As you can see, even though we have notasked the model to generate the sequences, it actually allows us to computethe probability for all the sequences, but this model now only needs Nparameters to characterize. That means if we specifyall the probabilities for all the words, then the model'sbehavior is completely specified. Whereas if we don't make this assumption,we would have to specify probabilities for all kinds of combinationsof words in sequences. So by making this assumption, it makes itmuch easier to estimate these parameters. So let's see a specific example here. Here I show two unigram languagemodels with some probabilities. And these are high probabilitywords that are shown on top. The first one clearly suggestsa topic of text mining, because the high probabilitywas all related to this topic. The second one is more related to health. Now we can ask the question, how likely were observe a particulartext from each of these two models? Now suppose we samplewords to form a document. Let's say we take the first distribution,would you like to sample words? What words do you think would begenerated while making a text or maybe mining maybe another word? Even food, which has a very small probability,might still be able to show up. But in general, high probabilitywords will likely show up more often. So we can imagine what general textof that looks like in text mining. In fact, with small probability, you might be able to actually generatethe actual text mining paper. Now, it will actually be meaningful,although the probability will be very, very small. In an extreme case, you mightimagine we might be able to generate a text mining paper that would beaccepted by a major conference. And in that case,the probability would be even smaller. But it's a non-zero probability, if we assume none of the wordshave non-zero probability. Similarly from the second topic, we can imagine we can generatea food nutrition paper. That doesn't mean we cannot generate thispaper from text mining distribution. We can, but the probability would be very,very small, maybe smaller than even generating a paper that can be acceptedby a major conference on text mining. So the point is thatthe keeping distribution, we can talk about the probability ofobserving a certain kind of text. Some texts will have higherprobabilities than others. Now let's look at the problemin a different way. Suppose we now have availablea particular document. In this case, many of the abstract orthe text mining table, and we see these word counts here. The total number of words is 100. Now the question you ask hereis an estimation question. We can ask the question which model, which one of these distribution hasbeen used to generate this text, assuming that the text has been generatedby assembling words from the distribution. So what would be your guess? What we have to decide are whatprobabilities text mining, etc., would have. Suppose the view for a second, andtry to think about your best guess. If you're like a lot of people,you would have guessed that well, my best guess is text has a probabilityof 10 out of 100 because I've seen text 10 times, andthere are in total 100 words. So we simply normalize these counts. And that's in fact the word justified, and your intuition is consistentwith mathematical derivation. And this is called the maximumlikelihood estimator. In this estimator,we assume that the parameter settings of those that would give our observethe data the maximum probability. That means if we change theseprobabilities, then the probability of observing the particular textdata would be somewhat smaller. So you can see,this has a very simple formula. Basically, we just need to look atthe count of a word in a document, and then divide it by the total number ofwords in the document or document lens. Normalize the frequency. A consequence of this is, of course, we're going to assignzero probabilities to unseen words. If we have an observed word, there will be no incentive to assign anon-zero probability using this approach. Why? Because that would take away probabilitymass for these observed words. And that obviously wouldn't maximize the probability of thisparticular observed text data. But one has still question whetherthis is our best estimate. Well, the answer depends on what kindof model you want to find, right? This estimator gives a best modelbased on this particular data. But if you are interested in a modelthat can explain the content of the full paper for this abstract, then youmight have a second thought, right? So for thing,there should be other words in the body of that article, sothey should not have zero probabilities, even though they're notobserved in the abstract. So we're going to cover thisa little bit more later in this class in the querylikelihood model. So let's take a look at some possibleuses of these language models. One use is simply to useit to represent the topics. So here I show some generalEnglish background texts. We can use this text toestimate a language model, and the model might look like this. Right, so on the top, we have thoseall common words, the, a, is, we, etc., and then we'll see somecommon words like these, and then some very,very rare words in the bottom. This is a background language model. It represents the frequency ofwords in English in general. This is the background model. Now let's look at another text,maybe this time, we'll look at the computerscience research papers. So we have a collection ofcomputer science research papers, we do as mentioned again, we can justuse the maximum likelihood estimator, where we simply normalize the frequencies. Now in this case, we'll getthe distribution that looks like this. On the top, it looks similar becausethese words occur everywhere, they are very common. But as we go down,we'll see words that are more related to computer science,computer software, text, etc. And so although here, we might also seethese words, for example, computer, but we can imagine the probability here ismuch smaller than the probability here. And we will see many other words here thatwould be more common in general English. So you can see this distributioncharacterizes a topic of the corresponding text. We can look at even the smaller text. So in this case,let's look at the text mining paper. Now if we do the same,we have another distribution, again the can be expectedto occur in the top. The sooner we see text, mining,association, clustering, these words have relativelyhigh probabilities. In contrast, in this distribution, thetext has a relatively small probability. So this means, again,based on different text data, we can have a different model,and the model captures the topic. So we call this documentthe language model, and we call this collection language model. And later, you will see how they'reused in the retrieval function. But now,let's look at another use of this model. Can we statistically find what wordsare semantically related to computer? Now how do we find such words? Well, our first thought is that let's takea look at the text that match computer. So we can take a look at all the documentsthat contain the word computer. Let's build a language model. We can see what words we see there. Well, not surprisingly, we see thesecommon words on top as we always do. So in this case, this language model givesus the conditional probability of seeing the word in the context of computer. And these common words willnaturally have high probabilities. But we also see the computer itself and software will have relativelyhigh probabilities. But if we just use this model, we cannot just say all these wordsare semantically related to computer. So ultimately, what we'd like toget rid of is these common words. How can we do that? It turns out that it's possibleto use language model to do that. But I suggest you think about that. So how can we know whatwords are very common, so that we want to kindof get rid of them? What model will tell us that? Well, maybe you can think about that. So the background language modelprecisely tells us this information. It tells us what wasour common in general. So if we use this background model, we would know that these wordsare common words in general. So it's not surprising to observethem in the context of computer. Whereas computer has a verysmall probability in general, so it's very surprising that we have seencomputer with this probability, and the same is true for software. So then we can use these twomodels to somehow figure out the words that are related to computer. For example, we can simply take the ratioof these group probabilities and normalize the topic of language modelby the probability of the word in the background language model. So if we do that, we take the ratio,we'll see that then on the top, computer is ranked, andthen followed by software, program, all these wordsrelated to computer. Because they occur very frequently in thecontext of computer, but not frequently in the whole collection, whereas these commonwords will not have a high probability. In fact,they have a ratio about 1 down there because they are not reallyrelated to computer. By taking the sample of textthat contains the computer, we don't really see more occurrencesof that than in general. So this shows that even withthese simple language models, we can do some limitedanalysis of semantics. So in this lecture,we talked about language model, which is basically a probabilitydistribution over text. We talked about the simplest languagemodel called unigram language model, which is also just a word distribution. We talked about the twouses of a language model. One is we represent the topic in adocument, in a collection, or in general. The other is we discoverword associations. In the next lecture, we're going to talkabout how language model can be used to design a retrieval function. Here are two additional readings. The first is a textbook on statisticalnatural language processing. The second is an article thathas a survey of statistical language models with a lot ofpointers to research work. [MUSIC]
[SOUND]This lecture is about the feedbackin the vector space model. In this lecture, we continue talkingabout the feedback in text retrieval. Particularly, we're going to talk aboutfeedback in the vector space model. As we have discussed before,in the case of feedback the task of text retrieval system is removed fromexamples in improved retrieval accuracy. We will have positive examples. Those are the documents thatassume would be relevant or be charged with being relevant. All the documents thatare viewed by users. We also have negative examples. Those are documents knownto be non-relevant. They can also be the documentsthat are skipped by users. The general method inthe vector space model for feedback is to modify our query vector. We want to place the query vector ina better position to make it accurate. And what does that mean exactly? Well, if we think about the query vectorthat would mean we would have to do something to the vector elements. And in general,that would mean we might add new terms. Or we might just weight of old terms orassign weights to new terms. As a result, in general,the query will have more terms. We often call this query expansion. The most effective method inthe vector space model for feedback is called the Rocchio Feedback, which wasactually proposed several decades ago. So the idea is quite simple. We illustrate this idea byusing a two dimensional display of all the documents in the collection andalso the query vector. So now we can see the queryvector is here in the center, and these are all the documents. So when we use the query back there anduse the same narrative function to find the most similar documents,we are basically doing a circle here and that these documents would bebasically the top-ranked documents. And these process are relevant documents,and these are relevant documents,for example, it's relevant, etc. And then these minuses are negativedocuments, like these. So our goal here is trying to movethis query back to some position, to improve the retrieval accuracy. By looking at this diagram,what do you think? Where should we move the query vector so that we can improvethe retrieval accuracy? Intuitively, where do youwant to move query vector? If you want to think more,you can pause the video. If you think about this picture, you canrealize that in order to work well in this case you want the query vector to be asclose to the positive vectors as possible. That means ideally, you want to placethe query vectors somewhere here. Or we want to move the queryvector closer to this point. Now so what exactly is this point? Well, if you want these relevantdocuments to rank on the top, you want this to be in the center ofall these relevant documents, right? Because then if you drawa circle around this one, you'll get all these relevant documents. So that means we can move the queryvector towards the centroid of all the relevant document vectors. And this is basically the idea of Rocchio. Of course, you can considerthe centroid of negative documents and we want to move away fromthe negative documents. Now your match that we're talking aboutmoving vector closer to some other vec and away from other vectors. It just means that we have this formula. Here you can see this isoriginal query vector and this average basically is the centroidvector of relevant documents. When we take the average of these vectors, then were computingthe centroid of these vectors. Similarly, this is the average ofnon-relevant document like this. So it's essentially ofnon-relevant documents. And we have these three parameters here,alpha, beta, and gamma. They are controllingthe amount of movement. When we add these two vectors together, we're moving the query vectorcloser to the centroid. This is when we add them together. When we subtracted this part, we kind of move the queryvector away from that centroid. So this is the main ideaof Rocchio feedback. And after we have done this, we will get a new query vector whichcan be used to score documents. This new query vector,will then reflect the move of this original query vector toward thisrelevant centroid vector and away from the non-relevant value. Okay, so let's take a look at the example. This is the example thatwe've seen earlier. Only that I deemed that displayof the actual documents. I only showed the vectorrepresentation of these documents. We have five documents here and we have to read in the documents here, right. And they're displayed in red. And these are the term vectors. Now I have just assumed some of weights. A lot of terms,we have zero weights of course. Now these are negative arguments. There are two here. There is another one here. Now in this Rocchio method, we firstcompute the centroid of each category. And so let's see,look at the centroid vector of the positive documents, we simply just,so it's very easy to see. We just add this with this onethe corresponding element. And then that's down here andtake the average. And then we're going to addthe corresponding elements and then just take the average. And so we do this for all this. In the end, what we have is this one. This is the average vector of these two,so it's a centroid of these two. Let's also look at the centroidof the negative documents. This is basically the same. We're going to take the averageof the three elements. And these are the correspondingelements in the three vectors, and so on and so forth. So in the end, we have this one. Now in the Rocchio feedbackmethod we're going to combine all these with the originalquery vector which is this. So now let's see how wecombine them together. Well, that's basically this. So we have a parameter alphacontrolling the original query times weight that's one. And now we have beta to controlthe inference of the positive centroid of the weight, that's 1.5. That comes from here. All right, so this goes here. And we also have this negativeweight here gamma here. And this way, it has come from,of course, the negative centroid here. And we do exactly the same forother terms, each is for one term. And this is our new vector. And we're going to use this new queryvector, this one to rank the documents. You can imagine what would happen, right? Because of the movement that this onewould matches these red documents much better because we movedthis vector closer to them. And it's going to penalize these blackdocuments, these non relevent documents. So this is precisely whatwe wanted from feedback. Now of course if we apply this method inpractice we will see one potential problem and that is the original query hasonly four terms that are now zero. But after we do query explaining and merging, we'll have many timesthat would have non zero weights. So the calculation willhave to involve more terms. In practice,we often truncate this matter and only retain the termswith highest weights. So let's talk about how weuse this method in practice. I just mentioned that they'reoften truncated vector. Consider only a small number ofwords that have highest weights in the centroid vector. This is for efficiency concern. I also said here that negative examples,or non-relevant examples tend not to be very useful, especiallycompared with positive examples. Now you can think about why. One reason is because negative documentstend to distract the query in all directions. So, when you take the average, it doesn't really tell you whereexactly it should be moving to. Whereas positive documentstend to be clustered together. And they will point you toa consistent direction. So that also means that sometimes we don'thave to use those negative examples. But note that in some cases, in difficultqueries where most results are negative, negative feedback after is very useful. Another thing is to avoid over-fitting. That means we have to keep relativelyhigh weight on the original query terms. Why? Because the sample that we see infeedback Is a relatively small sample. We don't want to overlytrust the small sample. And the original query termsare still very important. Those terms are heightened by the user and the user has decided that thoseterms are most important. So in order to preventthe us from over-fitting or drifting, prevent topic drifting due tothe bias toward the feed backing symbols. We generally would have to keep a prettyhigh weight on the original terms so it was safe to do that. And this is especially true forpseudo relevance feedback. Now, this method can be used for both relevance feedback andpseudo-relevance feedback. In the case of pseudo-feedback, the primeand the beta should be set to a smaller value because the relevant examplesare assumed not to be relevant. They're not as reliable asthe relevance feedback. In the case of relevance feedback,we obviously could use a larger value. So those parameters,they have to be set empirically. And the Rocchio Method isusually robust and effective. It's still a very popular method forfeedback. [MUSIC]
[MUSIC] So now let's take a look at the specificmethod that's based on regression. Now, this is one of the manydifferent methods, and in fact, it's one of the simplest methods. And I choose this to explainthe idea because it's simple. So in this approach, we simply assumethat the relevance of document with respect to a query is related to a linearcombination of all the features. Here I used Xi to denote the feature. So Xi of Q and D is a feature. And we can have as manyfeatures as we would like. And we assume that these featurescan be combined in a linear manner. And each feature is controlledby a parameter here, and this beta i is a parameter. That's a weighting parameter. A larger value would mean the featurewould have a higher weight, and it would contribute moreto the scoring function. This specific form of the functionactually also involves a transformation ofthe probability of relevance. So this is the probability of relevance. And we know that the probability ofrelevance is within the range from 0 to 1. And we could have just assumed thatthe scoring function is related to this linear combination. So we can do a linear regression. But then, the value of this linearcombination could easily go beyond 1. So this transformationhere would map the 0 to 1 range to the wholerange of real values, you can verify it by yourself. So this allows us then to connectto the probability of variance which is between 0 and 1 to a linearcombination of arbitrary features. And if we rewrite this into a probabilityfunction, we would get the next one. So on this equation, now we'llhave the probability of relevance. And on the right hand side,we'll have this form. Now, this form is clearly nonnegative, and it still involves a linearcombination of features. And it's also clear that if this value is, this is actually negative of the linearcombination in the equation above. If this value here is large, then it would mean this value is small. And therefore,this whole probability would be large. And that's we expect, that basically,it would mean if this combination gives us a high value, thenthe document's more likely irrelevant. So this is our hypothesis. Again, this is not necessarily the besthypothesis, but this is a simple way to connect these features withthe probability of relevance. So now we have this combination function. The next task is toestimate the parameters so that the function cache will be applied. But without knowing the beta values,it's harder to apply this function. So let's see how canestimate our beta values. All right,let's take a look at a simple example. In this example, we have three features. One is the BM25 score of the document andthe query. One is the PageRank score of the document,which might or might not depend on the query. We might have a topic-sensitive PageRank,that would depend on the query. Otherwise, the general PageRankdoesn't really depend on the query. And then we have BM25 score onthe anchor test of the document. Now, these are then the feature values fora particular document query pair. And in this case, the document is D1 andthe judgment says that it's relevant. Here's another training instance andit's these feature values, but in this case, it's not relevant. This is an oversimplified case wherewe just have two instances, but it's sufficient to illustrate the point. So what we can do is we usethe maximum likelihood estimator to actually estimate the parameters. Basically, we're going topredict the relevance status of the document basedon the feature values. That is, given that we observedthese feature values here. Can we predict the relevance here? Now, of course, the prediction would beusing this function that you see here. And we hypothesize that the probabilityof relevance is related to features in this way. So we are going to see, for what values ofbeta we can predict the relevance well. What do we mean by predictingthe relevance well? Well, we just mean, in the first case, for D1 this expression right hereshould give high values. In fact, we'll hope thisto gave a value close to 1. Why?Because this is a relevant document. On the other hand,in the second case, for D2, we hope this value will be small, right. Why? Because it's a non-relevant document. So now let's see how this canbe mathematically expressed. And this is similar to expressingthe probability of document, only that we are not talking aboutthe probability of words, but talking about the probabilityof relevance, 1 or 0. So what's the probabilityof this document being relevant if it has these feature values? Well, this is just this expression. We just need to plug in the Xi's. So that's what we will get. It's exactly like what we have seen above, only that we replaced theseXi's with now specific values. So for example, this 0.7 goes to here and this 0.11 goes to here. And these are different feature values, and we combine them inthis particular way. The beta values are still unknown. But this gives us the probabilitythat this document is relevant, if we assume such a model. Okay? And we want to maximize this probability,since this is a relevant document. What do we do for the second document? Well, we want to compute the probabilitythat the prediction is non-relevant. So this would mean we have tocompute 1 minus this expression, since this expression is actuallythe probability of relevance. So to compute the non-relevancefrom relevance, we just do 1 minusthe probability of relevance. Okay? So this whole expression thenjust is our probability of predicting these two relevance values. One is 1 here, one is 0. And this whole equationis our probability of observing a 1 here and observing a 0 here. Of course, this probabilitydepends on the beta values. So then our goal is to adjustthe beta values to make this whole thing reach its maximum,make it as large as possible. So that means we're going to compute this. The beta is just the parametervalues that would maximize this whole likelihood expression. And what it means is,if you look at the function, is, we're going to choose betas tomake this as large as possible and make this also as large as possible,which is equivalent to say, make this part as small as possible. And this is precisely what we want. So once we do the training,now we will know the beta values. So then this functionwould be well-defined. Once beta values are known, both this andthis would be completely specified. So for any new query and new document, we can simply compute the features forthat pair. And then we just use this formulato generate the ranking score. And this scoring function can be used torank documents for a particular query. So that's the basic ideaof learning to rank. [MUSIC]
#  Overview  In this module, you will become familiar with the course, your instructor, your classmates, and our learning environment.  #  Time  This orientation should take approximately **3** **hours** to complete.  #  Goals and Objectives  The goal of the orientation module is to familiarize you with the course structure and the online learning environment. The orientation also helps you obtain the technical skills required for the course.  After this module, you should be able to:    * Recall important information about this course.     * Get to know your classmates.     * Be familiar with how discussion forums operate in the course.   #  Instructional Activities  Below is a list of the activities and assignments you must complete in this module. Click on the name of each activity for more detailed instructions.  **Activity**  |  **Estimated Time Required**      ---|---      Watch the [ Course Introduction Video ](https://www.coursera.org/learn/cs-410/lecture/MsDCs/course-introduction- video)  |  25 minutes      Read and review the [ Syllabus ](https://www.coursera.org/learn/cs-410/supplement/1gMIe/syllabus) , [ Course Deadlines ](https://www.coursera.org/learn/cs-410/supplement/wBlrv/course- deadlines-late-policies-and-academic-calendar) , and [ Course Communication ](https://www.coursera.org/learn/cs-410/supplement/SVcVq/course-communication) pages  |  45 minutes      Complete the [ Orientation Quiz ](https://www.coursera.org/learn/cs-410/exam/E81Mz/orientation-quiz)  |  15 minutes      Complete the course [ Pre-Quiz ](https://www.coursera.org/learn/cs-410/quiz/GYBV8/pre-quiz)  |  30 minutes  
[SOUND]This lecture is a continuing discussion of generativeprobabilistic models for tax classroom. In this lecture, we're going todo a finishing discussion of generative probabilistic models fortext crossing. So this is a slide that you have seenbefore and here, we show how we define the mixture model for text crossing andwhat the likelihood function looks like. And we can also computethe maximum likelihood estimate, to estimate the parameters. In this lecture, we're going to do talkmore about how exactly we're going to compute the maximum likelihood estimate. As in most cases the Algorithm can be usedto solve this problem for mixture models. So here's the detail of this Algorithm fordocument clustering. Now, if you have understoodhow Algorithm works for topic models like TRSA, andI think here it would be very similar. And we just need to adapt a littlebit to this new mixture model. So as you may recall Algorithm starts withinitialization of all the parameters. So this is the same as whathappened before for topic models. And then we're going to repeatuntil the likelihood converges and in each step we'll do E step and M step. In M step, we're going to infer which distributionhas been used to generate each document. So I have to introducea hidden variable Zd for each document and this variable could takea value from the range of 1 through k, representing k different distributions. More specifically basically, we're goingto apply base rules to infer which distribution is more likely tohave generated this document, or computing the posterior probability ofthe distribution given the document. And we know it's proportionalto the probability of selecting thisdistribution p of Z the i. And the probability of generating thiswhole document from the distribution which is the product of the probabilities ofworld for this document as you see here. Now, as you all clear this use forkind of remember, the normalizer orthe constraint on this probability. So in this case, we knowthe constraint on this probability in E-Step is that all the probabilitiesof Z equals i must sum to 1. Because the documented must have beengenerated from precisely one of these k topics. So the probability of being generatedfrom each of them should sum to 1. And if you know this constraint, thenyou can easily compute this distribution as long as you know whatit is proportional to. So once you compute this product thatyou see here, then you simply normalize these probabilities,to make them sum to 1 over all the topics. So that's E-Step, after E-Step wewant to know which distribution is more likely to have generated thisdocument d, and which is unlikely. And then in M-Step we're going tore-estimate all the parameters based on the in further z values or in furtherknowledge about which distribution has been used to generate which document. So the re-estimation involves two kindsof parameters 1 is p of theta and this is the probability of selectinga particular distribution. Before we observe anything, we don't have any knowledge aboutwhich cluster is more likely. But after we have observedthat these documents, then we can crack the evidence toinfer which cluster is more likely. And so this is proportional to the sum of the probability of Zsub d j is equal to i. And so this gives us allthe evidence about using topic i, theta i to generate a document. Pull them together and again,we normalize them into probabilities. So this is for key of theta sub i. Now the other kind of parametersare the probabilities of words in each distribution, in each cluster. And this is very similarto the case piz and here we just report the kindsof words that are in documents that are inferredto have been generated from a particular topic of theta i here. This would allows to thenestimate how many words have actually been generated from theta i. And then we'll normalize again theseaccounts in the probabilities so that the probabilities on allthe words would sum to up. Note that it's very important tounderstand these constraints as they are precisely the normalizingin all these formulas. And it's also important to knowthat the distribution is over what? For example, the probability oftheta is over all the k topics, that's why these kprobabilities will sum to 1. Whereas the probability of a word giventheta is a probability distribution over all the words. So there are many probabilities andthey have to sum to 1. So now, let's take a look ata simple example of two clusters. I've two clusters,I've assumed some initialized values for the two distributions. And let's assume we randomlyinitialize two probability of selecting each cluster as 0.5,so equally likely. And then let's consider onedocument that you have seen here. There are two occurrences of text andtwo occurrences of mining. So there are four words together andmedical and health did not occur in this document. So let's think about the hidden variable. Now for each document then wemuch use a hidden variable. And before in piz,we used one hidden variable for each work because that'sthe output from one mixture model. So in our case the outputfrom the mixture model or the observation from mixturemodel is a document, not a word. So now we have one hidden variableattached to the document. Now that hidden variable must tell uswhich distribution has been used to generate the document. So it's going to take two values,one and two to indicate the two topics. So now how do we infer whichdistribution has been used generally d? Well it's been used base rule,so it looks like this. In order for the first topictheta 1 to generate a document, two things must happen. First, theta sub 1 musthave been selected. So it's given by p of theta 1. Second, it must have also be generatingthe four words in the document. Namely, two occurrences of text andtwo occurrences of sub mining. And that's why you see the numeratorhas the product of the probability of selecting theta 1 and the probability ofgenerating the document from theta 1. So the denominator is just the sum oftwo possibilities of generality in this document. And you can plug in the numericalvalues to verify indeed in this case, the document is more likelyto be generated from theta 1, much more likely than from theta 2. So once we have this probability, we can easily compute the probabilityof Z equals 2, given this document. How? Well, we can use the constraint. That's going to be 1 minus 100 over 101. So now it's important that you notethat in such a computation there is a potential problem of underflow. And that is because if you look at theoriginal numerator and the denominator, it involves the competition ofa product of many small probabilities. Imagine if a document has many words and it's going to be a very small value herethat can cause the problem of underflow. So to solve the problem,we can use a normalize. So here you see that we takea average of all these two math solutions to compute average atthe screen called a theta bar. And this average distributionwould be comparable to each of these distributions in termsof the quantities or the magnitude. So we can then divide the numerator and the denominator both by this normalizer. So basically this normalizesthe probability of generating this document by using thisaverage word distribution. So you can see the normalizer is here. And since we have used exactly the samenormalizer for the numerator and the denominator. The whole value of this expression is notchanged but by doing this normalization you can see we can make the numerators andthe denominators more manageable in that the overall value is notgoing to be very small for each. And thus we can avoidthe underflow problem. In some other times we sometimesalso use logarithm of the product to convert this into a sumof log of probabilities. This can help preserve precision as well,but in this case we cannot usealgorithm to solve the problem. Because there is a sum in the denominator,but this kind of normalizes can beeffective for solving this problem. So it's a technique that's sometimesuseful in other situations in other situations as well. Now let's look at the M-Step. So from the E-Step we can see our estimateof which distribution is more likely to have generated a document at d. And you can see d1's more likegot it from the first topic, where is d2 is more likefrom second topic, etc. Now, let's think about what weneed to compute in M-step well basically we need tore-estimate all the parameters. First, look at p of theta 1 andp of theta 2. How do we estimate that? Intuitively you can just pool togetherthese z, the probabilities from E-step. So if all of these documents say,well they're more likely from theta 1, then we intuitively would givea higher probability to theta 1. In this case,we can just take an average of these probabilities that you see here andwe've obtain a 0.6 for theta 1. So 01 is more likely and then theta 2. So you can see probability of02 would be natural in 0.4. What about these word of probabilities? Well we do the same, andintuition is the same. So we're going to see, in order to estimate the probabilitiesof words in theta 1, we're going to look at which documentshave been generated from theta 1. And we're going to pull together the wordsin those documents and normalize them. So this is basically what I just said. More specifically, we're going to do forexample, use all the kinds of text in these documents for estimatingthe probability of text given theta 1. But we're not going to use theirraw count or total accounts. Instead, we can do that discount themby the probabilities that each document is likely been generated from theta 1. So these gives us somefractional accounts. And then these accountswould be then normalized in order to get the probability. Now, how do we normalize them? Well these probability ofthese words must assign to 1. So to summarize our discussion ofgenerative models for clustering. Well we show that a slight variationof topic model can be used for clustering documents. And this also shows the powerof generating models in general. By changing the generation assumption andchanging the model slightly we can achieve different goals, and we can capturedifferent patterns and types of data. So in this case, each cluster isrepresented by unigram language model word distribution andthat is similar to topic model. So here you can see the word distributionactually generates a term cluster as a by-product. A document that is generated by firstchoosing a unigram language model. And then generating all the wordsin the document are using just a single language model. And this is very different from againcopy model where we can generate the words in the document by usingmultiple unigram language models. And then the estimated model parametersare given both topic characterization of each cluster and the probabilistic assignment ofeach document into a cluster. And this probabilistic assignmentsometimes is useful for some applications. But if we want to achieveharder clusters mainly to partition documents intodisjointed clusters. Then we can just force a document intothe cluster corresponding to the words distribution that's most likelyto have generated the document. We've also shown that the Algorithm canbe used to compute the maximum likelihood estimate. And in this case, we need to use a special number addition techniqueto avoid underflow. [MUSIC]
[SOUND] This lecture is about, Opinion Mining and Sentiment Analysis, covering, Motivation. In this lecture,we're going to start, talking about, mining a different kind of knowledge. Namely, knowledge about the observer orhumans that have generated the text data. In particular, we're going to talk aboutthe opinion mining and sentiment analysis. As we discussed earlier, text datacan be regarded as data generated from humans as subjective sensors. In contrast, we have other devices suchas video recorder that can report what's happening in the real world objective togenerate the viewer data for example. Now the main difference between testdata and other data, like video data, is that it has rich opinions, and the content tends to be subjectivebecause it's generated from humans. Now, this is actually a unique advantagedof text data, as compared with other data, because the office is a greatopportunity to understand the observers. We can mine text data tounderstand their opinions. Understand people's preferences,how people think about something. So this lecture and the following lectureswill be mainly about how we can mine and analyze opinions buriedin a lot of text data. So let's start withthe concept of opinion. It's not that easy toformally define opinion, but mostly we would defineopinion as a subjective statement describing what a personbelieves or thinks about something. Now, I highlighted quite a few words here. And that's because it's worth thinkinga little bit more about these words. And that will help us betterunderstand what's in an opinion. And this further helps us todefine opinion more formally. Which is always needed to computation toresolve the problem of opinion mining. So let's first look at the keyword of subjective here. This is in contrast with objectivestatement or factual statement. Those statements can be proved right orwrong. And this is a key differentiatingfactor from opinions which tends to be noteasy to prove wrong or right, because it reflects whatthe person thinks about something. So in contrast, objective statement canusually be proved wrong or correct. For example, you might say thiscomputer has a screen and a battery. Now that's something you can check. It's either having a battery or not. But in contrast with this, think aboutthe sentence such as, this laptop has the best battery orthis laptop has a nice screen. Now these statementsare more subjective and it's very hard to provewhether it's wrong or correct. So opinion, is a subjective statement. And next lets look atthe keyword person here. And that indicates thatis an opinion holder. Because when we talk about opinion,it's about an opinion held by someone. And then we notice thatthere is something here. So that is the target of the opinion. The opinion is expressedon this something. And now, of course, believes orthinks implies that an opinion will depend on the culture orbackground and the context in general. Because a person might thinkdifferent in a different context. People from different backgroundmay also think in different ways. So this analysis shows that there aremultiple elements that we need to include in order to characterize opinion. So, what's a basic opinionrepresentation like? Well, it should include atleast three elements, right? Firstly, it has to specifywhat's the opinion holder. So whose opinion is this? Second, it must also specify the target,what's this opinion about? And third, of course,we want opinion content. And so what exactly is opinion? If you can identify these, we get a basic understanding of opinionand can already be useful sometimes. You want to understand further,we want enriched opinion representation. And that means we also want tounderstand that, for example, the context of the opinion andwhat situation was the opinion expressed. For example, what time was it expressed? We, also, would like to, people understandthe opinion sentiment, and this is to understand that what the opinion tellsus about the opinion holder's feeling. For example, is this opinion positive,or negative? Or perhaps the opinion holder was happy orwas sad, and so such understanding obviousto those beyond just Extracting the opinion content,it needs some analysis. So let's take a simpleexample of a product review. In this case, this actually expressed theopinion holder, and expressed the target. So its obviously whats opinion holder and that's just reviewer and its also oftenvery clear whats the opinion target and that's the product review forexample iPhone 6. When the review is posted usuallyyou can't such information easier. Now the content, of course,is a review text that's, in general, also easy to obtain. So you can see product reviews are fairly easy to analyze in terms of obtaininga basic opinion of representation. But of course, if you want to get moreinformation, you might know the Context, for example. The review was written in 2015. Or, we want to know that the sentimentof this review is positive. So, this additional understanding ofcourse adds value to mining the opinions. Now, you can see in this case the taskis relatively easy and that's because the opinion holder and the opiniontarget have already been identified. Now let's take a look atthe sentence in the news. In this case, we have a implicitholder and a implicit target. And the tasker is in general harder. So, we can identify opinion holder here,and that's the governor of Connecticut. We can also identify the target. So one target is Hurricane Sandy, but there is also another targetmentioned which is hurricane of 1938. So what's the opinion? Well, there's a negative sentiment here that's indicated by words like bad andworst. And we can also, then, identify context,New England in this case. Now, unlike in the playoff review, all these elements must be extracted byusing natural RAM processing techniques. So, the task Is much harder. And we need a deeper naturallanguage processing. And these examples also suggest that a lot of work can beeasy to done for product reviews. That's indeed what has happened. Analyzing andassembling news is still quite difficult, it's more difficult than the analysisof opinions in product reviews. Now there are also some otherinteresting variations. In fact, here we're going toexamine the variations of opinions, more systematically. First, let's think aboutthe opinion holder. The holder could be an individual orit could be group of people. Sometimes, the opinionwas from a committee. Or from a whole country of people. Opinion target accounts will vary a lot. It can be about one entity,a particular person, a particular product, a particular policy, ect. But it could be about a group of products. Could be about the productsfrom a company in general. Could also be very specificabout one attribute, though. An attribute of the entity. For example,it's just about the battery of iPhone. It could be someone else's opinion. And one person might comment onanother person's Opinion, etc. So, you can see there is a lot ofvariation here that will cause the problem to vary a lot. Now, opinion content, of course,can also vary a lot on the surface, you can identify one-sentence opinion orone-phrase opinion. But you can also have longertext to express an opinion, like the whole article. And furthermore we identifythe variation in the sentiment or emotion damage that's abovethe feeding of the opinion holder. So, we can distinguish a positiveversus negative or mutual or happy versus sad, separate. Finally, the opinioncontext can also vary. We can have a simple context, likedifferent time or different locations. But there could be also complex contexts, such as some backgroundof topic being discussed. So when opinion is expressed inparticular discourse context, it has to be interpreted in different ways thanwhen it's expressed in another context. So the context can be very [INAUDIBLE] toentire discourse context of the opinion. From computational perspective, we're mostly interested in what opinionscan be extracted from text data. So, it turns out that we canalso differentiate, distinguish, different kinds of opinions in textdata from computation perspective. First, the observer might makea comment about opinion targeting, observe the word Soin case we have the author's opinion. For example,I don't like this phone at all. And that's an opinion of this author. In contrast, the text might alsoreport opinions about others. So the person could also Make observationabout another person's opinion and reported this opinion. So for example,I believe he loves the painting. And that opinion is really about the It isreally expressed by another person here. So, it doesn't mean thisauthor loves that painting. So clearly, the two kinds of opinionsneed to be analyzed in different ways, and sometimes in product reviews, you can see, although mostly the opinionsare false from this reviewer. Sometimes, a reviewer might mentionopinions of his friend or her friend. Another complication is thatthere may be indirect opinions or inferred opinions that can be obtained. By making inferences on what's expressed in the text that mightnot necessarily look like opinion. For example, one statement that might be, this phone ran out ofbattery in just one hour. Now, this is in a way a factual statementbecause It's either true or false, right? You can even verify that,but from this statement, one can also infer some negative opinionsabout the quality of the battery of this phone, or the feeling ofthe opinion holder about the battery. The opinion holder clearly wishedthat the battery do last longer. So these are interesting variationsthat we need to pay attention to when we extract opinions. Also, forthis reason about indirect opinions, it's often also very useful to extractwhatever the person has said about the product, and sometimes factualsentences like these are also very useful. So, from a practical viewpoint, sometimes we don't necessarilyextract the subject of sentences. Instead, again, all the sentences thatare about the opinions are useful for understanding the person orunderstanding the product that we commend. So the task of opinion mining can bedefined as taking textualized input to generate a set ofopinion representations. Each representation we shouldidentify opinion holder, target, content, and the context. Ideally we can also infer opinionsentiment from the comment and the context to better understand. The opinion. Now often, some elements ofthe representation are already known. I just gave a good example inthe case of product we'd use where the opinion holder and the opiniontarget are often expressly identified. And that's not why this turns out to beone of the simplest opinion mining tasks. Now, it's interesting to think aboutthe other tasks that might be also simple. Because those are the caseswhere you can easily build applications by usingopinion mining techniques. So now that we have talked about what isopinion mining, we have defined the task. Let's also just talk a little bit aboutwhy opinion mining is very important and why it's very useful. So here, I identify three major reasons,three broad reasons. The first is it can help decision support. It can help us optimize our decisions. We often look at other people's opinions,look at read the reviews in order to make a decisions likebuying a product or using a service. We also would be interestedin others opinions when we decide whom to vote for example. And policy makers, may also want to know people'sopinions when designing a new policy. So that's one general,kind of, applications. And it's very broad, of course. The second application is to understandpeople, and this is also very important. For example, it could helpunderstand people's preferences. And this could help usbetter serve people. For example, we optimize a product searchengine or optimize a recommender system if we know what people are interested in,what people think about product. It can also help with advertising,of course, and we can have targeted advertising if we know what kind ofpeople tend to like what kind of plot. Now the third kind of applicationcan be called voluntary survey. Now this is most important researchthat used to be done by doing surveys, doing manual surveys. Question, answer it. People need to feel informsto answer their questions. Now this is directly related to humansas sensors, and we can usually aggregate opinions from a lot of humans throughkind of assess the general opinion. Now this would be very useful forbusiness intelligence where manufacturers want to know where their productshave advantages over others. What are the winningfeatures of their products, winning features of competitive products. Market research has to do withunderstanding consumers oppinions. And this create very useful directive forthat. Data-driven social science researchcan benefit from this because they can do text mining to understandthe people's opinions. And if you can aggregate a lot of opinionsfrom social media, from a lot of, popular information then you can actuallydo some study of some questions. For example, we can study the behavior ofpeople on social media on social networks. And these can be regarded as voluntarysurvey done by those people. In general, we can gain a lot of advantagein any prediction task because we can leverage the text data asextra data above any problem. And so we can use text basedprediction techniques to help you make predictions orimprove the accuracy of prediction. [MUSIC]
[SOUND]This lecture is aboutthe contextual text mining. Contextual text miningis related to multiple kinds of knowledge that we mine fromtext data, as I'm showing here. It's related to topic mining because youcan make topics associated with context, like time or location. And similarly, we can make opinionmining more contextualized, making opinions connected to context. It's related to text based predictionbecause it allows us to combine non-text data with text data to derivesophisticated predictors for the prediction problem. So more specifically, why are weinterested in contextual text mining? Well, that's first because textoften has rich context information. And this can include direct context suchas meta-data, and also indirect context. So, the direct context can growthe meta-data such as time, location, authors, andsource of the text data. And they're almost always available to us. Indirect context refers to additionaldata related to the meta-data. So for example, from office,we can further obtain additional context such as social network ofthe author, or the author's age. Such information is not in generaldirectly related to the text, yet through the process, we can connect them. There could be other textdata from the same source, as this one through the other text canbe connected with this text as well. So in general, any related datacan be regarded as context. So there could be removed orrated for context. And so what's the use? What is text context used for? Well, context can be used to partitiontext data in many interesting ways. It can almost allow us to partitiontext data in other ways as we need. And this is very importantbecause this allows us to do interesting comparative analyses. It also in general,provides meaning to the discovered topics, if we associate the text with context. So here's illustration of how context can be regarded as interestingways of partitioning of text data. So here I just showed some researchpapers published in different years. On different venues, different conference names here listed onthe bottom like the SIGIR or ACL, etc. Now such text data can be partitioned in many interesting waysbecause we have context. So the context here just includes time andthe conference venues. But perhaps we can includesome other variables as well. But let's see how we can partitionthis interesting of ways. First, we can treat eachpaper as a separate unit. So in this case, a paper ID and the,each paper has its own context. It's independent. But we can also treat all the paperswithin 1998 as one group and this is only possible becauseof the availability of time. And we can partition data in this way. This would allow us to compare topics forexample, in different years. Similarly, we can partitionthe data based on the menus. We can get all the SIGIR papers andcompare those papers with the rest. Or compare SIGIR papers with KDD papers,with ACL papers. We can also partition the data to obtainthe papers written by authors in the U.S., and that of course,uses additional context of the authors. And this would allow us to thencompare such a subset with another set of papers writtenby also seen in other countries. Or we can obtain a set ofpapers about text mining, and this can be compared withpapers about another topic. And note that thesepartitionings can be also intersected with each other to generateeven more complicated partitions. And so in general, this enablesdiscovery of knowledge associated with different context as needed. And in particular,we can compare different contexts. And this often gives usa lot of useful knowledge. For example, comparing topics over time,we can see trends of topics. Comparing topics in differentcontexts can also reveal differences about the two contexts. So there are many interesting questionsthat require contextual text mining. Here I list some very specific ones. For example, what topics havebeen getting increasing attention recently in data mining research? Now to answer this question, obviously we need to analyzetext in the context of time. So time is context in this case. Is there any difference in the responsesof people in different regions to the event, to any event? So this is a very broadan answer to this question. In this case of course,location is the context. What are the common researchinterests of two researchers? In this case, authors can be the context. Is there any difference in the researchtopics published by authors in the USA and those outside? Now in this case,the context would include the authors and their affiliation and location. So this goes beyond justthe author himself or herself. We need to look at the additionalinformation connected to the author. Is there any difference in the opinionsof all the topics expressed on one social network and another? In this case, the social network ofauthors and the topic can be a context. Other topics in news data thatare correlated with sudden changes in stock prices. In this case, we can use a time seriessuch as stock prices as context. What issues mattered in the 2012presidential campaign, or presidential election? Now in this case,time serves again as context. So, as you can see,the list can go on and on. Basically, contextual text miningcan have many applications. [MUSIC]
[SOUND] This lecture is about natural language content analysis. Natural language content analysisis the foundation of text mining. So we're going to first talk about this. And in particular, natural language processing witha factor how we can present text data. And this determines what algorithms canbe used to analyze and mine text data. We're going to take a look at the basicconcepts in natural language first. And I'm going to explain these concepts using a similar examplethat you've all seen here. A dog is chasing a boy on the playground. Now this is a very simple sentence. When we read such a sentencewe don't have to think about it to get the meaning of it. But when a computer has tounderstand the sentence, the computer has to gothrough several steps. First, the computer needsto know what are the words, how to segment the words in English. And this is very easy,we can just look at the space. And then the computer will needthe know the categories of these words, syntactical categories. So for example, dog is a noun,chasing's a verb, boy is another noun etc. And this is called a Lexical analysis. In particular, tagging these wordswith these syntactic categories is called a part-of-speech tagging. After that the computer also needs tofigure out the relationship between these words. So a and dog would form a noun phrase. On the playground would bea prepositional phrase, etc. And there is certain way forthem to be connected together in order for them to create meaning. Some other combinationsmay not make sense. And this is called syntactical parsing, or syntactical analysis,parsing of a natural language sentence. The outcome is a parse treethat you are seeing here. That tells us the structureof the sentence, so that we know how we caninterpret this sentence. But this is not semantics yet. So in order to get the meaning wewould have to map these phrases and these structures into some real worldantithesis that we have in our mind. So dog is a concept that we know,and boy is a concept that we know. So connecting these phrasesthat we know is understanding. Now for a computer, would have to formallyrepresent these entities by using symbols. So dog, d1 means d1 is a dog. Boy, b1 means b1 refers to a boy etc. And also represents the chasingaction as a predicate. So, chasing is a predicate here with three arguments, d1, b1, and p1. Which is playground. So this formal rendition ofthe semantics of this sentence. Once we reach that level of understanding,we might also make inferences. For example, if we assume there's a rulethat says if someone's being chased then the person can get scared, then wecan infer this boy might be scared. This is the inferred meaning,based on additional knowledge. And finally, we might even further infer what this sentence is requesting, or why the person who say it ina sentence, is saying the sentence. And so, this has to do withpurpose of saying the sentence. This is called speech act analysis orpragmatic analysis. Which first to the use of language. So, in this case a person saying thismay be reminding another person to bring back the dog. So this means when saying a sentence,the person actually takes an action. So the action here is to make a request. Now, this slide clearly shows thatin order to really understand a sentence there are a lot ofthings that a computer has to do. Now, in general it's very hard fora computer will do everything, especially if you would wantit to do everything correctly. This is very difficult. Now, the main reason why naturallanguage processing is very difficult, it's because it's designed it willmake human communications efficient. As a result, for example,with only a lot of common sense knowledge. Because we assume all ofus have this knowledge, there's no need to encode this knowledge. That makes communication efficient. We also keep a lot of ambiguities,like, ambiguities of words. And this is again, because we assume wehave the ability to disambiguate the word. So, there's no problem withhaving the same word to mean possibly different thingsin different context. Yet fora computer this would be very difficult because a computer does not havethe common sense knowledge that we do. So the computer will be confused indeed. And this makes it hard fornatural language processing. Indeed, it makes it very hard for every step in the slidethat I showed you earlier. Ambiguity is a main killer. Meaning that in every stepthere are multiple choices, and the computer would have todecide whats the right choice and that decision can be very difficultas you will see also in a moment. And in general, we need common sense reasoning in orderto fully understand the natural language. And computers today don't yet have that. That's why it's very hard for computers to precisely understandthe natural language at this point. So here are some specificexamples of challenges. Think about the world-level ambiguity. A word like design can be a noun ora verb, so we've got ambiguous part of speech tag. Root also has multiple meanings,it can be of mathematical sense, like in the square of, orcan be root of a plant. Syntactic ambiguity refersto different interpretations of a sentence in terms structures. So for example, natural language processing canactually be interpreted in two ways. So one is the ordinary meaning that we will be getting as we'retalking about this topic. So, it's processing of natural language. But there's is also anotherpossible interpretation which is to say languageprocessing is natural. Now we don't generally have this problem,but imagine for the computer to determine the structure, the computer would haveto make a choice between the two. Another classic example is a mansaw a boy with a telescope. And this ambiguity lies inthe question who had the telescope? This is called a prepositionalphrase attachment ambiguity. Meaning where to attach thisprepositional phrase with the telescope. Should it modify the boy? Or should it be modifying, saw, the verb. Another problem is anaphora resolution. In John persuaded Bill to buy a TV forhimself. Does himself refer to John or Bill? Presupposition is another difficulty. He has quit smoking impliesthat he smoked before, and we need to have such a knowledge inorder to understand the languages. Because of these problems, the stateof the art natural language processing techniques can not do anything perfectly. Even forthe simplest part of speech tagging, we still can not solve the whole problem. The accuracy that are listed here,which is about 97%, was just taken from some studies earlier. And these studies obviously have tobe using particular data sets so the numbers here are notreally meaningful if you take it out of the context of the dataset that are used for evaluation. But I show these numbers mainly to giveyou some sense about the accuracy, or how well we can do things like this. It doesn't mean any data setaccuracy would be precisely 97%. But, in general, we can do parsing speechtagging fairly well although not perfect. Parsing would be more difficult, but forpartial parsing, meaning to get some phrases correct, we can probablyachieve 90% or better accuracy. But to get the complete parse treecorrectly is still very, very difficult. For semantic analysis, we can also dosome aspects of semantic analysis, particularly, extraction of entities andrelations. For example, recognizing this isthe person, that's a location, and this person andthat person met in some place etc. We can also do word sense to some extent. The occurrence of root in this sentencerefers to the mathematical sense etc. Sentiment analysis is another aspectof semantic analysis that we can do. That means we can tag the sensesas generally positive when it's talking about the product ortalking about the person. Inference, however, is very hard,and we generally cannot do that for any big domain and if it's onlyfeasible for a very limited domain. And that's a generally difficultproblem in artificial intelligence. Speech act analysis isalso very difficult and we can only do this probably forvery specialized cases. And with a lot of help from humansto annotate enough data for the computers to learn from. So the slide also shows that computers are far from being able tounderstand natural language precisely. And that also explains why the textmining problem is difficult. Because we cannot rely onmechanical approaches or computational methods tounderstand the language precisely. Therefore, we have to usewhatever we have today. A particular statistical machine learningmethod of statistical analysis methods to try to get as much meaningout from the text as possible. And, later you will seethat there are actually many such algorithmsthat can indeed extract interesting model from text even thoughwe cannot really fully understand it. Meaning of all the naturallanguage sentences precisely. [MUSIC]
[SOUND]. This lecture is about the syntagmaticrelation discovery and mutual information. In this lecture we are going to continuediscussing syntagmatic relation discovery. In particular,we are going to talk about another the concept in the information series,we called it mutual information and how it can be used to discoversyntagmatic relations. Before we talked about the problemof conditional entropy and that is the conditional entropycomputed different pairs of words. It is not really comparable, sothat makes it harder with this cover, strong synagmatic relationsglobally from corpus. So now we are going to introduce mutualinformation, which is another concept in the information seriesthat allows us to, sometimes, normalize the conditional entropy to makeit more comparable across different pairs. In particular, mutual informationin order to find I(X:Y), matches the entropy reductionof X obtained from knowing Y. More specifically the question weare interested in here is how much of an entropy of X canwe obtain by knowing Y. So mathematically it can bedefined as the difference between the original entropy of X, andthe condition of Y of X given Y. And you might see,as you can see here it can also be defined as reduction of entropy ofY because of knowing X. Now normally the two conditionalinterface H of X given Y and the entropy of Y given X are not equal,but interestingly, the reduction of entropy by knowingone of them, is actually equal. So, this quantity is called a MutualInformation in order to buy I here. And this function has some interestingproperties, first it is also non-negative. This is easy to understand becausethe original entropy is always not going to be lower than the possibilityreduced conditional entropy. In other words, the conditional entropywill never exceed the original entropy. Knowing some information canalways help us potentially, but will not hurt us in predicting x. The signal property is that itis symmetric like additional entropy is not symmetrical,mutual information is, and the third property is that Itreaches its minimum, zero, if and only if the two random variablesare completely independent. That means knowing one of them does nottell us anything about the other and this last property can be verified bysimply looking at the equation above and it reaches 0 if andonly the conditional entropy of X [INAUDIBLE] Y is exactly the sameas original entropy of X. So that means knowing why it did nothelp at all and that is when X and a Y are completely independent. Now when we fix X to rank differentYs using conditional entropy would give the same order asranking based on mutual information because in the function here,H(X) is fixed because X is fixed. So ranking based on mutual entropy isexactly the same as ranking based on the conditional entropy of X given Y, but the mutual information allows us tocompare different pairs of x and y. So, that is why mutual information ismore general and in general, more useful. So, let us examine the intuitionof using mutual information for Syntagmatical Relation Mining. Now, the question we ask forcingthat relation mining is, whenever "eats" occurs,what other words also tend to occur? So this question can be framed asa mutual information question, that is, which words have high mutualinformation was eats, so computer the missing informationbetween eats and other words. And if we do that, and it is basicallya base on the same as conditional we will see that words thatare strongly associated with eats, will have a high point. Whereas words that are not relatedwill have lower mutual information. For this, I will give some example here. The mutual information between "eats" and"meats", which is the same as between "meats" and"eats," because the information is symmetrical is expected to be higher thanthe mutual information between eats and the, because knowing the does notreally help us as a predictor. It is similar, andknowing eats does not help us predicting, the as well. And you also can easilysee that the mutual information between a word anditself is the largest, which is equal tothe entropy of this word and so, because in this case the reduction is maximum because knowing one allowsus to predict the other completely. So the conditional entropy is zero, therefore the mutual informationreaches its maximum. It is going to be larger, then are equalto the machine volume eats in other words. In other words picking any other word and the computer picking between eats andthat word. You will not get any information largerthe computation from eats and itself. So now let us look at how tocompute the mute information. Now in order to do that, we often use a different form of mutualinformation, and we can mathematically rewrite the mutual informationinto the form shown on this slide. Where we essentially seea formula that computes what is called a KL-divergence or divergence. This is another termin information theory. It measures the divergencebetween two distributions. Now, if you look at the formula,it is also sum over many combinations of different values of the two randomvariables but inside the sum, mainly we are doing a comparisonbetween two joint distributions. The numerator has the joint, actual observed the joint distributionof the two random variables. The bottom part or the denominator can be interpreted as the expected jointdistribution of the two random variables, if they were independent because whentwo random variables are independent, they are joined distribution is equal tothe product of the two probabilities. So this comparison will tell us whetherthe two variables are indeed independent. If they are indeed independent then wewould expect that the two are the same, but if the numerator is differentfrom the denominator, that would mean the two variables are not independent andthat helps measure the association. The sum is simply to take intoconsideration of all of the combinations of the values of thesetwo random variables. In our case, each random variablecan choose one of the two values, zero or one, sowe have four combinations here. If we look at this form of mutualinformation, it shows that the mutual information matches the divergenceof the actual joint distribution from the expected distributionunder the independence assumption. The larger this divergence is, the higherthe mutual information would be. So now let us further look at whatare exactly the probabilities, involved in this formulaof mutual information. And here, this is all the probabilitiesinvolve, and it is easy for you to verify that. Basically, we have first to[INAUDIBLE] probabilities corresponding to the presence orabsence of each word. So, for w1,we have two probabilities shown here. They should sum to one, because a wordcan either be present or absent. In the segment, and similarly for the second word, we also have twoprobabilities representing presence or absences of this word, andthere is some to y as well. And finally, we have a lot ofjoined probabilities that represent the scenarios of co-occurrences ofthe two words, and they are shown here. And they sum to one because the twowords can only have these four possible scenarios. Either they both occur, so in that case both variables will havea value of one, or one of them occurs. There are two scenarios. In these two cases one of the randomvariables will be equal to one and the other will be zero and finally we havethe scenario when none of them occurs. This is when the two variablestaking a value of zero. So these are the probabilities involvedin the calculation of mutual information, over here. Once we know how to calculatethese probabilities, we can easily calculatethe mutual information. It is also interesting to know thatthere are actually some relations or constraint among these probabilities,and we already saw two of them, right? So in the previous slide, that you have seen thatthe marginal probabilities of these words sum to one andwe also have seen this constraint, that says the two words have thesefour scenarios of co-occurrency, but we also have some additionalconstraints listed in the bottom. For example, this one means if we add up the probabilities that we observethe two words occur together and the probabilities when the first wordoccurs and the second word does not occur. We get exactly the probabilitythat the first word is observed. In other words, when the word is observed. When the first word is observed, and there are only two scenarios, depending onwhether the second word is also observed. So, this probability captures the firstscenario when the second word actually is also observed, and this captures the second scenariowhen the second word is not observed. So, we only see the first word, and it is easy to see the other equationsalso follow the same reasoning. Now these equations allow us tocompute some probabilities based on other probabilities, andthis can simplify the computation. So more specifically,if we know the probability that a word is present, like in this case,so if we know this, and if we know the probability ofthe presence of the second word, then we can easily computethe absence probability, right? It is very easy to use thisequation to do that, and so we take care of the computation ofthese probabilities of presence and absence of each word. Now let's look atthe [INAUDIBLE] distribution. Let us assume that we also have available the probability thatthey occurred together. Now it is easy to see that we canactually compute all the rest of these probabilities based on these. Specifically forexample using this equation we can compute the probability that the first wordoccurred and the second word did not, because we know these probabilities inthe boxes, and similarly using this equation we can compute the probabilitythat we observe only the second word. Word. And then finally,this probability can be calculated by using this equation becausenow this is known, and this is also known, andthis is already known, right. So this can be easier to calculate. So now this can be calculated. So this slide shows that we onlyneed to know how to compute these three probabilitiesthat are shown in the boxes, naming the presence of each word and theco-occurence of both words, in a segment. [MUSIC]
This lecture is about the expectation-maximizationalgorithm, also called the EM algorithm. In this lecture, we'regoing to continue the discussion ofprobabilistic topic models. In particular, we're going tointroduce the EM algorithm, which is a family ofuseful algorithms for computing the maximum likelihood estimateof mixture models. So this is now familiar scenario ofusing a two component, the mixture model, to try to factor outthe background words from one topic wordof distribution here. So we're interested incomputing this estimate, and we're going to try to adjust these probability values to maximize the probabilityof the observed document. Note that we assume that all the other parameters are known. So the only thing unknown is the word probabilitiesgiven by theta sub. In this lecture, we'regoing to look into how to compute this maximumlikelihood estimate. Now, let's start with the idea of separating the words inthe text data into two groups. One group would be explainedby the background model. The other group wouldbe explained by the unknown topicword distribution. After all, this isthe basic idea of mixture model. But suppose we actually know which word is fromwhich distribution? So that would mean, for example, these words the, is, and we are known to be from this backgroundword distribution. On the other hand, theother words text, mining, clustering etc are known to be from the topic word distribution. If you can see the color, then these are shown in blue. These blue words are then assumed that to be fromthe topic word distribution. If we already know howto separate these words, then the problem of estimating the word distributionwould be extremely simple. If you think aboutthis for a moment, you'll realize that, well, we can simply takeall these words that are known to be from this word distribution theta sub dand normalize them. So indeed this problem would be very easy to solve if we had known which words are from which a distribution precisely, and this is in fact making this model nolonger a mixture model because we can already observe which distribution has been used to generatewhich part of the data. So we actually go back to the single worddistribution problem. In this case let's call these words that areknown to be from theta d, a pseudo document of d prime, and now all we need todo is just normalize these words countsfor each word w_i. That's fairly straightforward. It's just dictated by themaximum likelihood estimator. Now, this idea howeverdoesn't work because we in practice don't really know which word is fromwhich distribution, but this gives usthe idea of perhaps we can guess which word isfrom which it is written. Specifically givenall the parameters, can we infer the distributiona word is from. So let's assume that we actually know tentative probabilities for these words in theta sub d. So now all the parameters are known for this mixture model, and now let's considera word like a "text". So the question is, do youthink "text" is more likely having been generated from theta sub d or fromtheta sub of b? So in other words,we want to infer which distribution has beenused to generate this text. Now, this inference process is a typical Bayesian inferencesituation where we have some prior aboutthese two distributions. So can you see whatis our prior here? Well, the prior here is the probability ofeach distribution. So the prior is given bythese two probabilities. In this case, the prior is saying that each modelis equally likely, but we can imagine perhaps adifferent prior is possible. So this is called a priorbecause this is our guess of which distribution hasbeen used to generate a word before we evenoff reserve the word. So that's why wecall it the prior. So if we don't observe the word, we don't know what wordhas been observed. Our best guess is to saywell, they're equally likely. All right. So it'sjust flipping a coin. Now in Bayesian inference wetypically learn with update our belief after we haveobserved the evidence. So what is the evidence here? Well, the evidencehere is the word text. Now that we know we'reinterested in the word text. So text that can beregarded as evidence, and if we use Bayes rule to combine theprior and the data likelihood, what we will end upwith is to combine the prior with the likelihoodthat you see here, which is basicallythe probability of the word text fromeach distribution. We see that in both casesthe text is possible. Note that even in the backgroundit is still possible, it just has a verysmall probability. So intuitively what wouldbe your guess in this case. Now if you're like many others, you are guess textis probably from theta sub d. It's more likelyfrom theta sub d. Why? You will probably see thatit's because text that has a much higher probabilityhere by the theta sub d, then by the background model which has a verysmall probability. By this we're going to say, well, text is more likely from theta sub d. So you see our guess of whichdistribution has been used to generatethe text would depend on how high the probability of the text is ineach word distribution. We can do, tend to guess the distribution that gives us a word a higher probability, and this is likely tomaximize the likelihood. So we're going to choose a word that hasa higher likelihood. So in other words,we're going to compare these two probabilities of the word given byeach distributions. But our guess must alsobe affected by the prior. So we also need tocompare these two priors. Why? Because imagine if weadjust these probabilities, we're going to saythe probability of choosing a background model isalmost 100 percent. Now, if you have that kindof strong prior, then that wouldaffect your guess. You might think,well, wait a moment, maybe text could have beenfrom the background as well. Although the probabilityis very small here, the prior is very high. So in the end, we haveto combine the two, and the base formula provides us a solid and principled way of making this kind ofguess to quantify that. So more specifically, let's think aboutthe probability that this word has been generated in fact from from theta sub d. Well, in order for textsto be generated from theta sub d two thingsmust happen. First, the theta sub dmust have been selected, so we have the selectionprobability here. Secondly, we also have to actually have observed textfrom the distribution. So when we multiplythe two together, we get the probabilitythat text has in fact been generated fromtheta sub d. Similarly, for the background model, the probability of generating text is another productof a similar form. Now, we also introduced the latent variablez here to denote whether the word is fromthe background or the topic. When z is zero, it means it's from the topictheta sub d. When it's one, it means it's fromthe background theta sub b. So now we have the probability that textis generated from each. Then we can simply normalize them to have an estimateof the probability that the word text is from theta sub d orfrom theta sub b. Then equivalently, theprobability that z is equal to zero given thatthe observed evidence is text. So this is applicationof Bayes rule. But this step is verycrucial for understanding the EM algorithm becauseif we can do this, then we would be able to first initialize the parameter valuessomewhat randomly, and then we're going to takea guess of these z values. Which distributing has beenused to generate which word, and the initializedthe parameter values would allow us to have a complete specification ofthe mixture model which further allows us toapply Bayes rule to infer which distribution is morelikely to generate each word. This predictionessentially helped us to separate the words fromthe two distributions. Although we can'tseparate them for sure, but we can separate themprobabilistically as shown here.
[MUSIC] This lecture is aboutthe text retrieval problem. This picture shows our overall plan forlectures. In the last lecture, we talked aboutthe high level strategies for text access. We talked about push versus pull. Such engines are the main tools forsupporting the pull mode. Starting from this lecture, we're going to talk about the howsearch engines work in detail. So first it's aboutthe text retrieval problem. We're going to talk aboutthe three things in this lecture. First, we define Text Retrieval. Second we're going to make a comparisonbetween Text Retrieval and the related task Database Retrieval. Finally, we're going to talk aboutthe Document Selection versus Document Ranking as two strategies forresponding to a user's query. So what is Text Retrieval? It should be a task that's familiar for the most of us because we're usingweb search engines all the time. So text retrieval is basically a task where the system would respond toa user's query With relevant documents. Basically, it's for supporting a query as one way to implement the pollmode of information access. So the situation is the following. You have a collection oftext retrieval documents. These documents could be allthe webpages on the web, or all the literature articlesin the digital library. Or maybe all the textfiles in your computer. A user will typically give a query tothe system to express information need. And then, the system would returnrelevant documents to users. Relevant documents refer to thosedocuments that are useful to the user who typed in the query. All this task is a phone callthat information retrieval. But literally information retrieval wouldbroadly include the retrieval of other non-textual information as well,for example audio, video, etc. It's worth noting thatText Retrieval is at the core of information retrieval inthe sense that other medias such as video can be retrieved byexploiting the companion text data. So for example,current the image search engines actually match a user's query wasthe companion text data of the image. This problem is alsocalled search problem. And the technology is often calledthe search technology industry. If you ever take a course in databases it will be useful to pausethe lecture at this point and think about the differences betweentext retrieval and database retrieval. Now these two tasksare similar in many ways. But, there are some important differences. So, spend a moment to think aboutthe differences between the two. Think about the data, and the informationmanaged by a search engine versus those that are managedby a database system. Think about the different betweenthe queries that you typically specify for database system versus queries thatare typed in by users in a search engine. And then finally think about the answers. What's the difference between the two? Okay, so if we think about the informationor data managed by the two systems, we will see that in text retrieval. The data is unstructured, it's free text. But in databases, they are structured datawhere there is a clear defined schema to tell you this column is the namesof people and that column is ages, etc. The unstructured text is not obvious what are the names of peoplementioned in the text. Because of this difference, we also seethat text information tends to be more ambiguous and we talk about that in theprocessing chapter, whereas in databases. But they don't tend to havewhere to find the semantics. The results importantdifference in the queries, and this is partly due to the differencein the information or data. So test queries tend to be ambiguous. Whereas in their research,the queries are typically well-defined. Think about a SQL query that would clearlyspecify what records to be returned. So it has very well-defined semantics. Keyword queries or electronic queries tend to be incomplete,also in that it doesn't really specify what documentsshould be retrieved. Whereas complete specification forwhat should be returned. And because of these differences,the answers would be also different. Being the case of text retrieval, we'relooking for it rather than the documents. In the database search,we are retrieving records or match records with the sequelquery more precisely. Now in the case of text retrieval,what should be the right answers to the query is not very well specified,as we just discussed. So it's unclear what should bethe right answers to a query. And this has very important consequences,and that is, textual retrieval isan empirically defined problem. So this is a problem becauseif it's empirically defined, then we can not mathematically prove onemethod is better than another method. That also means we must relyon empirical evaluation involving users to knowwhich method works better. And that's why we have. You need more than one lecturesto cover the issue of evaluation. Because this is very important topic forSir Jennings. Without knowing how to evaluate heroismproperly, there's no way to tell whether we have got the better orwhether one system is better than another. So now let's look atthe problem in a formal way. So, this slide shows a formal formulationof the text retrieval problem. First, we have our vocabulary set, whichis just a set of words in a language. Now here,we are considering only one language, but in reality, on the web,there might be multiple natural languages. We have texts that are inall kinds of languages. But here for simplicity, we justassume that is one kind of language. As the techniques used for retrievingdata from multiple languages Are more or less similar to the techniques used forretrieving documents in one end, which although there is important difference,the principle methods are very similar. Next, we have the query,which is a sequence of words. And so here, you can see the query is defined asa sequence of words. Each q sub i is a word in the vocabulary. A document is defined in the same way,so it's also a sequence of words. And here,d sub ij is also a word in the vocabulary. Now typically, the documentsare much longer than queries. But there are also cases wherethe documents may be very short. So you can think about whatmight be a example of that case. I hope you can think of Twitter search. Tweets are very short. But in general,documents are longer than the queries. Now, then we havea collection of documents, and this collection can be very large. So think about the web. It could be very large. And then the goal of text retrievalis you'll find the set of relevant in the documents, which we denote by R'(q),because it depends on the query. And this in general, a subset of allthe documents in the collection. Unfortunately, this set of relevantdocuments is generally unknown, and user-dependent in the sense that,for the same query typed in by different users, they expectthe relevant documents may be different. The query given to us bythe user is only a hint on which document should be in this set. And indeed, the user is generallyunable to specify what exactly should be in this set, especially in the caseof web search, where the connection's so large, the user doesn't have completeknowledge about the whole production. So the best search systemcan do is to compute an approximation of thisrelevant document set. So we denote it by R'(q). So formerly,we can see the task is to compute this R'(q) approximation ofthe relevant documents. So how can we do that? Now imagine if you are now askedto write a program to do this. What would you do? Now think for a moment. Right, so these are your input. The query, the documents. And then you are to computethe answers to this query, which is a set of documents thatwould be useful to the user. So, how would you solve the problem? Now in general,there are two strategies that we can use. The first strategy is we do a documentselection, and that is, we're going to have a binary classificationfunction, or binary classifier. That's a function thatwould take a document and query as input, and then give a zero or one as output to indicate whether thisdocument is relevant to the query or not. So in this case, you can see the document. The relevant document is set,is defined as follows. It basically, all the documents thathave a value of 1 by this function. So in this case, you can see the system must have decideif the document is relevant or not. Basically, it has to saywhether it's one or zero. And this is called absolute relevance. Basically, it needs to knowexactly whether it's going to be useful to the user. Alternatively, there's anotherstrategy called document ranking. Now in this case, the system is not going to make a callwhether a document is random or not. But rather the system is going touse a real value function, f here. That would simply give us a value that would indicate whichdocument is more likely relevant. So it's not going to make a call whetherthis document is relevant or not. But rather it would say whichdocument is more likely relevant. So this function then can beused to random documents, and then we're going to letthe user decide where to stop, when the user looks at the document. So we have a threshold thetahere to determine what documents should be inthis approximation set. And we're going to assumethat all the documents that are ranked above the thresholdare in this set, because in effect, these are the documents thatwe deliver to the user. And theta is a cutoffdetermined by the user. So here we've got some collaborationfrom the user in some sense, because we don't really make a cutoff. And the user kind of helpedthe system make a cutoff. So in this case,the system only needs to decide if one document is morelikely relevant than another. And that is, it only needs todetermine relative relevance, as opposed to absolute relevance. Now you can probably already sense that relative relevance would be easier todetermine than absolute relevance. Because in the first case, we have to say exactly whethera document is relevant or not. And it turns out that ranking is indeedgenerally preferred to document selection. So let's look at these twostrategies in more detail. So this picture shows how it works. So on the left side,we see these documents, and we use the pluses to indicatethe relevant documents. So we can see the true relevantdocuments here consists this set of true relevant documents, consistsof these process, these documents. And with the document selection function, we're going to basicallyclassify them into two groups, relevant documents, and non-relevant ones. Of course, the classified will notbe perfect so it will make mistakes. So here we can see, in the approximationof the relevant documents, we have got some number in the documents. And similarly, there is a relevant document that'smisclassified as non-relevant. In the case of document ranking,we can see the system seems like, simply ranks all the documents inthe descending order of the scores. And then, we're going to let the userstop wherever the user wants to stop. If the user wants toexamine more documents, then the user will scroll down somemore and then stop [INAUDIBLE]. But if the user only wants toread a few random documents, the user might stop at the top position. So in this case, the user stops at d4. So in fact, we have deliveredthese four documents to our user. So as I said ranking is generallypreferred, and one of the reasons is because the classifier in the case ofdocument selection is unlikely accurate. Why? Because the only clueis usually the query. But the query may not be accurate in thesense that it could be overly constrained. For example, you might expect relevantdocuments to talk about all these topics by using specific vocabulary. And as a result,you might match no relevant documents. Because in the collection, no others have discussed the topicusing these vocabularies, right? So in this case,we'll see there is this problem of no relevant documents to return inthe case of over-constrained query. On the other hand,if the query is under-constrained, for example, if the query does not have sufficient descriptivewords to find the random documents. You may actually end up having ofover delivery, and this when you thought these words my be sufficientto help you find the right documents. But, it turns out theyare not sufficient and there are many distractions,documents using similar words. And so, this is a case of over delivery. Unfortunately, it's very hard to find theright position between these two extremes. Why? Because whether users looking forthe information in general the user does not have a good knowledge aboutthe information to be found. And in that case, the user does nothave a good knowledge about what vocabularies will be used inthose relevent documents. So it's very hard for a user to pre-specify the rightlevel of constraints. Even if the classifier is accurate,we also still want to rend these relevant documents, because theyare generally not equally relevant. Relevance is often a matter of degree. So we must prioritize these documents fora user to examine. And note that thisprioritization is very important because a user cannotdigest all the content the user generally would have tolook at each document sequentially. And therefore, it would make sense tousers with the most relevant documents. And that's what ranking is doing. So for these reasons,ranking is generally preferred. Now this preference also hasa theoretical justification and this is given by the probabilityranking principle. In the end of this lecture,there is reference for this. This principle says, returning a rankedlist of documents in descending order of probability that a documentis relevant to the query is the optimal strategy underthe following two assumptions. First, the utility ofa document (to a user) Is independent of the utilityof any other document. Second, a user would be assumed tobrowse the results sequentially. Now it's easy to understand why theseassumptions are needed in order to justify Site for the ranking strategy. Because if the documents are independent, then we can evaluate the utilityof each document that's separate. And this would allow the computerscore for each document independently. And then, we are going to rank thesedocuments based on the scrolls. The second assumption is to say that theuser would indeed follow the rank list. If the user is not going to followthe ranked list, is not going to examine the documents sequentially, then obviouslythe ordering would not be optimal. So under these two assumptions, we cantheoretically justify the ranking strategy is, in fact, the best that you could do. Now, I've put one question here. Do these two assumptions hold? I suggest you to pause the lecture,for a moment, to think about this. Now, can you think ofsome examples that would suggest these assumptionsaren't necessarily true. Now, if you think for a moment, you may realize none ofthe assumptions Is actually true. For example, in the case ofindependence assumption we might have documents that have similar orexactly the same content. If we look at each of them alone,each is relevant. But if the user has already seenone of them, we can assume it's generally not very useful for the user tosee another similar or duplicated one. So clearly the utilityon the document that is dependent on other documentsthat the user has seen. In some other cases you might seea scenario where one document that may not be useful to the user, but when threeparticular documents are put together. They provide answers tothe user's question. So this is a collective relevance andthat also suggests that the value of the document mightdepend on other documents. Sequential browsing generally would makesense if you have a ranked list there. But even if you have a rank list,there is evidence showing that users don't always just go strictlysequentially through the entire list. They sometimes will look at the bottom forexample, or skip some. And if you think about the morecomplicated interfaces that we could possibly use liketwo dimensional in the phase. Where you can put that additionalinformation on the screen then sequential browsing is a veryrestricted assumption. So the point here is that none of these assumptions isreally true but less than that. But probability ranking principleestablishes some solid foundation for ranking as a primary pattern forsearch engines. And this has actually been the basis for a lot of research work ininformation retrieval. And many hours have been designedbased on this assumption, despite that the assumptionsaren't necessarily true. And we can address this problemby doing post processing Of a ranked list, for example,to remove redundancy. So to summarize this lecture, the main points that you cantake away are the following. First, text retrieval isan empirically defined Problem. And that means which algorithm isbetter must be judged by the users. Second, document rankingis generally preferred. And this will help users prioritizeexamination of search results. And this is also to bypass the difficultyin determining absolute relevance Because we can get some help from usersin determining where to make the cut off, it's more flexible. So, this further suggests that the maintechnical challenge in designing a search engine is the designeffective ranking function. In other words, we need to definewhat is the value of this function F on the query and document pair. How we design such a function is the maintopic in the following lectures. There are two suggestedadditional readings. The first is the classical paper onthe probability ranking principle. The second one is a must-read for anyonedoing research on information retrieval. It's a classic IR book, which hasexcellent coverage of the main research and results in early days up tothe time when the book was written. Chapter six of this book hasan in-depth discussion of the Probability Ranking Principle andProbably for retrieval models in general. [MUSIC]
[SOUND] This lecture is about Document Length Normalizationin the Vector Space Model. In this lecture, we will continuethe discussion of the vector space model. In particular, we're going to discuss theissue of document length normalization. So far in the lectures about the vectorspace model, we have used the various signals from the document to assessthe matching of the document with a query. In particular,we have considered the tone frequency. The count of a tone in a document. We have also considered it'sglobal statistics such as, IDF, Inverse Document Frequency. But we have not considereddocument lengths. So here I show two example documents,d4 is much shorter with only 100 words. D6 on the other hand, has a 5000 words. If you look at the matchingof these query words, we see that in d6, there are morematchings of the query words. But one might reason that,d6 may have matched these query words in a scattered manner. So maybe the topic of d6, is notreally about the topic of the query. So, the discussion of the campaignat the beginning of the document, may have nothing to do with the managingof presidential at the end. In general,if you think about the long documents, they would have a higher chance formatching any query. In fact, if you generate a long documentrandomly by assembling words from a distribution of words, then eventuallyyou probably will match an inquiry. So in this sense, we should penalize ondocuments because they just naturally have better chance matching to any query, andthis is idea of document normalization. We also need to be careful in avoidingto over penalize long documents. On the one hand,we want to penalize the long document. But on the other hand,we also don't want to over-penalize them. Now, the reasoning is becausea document that may be long because of different reasons. In one case, the document may belong because it uses more words. So for example, think about the vortexarticle on the research paper. It would use more words thanthe corresponding abstract. So, this is a case where we probablyshould penalize the matching of long documents such as a full paper. When we compare the matchingof words in such a long document with matching ofthe words in the shop abstract. Then long papers in general, have a higher chance of matching clearerwords, therefore, we should penalize them. However, there is another casewhen the document is long, and that is when the documentsimply has more content. Now consider anothercase of long document, where we simply concatenate a lotof abstracts of different papers. In such a case, obviously, we don't wantto over-penalize such a long document. Indeed, we probably don't want to penalizesuch a document because it's long. So that's why, we need to be careful aboutusing the right degree of penalization. A method of that has been working well,based on recent results, is called a pivoted length normalization. And in this case, the idea is to use the average documentlength as a pivot, as a reference point. That means we'll assume that forthe average length documents, the score is about right sothe normalizer would be 1. But if the document is longerthan the average document length, then there will be some penalization. Whereas if it's a shorter,then there is even some reward. So this is illustrated atusing this slide, on the axis, x-axis you can see the length of document. On the y-axis, we show the normalizer. In this case, the Pivoted LengthNormalization formula for the normalizer, is seeing to be interpolation of 1 and the normalize the document in lengthcontrolled by a parameter B here. So you can see here,when we first divide the length of the document by the average documents,this not only gives us some sense about how this document iscompared with average documents, but also gives us a benefit of notworrying about the unit of length. We can measure the length by words orby characters. Anyway, this normalizerhas interesting property. First we see that, if we set the parameterb to 0 then the value would be 1. So, there's no lens normalization at all. So, b, in this sense,controls the lens normalization. Whereas, if we set b to a nonzero value,then the normalizer would look like this. All right, sothe value would be higher for documents that are longer thanthe average document lens. Whereas, the value ofthe normalizer would be shorter, would be smaller for shorter documents. So in this sense,we see there is a penalization for long documents, andthere's a reward for short documents. The degree of penalizationis controlled by b, because if we set b to a larger value,then the normalizer would look like this. There's even more penalization forlong documents and more reward for the short documents. By adjusting b, which varies from 0 to 1, we can control the degreeof length normalization. So, if we plug in this lengthnormalization fact that into the vector space model, ranking functionsis that we have already examined them. Then we will end up havingthe following formulas. And these are in fact the state ofthe vector space model formulas. Let's take a look at each of them. The first one is called a pivoted lengthnormalization vector space model, and a reference in [INAUDIBLE]duration of this model. And here we see that, it's basicallya TFI model that we have discussed, the idea of component shouldbe very familiar to you. There is also a query termfrequency component here. And then, in the middle, there isthe normalizer tf and in this case, we see we use the double logarithmas we discussed before and this is to achievea sublinear transformation. But we also put a documentthe length normalizer in the bottom. Right, so this would causepenalization for long document, because the larger the denominator is,then the smaller the is. And this is of course controlledby the parameter b here. And you can see again, if b is set to 0then there is no length normalization. Okay, so this is one of the two mosteffective at these base model formulas. The next one called a BM25 or Okapi, is also similar in that italso has a IDF component here, and query IDF component here. But in the middle,the normal issue's a little bit different. As we explained,there is our copy tf transformation here, and that does sublineartransformation with the upper bound. In this case we have put the lengthnormalization factor here. We're adjusting k butit achieves a similar factor, because we put a normalizerin the denominator. Therefore, again, if a document is longerthen the term weight will be smaller. So you can see after we have gone throughall the n answers that we talked about, and we have in the end reachedthe basically the state of god functions. So, So far, we have talked about mainly how to place the documentvector in the vector space. And, this has played an important rolein determining the effectiveness of the simple function. But there are also other dimensions,where we did not really examine details. For example, can we furtherimprove the instantiation of the dimension of the Vector Space Model? Now, we've just assumed that the bagof words representation should issue dimension as a word but obviously,we can see there are many other choices. For example, a stemmed word, thoseare the words that haven't transformed into the same root form, so that computation and computing were allbecome the same and they can be match. We get those stop word removal. This is to remove some very common wordsthat don't carry any content like the off. We get use of phrasesto define dimensions. We can even use later inthe semantical analysis, it will find some clusters of words that represent thea late in the concept as one by an engine. We can also use smaller unit,like a character end grams those are sequences of andthe characters for dimensions. However, in practice, people have foundthat the bag-of-words representation with phrases is still the most effectiveone and it's also efficient. So, this is still so far the mostpopular dimension instantiation method. And it's used in all major search engines. I should also mention, that sometimeswe need to do language specific and domain specific tokenization. And this is actually very important, as wemight have variations of terms that might prevent us from matching them with eachother, even when they mean the same thing. In some languages like Chinese,there is also the challenge in segmenting text to obtain word band rates becauseit's just a sequence of characters. A word might correspond to onecharacter or two characters or even three characters. So, it's easier in English when wehave a space to separate the words. In some other languages, we may needto do some Americanize processing to figure a way out of whatare the boundaries for words. There is also the possibility toimprove the similarity of the function. And sofar we have used as a top product, but one can imagine there are other measures. For example, we can measure the cosineof the angle between two vectors. Or we can use Euclidean distance measure. And these are all possible, but dot product seems still the best andone reason is because it's very general. In fact that it's sufficiently general, if you consider the possibilitiesof doing waiting in different ways. So, for example, cosine measure can be thought of as thethought product of two normalized factors. That means, we first normalize each factorand then we take the thought product. That would be criticalto the cosine measure. I just mentioned that the BM25, seems tobe one of the most effective formulas. But there has been also furtherdevelopments in improving BM25. Although, none of these words havechanged the BM25 fundamental. So in one line work,people have divide the BM25 F. Here, F stands for field, and this isuse BM25 for documents with structures. So for example, you might considera title field, the abstract, or body of the research article. Or even anchor text on the web page,those are the text fields that describe links to other pages andthese can all be combined with a proper way of different fields to helpimprove scoring for different documents. When we use BM25 for such a document and the obvious choice is to apply BM25 foreach field and then combine the scores. Basically, the idea of BM25F isto first combine the frequency counts of terms in all the fields,and then apply BM25. Now, this has advantage of avoiding overcounting the first occurrence of the term. Remember in the sublineartransformation of TF, the first occurrence is very important andit contributes a large weight. And if we do that for all the fields, then the same term might have gaineda lot of advantage in every field. But when we combine theseword frequencies together, we just do the transformation one time. At that time, then the extra occurrences will not becounted as fresh first recurrences. And this method has been working very wellfor scoring structure with documents. The other line of extensionis called a BM25+. In this line,risk is to have to address the problem of over penalization oflong documents by BM25. So to address this problem,the fix is actually quite simple. We can simply add a small constantto the TF normalization formula. But what's interesting is that,we can analytically prove that by doing such a small modification,we will fix the problem of over penalization oflaw documents by the original BM25. So the new formula called BM25+, is empirically andanalytically shown to be better than BM25. So to summarize all what we havesaid about vector space model, here are the major take away points. First, in such a model,we use the similarity of relevance. Assuming that relevance of a documentwith respect to a query, is basically proportional to the similaritybetween the query and the document. So naturally,that implies that the query and document must have beenrepresented in the same way. And in this case, we will present them asvectors in high-dimensional vector space. Where the dimensions are defined by words,or concepts, or terms, in general. And we generally, need to use a lot ofheuristics to design the ranking function. We use some examples, which showthe needs for several heuristics, including Tf weighting and transformation. And IDF weighting, anddocument length normalization. These major heuristics are the mostimportant of heuristics, to ensure such a general ranking functionto work well for all kinds of test. And finally, BM25 andpivoted normalization seem to be the most effective formulasout of the vector space model. Now I have to say that, I put BM25 inthe category of vector space model, but in fact, the BM25 has been derivedusing probabilistic model. So the reason why I've put it inthe vector space model is first, the ranking function actually has a niceinterpretation in the vector space model. We can easily see, it looks very much like a vector spacemodel, with a special waiting function. The second reason is because the originalBM25, has somewhat different form of IDF. And that form of IDF afterthe [INAUDIBLE] doesn't work so well as the standard IDFthat you have seen here. So as effective retrieval function, BM25 should probably use a heuristicmodification of the IDF. To make them even more looklike a vector space model There are some additional readings. The first is, a paper aboutthe pivoted length normalization. It's an excellent exampleof using empirical data analysis to suggest the need forlength normalization and then further derive the lengthnormalization formula. The second, is the original paperwhere the BM25 was proposed. The third paper,has a thorough discussion of BM25 and its extensions, particularly BM25 F. And finally, in the last paperhas a discussion of improving BM25 to correct the overpenalization of long documents. [MUSIC]
[MUSIC] This lecture is about,how we can evaluate a ranked list? In this lecture, we will continuethe discussion of evaluation. In particular, we are going to look at, how we canevaluate a ranked list of results. In the previous lecture,we talked about, precision-recall. These are the two basic measures for, quantitatively measuringthe performance of a search result. But, as we talked about, ranking, before, we framed that the text of retrievalproblem, as a ranking problem. So, we also need to evaluate the,the quality of a ranked list. How can we use precision-recallto evaluate, a ranked list? Well, naturally, we have to look after theprecision-recall at different, cut-offs. Because in the end, the approximationof relevant documents, set, given by a ranked list, is determinedby where the user stops browsing. Right?If we assume the user, securely browses, the list of results, the user would,stop at some point, and that point would determine the set. And then,that's the most important, cut-off, that we have to consider,when we compute the precision-recall. Without knowing whereexactly user would stop, then we have to consider, allthe positions where the user could stop. So, let's look at these positions. Look at this slide, andthen, let's look at the, what if the user stops at the,the first document? What's the precision-recall at this point? What do you think? Well, it's easy to see, that this documentis So, the precision is one out of one. We have, got one document,and that's relevent. What about the recall? Well, note that, we're assuming that,there are ten relevant documents, for this query in the collection,so, it's one out of ten. What if the user stopsat the second position? Top two. Well, the precision is the same,100%, two out of two. And, the record is two out of ten. What if the user stopsat the third position? Well, this is interesting,because in this case, we have not got any, additional relevant document,so, the record does not change. But the precision is lower,because we've got number [INAUDIBLE] so, what's exactly the precision? Well, it's two out of three, right? And, recall is the same, two out of ten. So, when would see another point,where the recall would be different? Now, if you look down the list,well, it won't happen until, we have, seeing another relevant document. In this case D5, at that point, the,the recall is increased through three out of ten, and,the precision is three out of five. So, you can see, if we keep doing this,we can also get to D8. And then, we will havea precision of four out of eight, because there are eight documents,and four of them are relevant. And, the recall is a four out of ten. Now, when can we get,a recall of five out of ten? Well, in this list, we don't have it,so, we have to go down on the list. We don't know, where it is? But, as convenience, we often assume that,the precision is zero, at all the, the othe,the precision are zero at all the other levels of recall,that are beyond the search results. So, of course,this is a pessimistic assumption, the actual position would be higher,but we make, make this assumption, in order to, have an easy way to, compute another measure called AveragePrecision, that we will discuss later. Now, I should also say, now, here you see, we make these assumptions thatare clearly not, accurate. But, this is okay, forthe purpose of comparing to, text methods. And, this is for the relative comparison,so, it's okay, if the actual measure, or actual, actual number deviatesa little bit, from the true number. As long as the deviation, is not biased toward any particularretrieval method, we are okay. We can still,accurately tell which method works better. And, this is important point,to keep in mind. When you compare different algorithms, the key's to avoid anybias toward each method. And, as long as, you can avoid that. It's okay, for you to do transformationof these measures anyway, so, you can preserve the order. Okay, so, we'll just talk about, we can get a lot of precision-recallnumbers at different positions. So, now, you can imagine,we can plot a curve. And, this just shows on the,x-axis, we show the recalls. And, on the y-axis, we show the precision. So, the precision line was marked as .1,.2, .3, and, 1.0. Right?So, this is, the different, levels of recall. And,, the y-axis also has,different amounts, that's for precision. So, we plot the, these, precision-recallnumbers, that we have got, as points on this picture. Now, we can further, andlink these points to form a curve. As you'll see, we assumed all the other, precisionas the high-level recalls, be zero. And, that's why, they are down here,so, they are all zero. And this, the actual curve probably willbe something like this, but, as we just discussed, it, it doesn't matter thatmuch, for comparing two methods. because this would be,underestimated, for all the method. Okay, so, now that we,have this precision-recall curve, how can we compare ranked to back list? All right, so, that means,we have to compare two PR curves. And here, we show, two cases. Where system A is showing red,system B is showing blue, there's crosses. All right, so, which one is better? I hope you can see,where system A is clearly better. Why?Because, for the same level of recall, see same level of recall here,and you can see, the precision point by system A is better,system B. So, there's no question. In here, you can imagine, what does thecode look like, for ideal search system? Well, it has to have perfect,precision at all the recall points, so, it has to be this line. That would be the ideal system. In general, the higher the curve is,the better, right? The problem is that,we might see a case like this. This actually happens often. Like, the two curves cross each other. Now, in this case, which one is better? What do you think? Now, this is a real problem,that you actually, might have face. Suppose, you build a search engine,and you have a old algorithm, that's shown here in blue, or system B. And, you have come up with a new idea. And, you test it. And, the results are shown in red,curve A. Now, your question is, is your newmethod better than the old method? Or more, practically,do you have to replace the algorithm that you're already using, your, in your searchengine, with another, new algorithm? So, should we use system,method A, to replace method B? This is going to be a real decision,that you to have to make. If you make the replacement, the searchengine would behave like system A here, whereas, if you don't do that,it will be like a system B. So, what do you do? Now, if you want to spend more timeto think about this, pause the video. And, it's actually veryuseful to think about that. As I said, it's a real decision that youhave to make, if you are building your own search engine, or if you're working, fora company that, cares about the search. Now, if you have thought about this for a moment, you might realize that,well, in this case, it's hard to say. Now, some users might like a system A,some users might like, like system B. So, what's the difference here? Well, the difference is just that,you know, in the, low level of recall,in this region, system B is better. There's a higher precision. But in high recall region,system A is better. Now, so, that also means,it depends on whether the user cares about the high recall, orlow recall, but high precision. You can imagine, if someone is just goingto check out, what's happening today, and want to find out somethingrelevant in the news. Well, which one is better? What do you think? In this case, clearly, system B is better, because the user is unlikelyexamining a lot of results. The user doesn't care about high recall. On the other hand,if you think about a case, where a user is doing you are,starting a problem. You want to find, whether your idea ha,has been started before. In that case, you emphasize high recall. So, you want to see,as many relevant documents as possible. Therefore, you might, favor, system A. So, that means, which one is better? That actually depends on users,and more precisely, users task. So, this means, you may not necessarilybe able to come up with one number, that would accuratelydepict the performance. You have to look at the overall picture. Yet, as I said, when you havea practical decision to make, whether you replace ours with another, then you may have to actually come up witha single number, to quantify each, method. Or, when we compare many differentmethods in research, ideally, we have one number to compare, them with, so, thatwe can easily make a lot of comparisons. So, for all these reasons, it is desirableto have one, single number to match it up. So, how do we do that? And, that,needs a number to summarize the range. So, here again it'sthe precision-recall curve, right? And, one way to summarizethis whole ranked, list, for this whole curve,is look at the area underneath the curve. Right?So, this is one way to measure that. There are other ways to measure that,but, it just turns out that,, this particular way of matchingit has been very, popular, and has been used, since a long time ago fortext And, this is, basically, in this way, andit's called the average precision. Basically, we're going to take a, a lookat the, every different, recall point. And then, look out for the precision. So, we know, you know,this is one precision. And, this is another,with, different recall. Now, this, we don't count to this one, because the recall level is the same,and we're going to, look at the, this number, and that's precision ata different recall level et cetera. So, we have all these, you know, added up. These are the precisionsat the different points, corresponding to retrieving the firstrelevant document, the second, and then, the third, that follows, et cetera. Now, we missed the many relevantdocuments, so, in all of those cases, we just, assume,that they have zero precisions. And then, finally, we take the average. So, we divide it by ten, and which is the total number of relevantdocuments in the collection. Note that here,we're not dividing this sum by four. Which is a number retrievedrelevant documents. Now, imagine, if I divide by four,what would happen? Now, think about this, for a moment. It's a common mistake that people,sometimes, overlook. Right, so, if we, we divide this by four,it's actually not very good. In fact, that you are favoring a system,that would retrieve very few random documents, as in that case,the denominator would be very small. So, this would be, not a good matching. So, note that this denomina,denominator is ten, the total number of relevant documents. And, this will basically ,computethe area, and the needs occur. And, this is the standard method,used for evaluating a ranked list. Note that, it actually combinesrecall and, precision. But first, you know, we haveprecision numbers here, but secondly, we also consider recall, because if missedmany, there would be many zeros here. All right, so,it combines precision and recall. And furthermore, you can see thismeasure is sensitive to a small change of a position of a relevant document. Let's say, if I move this relevantdocument up a little bit, now, it would increase this means,this average precision. Whereas, if I move any relevant document,down, let's say, I move this relevant document down, then it would decrease,uh,the average precision. So, this is a very good, because it's a very sensitive tothe ranking of every relevant document. It can tell, small differencesbetween two ranked lists. And, that is what we want, sometimes one algorithm only worksslightly better than another. And, we want to see this difference. In contrast, if we look atthe precision at the ten documents. If we look at this, this whole set, well, what, what's the precision,what do you think? Well, it's easy to see,that's a four out of ten, right? So, that precision is very meaningful,because it tells us, what user would see? So, that's pretty useful, right? So, it's a meaningful measure,from a users perspective. But, if we use this measure tocompare systems, it wouldn't be good, because it wouldn't be sensitive to wherethese four relevant documents are ranked. If I move them around the precisionat ten, still, the same. Right.So, this is not a good measure forcomparing different algorithms. In contrast, the average precisionis a much better measure. It can tell the difference of, different, a difference in ranked list in,subtle ways. [MUSIC]
[SOUND] This lecture is about query likelihood, probabilistic retrieval model. In this lecture, we continue the discussion ofprobabilistic retrieval model. In particular, we're going to talk aboutthe query light holder retrieval function. In the query light holder retrieval model,our idea is model. How like their user who likes a documentwith pose a particular query? So in this case,you can imagine if a user likes this particular document abouta presidential campaign news. Now we assume,the user would use this a document as a basis to impose a query to try andretrieve this document. So again, imagine use a processthat works as follows. Where we assume thatthe query is generated by assembling words from the document. So for example, a user mightpick a word like presidential, from this document andthen use this as a query word. And then the user would pickanother word like campaign, and that would be the second query word. Now this of course is an assumptionthat we have made about how a user would pose a query. Whether a user actually followed thisprocess may be a different question, but this assumption has allowed us to formerlycharacterize this conditional probability. And this allows us to also not rely onthe big table that I showed you earlier to use empirical data toestimate this probability. And this is why we can use thisidea then to further derive retrieval function that we canimplement with the program language. So as you see the assumptionthat we made here is each query word is independent of the sample. And also each word is basicallyobtained from the document. So now let's see how this works exactly. Well, since we are completinga query likelihood then the probability here is justthe probability of this particular query, which is a sequence of words. And we make the assumption that eachword is generated independently. So as a result, the probabilityof the query is just a product of the probability of each query word. Now how do we computethe probability of each query word? Well, based on the assumption that a word is picked from the documentthat the user has in mind. Now we know the probability of each wordis just the relative frequency of each word in the document. So for example, the probability ofpresidential given the document. Would be just the countof presidential document divided by the total number of wordsin the document or document s. So with these assumptions we now haveactually a simple formula for retrieval. We can use this to rank our documents. So does this model work? Let's take a look. Here are some example documentsthat you have seen before. Suppose now the query ispresidential campaign and we see the formula here on the top. So how do we score this document? Well, it's very simple. We just count how many times dowe have seen presidential or how many times do we have seen campaigns,etc. And we see here 44, andwe've seen presidential twice. So that's 2 over the length ofdocument 4 multiplied by 1 over the length of document 4 forthe probability of campaign. And similarly, we can get probabilitiesfor the other two documents. Now if you look at these numbers orthese formulas for scoring all these documents,it seems to make sense. Because if we assume d3 andd4 have about the same length, then looks like a nominal rank d4above d3 and which is above d2. And as we would expect,looks like it did captures a TF query state, and sothis seems to work well. However, if we try a differentquery like this one, presidential campaign updatethen we might see a problem. Well what problem? Well think about the update. Now none of these documentshas mentioned update. So according to our assumption that a userwould pick a word from a document to generate a query, then the probability ofobtaining the word update would be what? Would be 0. So that causes a problem,because it would cause all these documents to have zero probabilityof generating this query. Now why it's fine to have zero probabilityfor d2, which is non-relevant? It's not okay to have 0 for d3 and d4 because now we no longercan distinguish them. What's worse? We can't even distinguish them from d2. So that's obviously not desirable. Now when a [INAUDIBLE] has such result, we should think about whathas caused this problem? So we have to examine whatassumptions have been made, as we derive this ranking function. Now is you examine those assumptionscarefully you will realize, what has caused this problem? So take a moment to think about it. What do you think is the reason why updatehas zero probability and how do we fix it? So if you think about this from the momentyou realize that that's because we have made an assumptionthat every query word must be drawn from the documentin the user's mind. So in order to fix this, we have toassume that the user could have drawn a word not necessarily from the document. So that's the improved model. An improvement here is to say that, well instead of drawinga word from the document, let's imagine that the user would actuallydraw a word from a document model. And so I show a model here. And we assume that this document isgenerated using this unigram language model. Now, this model doesn't necessarily assignzero probability for update in fact, we can assume this model does notassign zero probability for any word. Now if we're thinking this way thenthe generation process is a little bit different. Now the user has this model in mindinstead of this particular document. Although the model has to beestimated based on the document. So the user can again generatethe query using a singular process. Namely, pick a word for example,presidential and another word campaign. Now the difference is that this timewe can also pick a word like update, even though update doesn'toccur in the document to potentially generatea query word like update. So that a query was updated1 times 0 probabilities. So this would fix our problem. And it's also reasonable because when ourthinking of what the user is looking for in a more general way, that is uniquelanguage model instead of fixed document. So how do we computethis query likelihood? If we make this sum wideinvolved two steps. The first one is compute this model, andwe call it document language model here. For example, I've shown two pulse modelshere, it's major based on two documents. And then given a query like a data miningalgorithms the thinking is that we'll just compute the likelihood of this query. And by making independenceassumptions we could then have this probability as a product ofthe probability of each query word. We do this for both documents, andthen we can score these two documents and then rank them. So that's the basic idea of thisquery likelihood retrieval function. So more generally this ranking functionwould look like in the following. Here we assume that the query has n words, w1 through wn, andthen the scoring function. The ranking function is the probabilitythat we observe this query, given that the user isthinking of this document. And this is assume it will be product ofprobabilities of all individual words. This is based on independent assumption. Now we actually often scorethe document before this query by using log of the query likelihoodas shown on the second line. Now we do this to avoid having a lot of small probabilities,mean multiply together. And this could cause under flow and wemight loose the precision by transforming the value in our algorithm function. We maintain the order of these documentsyet we can avoid the under flow problem. And so if we take longer thantransformation of course, the product would become a sumas you on the second line here. So the sum of all the querywords inside of the sum that is one of the probability ofthis word given by the document. And then we can further rewritethe sum to a different form. So in the first sum here, in this sum, we have it over all the query words andquery word. And in this sum we have a sumof all the possible words. But we put a counter hereof each word in the query. Essentially we are only consideringthe words in the query, because if a word is not in the query,the count will be 0. So we're still consideringonly these n words. But we're using a different form asif we were going to take a sample of all the words in the vocabulary. And of course, a word might occurmultiple times in the query. That's why we have a count here. And then this part is log ofthe probability of the word, given by the document language model. So you can see in this retrieval function, we actually know the countof the word in the query. So the only thing that we don't knowis this document language model. Therefore, we have convertedthe retrieval problem include the problem of estimatingthis document language model. So that we can compute the probability ofeach query word given by this document. And different estimation methods wouldlead to different ranking functions. This is just like a different way toplace document in the vector space which leads to a different rankingfunction in the vector space model. Here different ways toestimate will lead to a different ranking function forquery likelihood. [MUSIC]
[SOUND]This lecture is about the feedback inthe language modeling approach. In this lecture, we will continue thediscussion of feedback in text retrieval. In particular, we're going to talk about the feedbackin language modeling approaches. So we derive the query likelihood rankingfunction by making various assumptions. As a basic retrieval function,all those formulas worked well. But if we think about the feedbackinformation, it's a little bit awkward to use query likelihood to perform feedback,because a lot of times the feedback information isadditional information about the query. But we assume the query hasgenerated it by assembling words from a language model inthe query likelihood method. It's kind of unnatural to samplewords that form feedback documents. As a result, researchers proposed a wayto generalize query likelihood function, and it's called Kullback-Leiblerdivergence retrieval model. And this model is actually goingto make the query likelihood retrieval function muchcloser to vector space model. Yet this form of the language modelcan be regarded as a generalization of query likelihood, in the sense that it cancover query likelihood as a special case. And in this case, then feedback can be achieved throughsimply query model estimation or updating. This is very similar to Rocchio,which updates the query vector. So let's see what is thisKL-divergence retrieval model. So on the top, what you see is a querylikelihood retrieval function, this one. And then KL-divergence, oralso called cross entropy, retrieval model is basically to generalize the frequency part hereinto a language model. So basically it's the difference given by the probabilistic model here tocharacterize what the user is looking for, versus the count of query words there. And this difference allows us to plug invarious different ways to estimate this. So this can be estimatedin many different ways, including using feedback information. But this is called a KL-divergence,because this can be interpreted as matchingthe KL-divergence of two distributions. One is the query model,denoted by this distribution. One is the documentlanguage model here and smooth them with a collectionlanguage model, of course. And we are not going to talkabout the detail of that, and you'll find it in some references. It's also called cross entropy because,in fact, we ignore some terms inthe KL-divergence function and we will end up havingactually cross entropy. And both are terms of information theory. But anyway, for our purposes here, you can just see the twoformulas look almost identical, except that here we have a probability ofa word given by a query language model. And here the sum is over all the wordsthat are in the document and also with the nonzero probability forthe query model. So it's kind of, again, a generalizationof sum over all the matching query words. Now you can also easily see we can recoverthe query likelihood retrieval function by simply setting this query model to therelative frequency of a word in the query. This is very easy tosee once you plug this into here you can eliminate thisquery length as a constant. And then you will get exactly like that. So you can see the equivalence. And that's also why this KL-divergencemodel can be regarded as a generalization of query likelihood, because we can coverquery likelihood as a special case. But it would also allow usto do much more than that. So this is how we can use theKL-divergence model to then do feedback. The picture shows that we firstestimate a document language model, then we estimate a query language model,and we compute the KL-divergence. This is often denoted by a D here. But this basically means this isexactly like the vector space model, because we compute a vector for thedocument, then compute another vector for the query, andthen we compute the distance. Only that these vectors are of specialforms, they are probability distributions. And then we get the results andwe can find some feedback documents. Let's assume they are mostlypositive documents, although we could also considerboth kinds of documents. So what we could do is, like in Rocchio,we're going to compute another language model called the feedbacklanguage model here. Again, this is going to be another vectorjust like the computing centroid of vector in Rocchio. And then this model can be combinedwith the original query model using a linear interpolation, andthis would then give us an update model, just like, again, in Rocchio. So here we can see the parameter alphacan control the amount of feedback. If it's set to zero,then essentially there is no feedback. If it's set to one, we get full feedbackand we ignore the original query. And this is generally not desirable,right? So unless you are absolutely sure youhave seen a lot of relevant documents, then the query terms are not important. So of course, the main question here is,how do you compute this theta F? This is the big question here, andonce you can do that, the rest is easy. So here we will talk aboutone of the approaches, and there are many approaches, of course. This approach is basedon generative model, and I'm going to show you how it works. This will use a generative mixture model. So this picture shows thatwe have this model here, the feedback model thatwe want to estimate. And the basis is the feedback documents. Let's say we are observingthe positive documents. These are the clicked documents by usersor random documents judged by users, or are simply top ranked documentsthat we assume to be relevant. Now imagine how we cancompute a centroid for these documents by using language model. One approach is simply to assume these documents are generatedfrom this language model. As we did before, what we could dois just normalize the word frequency here to here andthen we will get this word distribution. Now the question is whether thisdistribution is good for feedback. Well, you can imagine the topranked word would be what? What do you think? Well, those words would be common words. As we always see in a language model, the top ranked words are actuallycommon words like the, a, etc. So it's not very good for feedback,because we would be adding a lot of such words to our query when we interpolatethis with the original query model. So this was not good, sowe need to do something. In particular, we are trying toget rid of those common words. And we have seen actually one wayto do that by using background language model in the case oflearning the associations of words, the words that are relatedto the word computer. We could do that and that would beanother way to do this, but here we are going to talk about another approachwhich is a more principled approach. In this case, we're going to say well,you said that there are common words here in these documents that should notbelong to this topic model, right? So now what we can do is to assume that,well, those words are generated frombackground language model, so they will generate those words like the,for example. And if we use maximum likelihood estimate, note that if all the words heremust be generated from this model, then this model is forced to assignhigh probabilities to a word like the, because it occurs so frequently here. Note that in order to reduce itsprobability in this model, we have to have another model, which is this one,to help explain the word the here. And in this case, it's not appropriate to use the backgroundlanguage model to achieve this goal because this model would assign highprobabilities to these common words. So in this approach, then, we assume this machine that was generatingthese words would work as follows. We have a source control up here. Imagine we flip a coin here todecide what distribution to use. With probability of lambda,the coin shows up as head and we're going to usethe background language model. And we're going to do that insample word from that model. With probability of 1 minus lambda,we're going to decide to use a known topic model, here,that we would like to estimate. And we're going to thengenerate a word here. If we make this assumption and this wholething will be just one model, and we call this a mixture model because there are twodistributions that are mixed together. And we actually don't know wheneach distribution is used. So again,think of this whole thing as one model, and we can still ask for words and it willstill give us a word in a random manner. And of course, which word will show upwill depend on both this distribution and that distribution. In addition,it would also depend on this lambda, because if you say lambda is very high andit's going to always use the background distribution,you will get different words. Then if you say, well, lambda isvery small, we're going to use this. So all of theseare parameters in this model. And then if you're thinking this way, basically we can do exactlythe same as what we did before. We're going to use maximum likelihoodestimator to adjust this model, to estimate the parameters. Basically we're going toadjust this parameter so that we can best explain all the data. The difference now is that we are notasking this model a known to explain this. But rather we are going to ask this wholemodel, mixture model, to explain the data. Because it has got some helpfrom the background model, it doesn't have to assign highprobabilities to words like the. As a result, it will then assign higherprobabilities to other words that are common here butnot having high probability here. So those would be common here. And if they're common, they wouldhave to have high probabilities, according to a maximumlikelihood estimate method. And if they are rare here,then you don't get much help from this background model. As a result, this topic modelmust assign high probabilities. So the high probability words,according to the topic model, would be those that are common here butrare in the background. So this is basically a little bitlike an idea of weighting here. But this would allow us to achieve theeffect of removing these topic words that are meaningless in the feedback. So mathematically, what we have isto compute the likelihood, again, local likelihood,of the feedback documents. And note that we also have anotherparameter, lambda here, but we assume that the lambda denotesthe noise in the feedback document. So we are going to,let's say set this to a parameter. Let's say 50% of the words are noise or90% are noise. And this can then beassumed it will be fixed. If we assume this is fixed, then we onlyhave these probabilities as parameters, just like in the simpleunigram language model. We have n parameters,n is the number of words. And then the likelihoodfunction would look like this. It's very similar to the globallikelihood function we see before, except that inside the logarithmthere's a sum here. And this sum is because weconsider two distributions. And which one is used would depend onlambda, and that's why we have this form. But mathematically, this is the functionwith theta as unknown variables. So this is just a function. All the other values are known except forthis guy. So we can then choose thisprobability distribution to maximize this log likelihood, the same idea as the maximum likelihoodestimate as a mathematical problem. We just have to solve thisoptimization problem. We essentially would try allthe theta values until we find one that gives this whole thingthe maximum probability. So it's a well-defined math problem. Once we have done that, we obtain thistheta F that can then be interpolated with original query model to the feedback. So here are some examples ofthe feedback model learned from a web document collection. And we do pseudo-feedback we justuse the top ten documents and we use this mixture model. So the query is airport security. What we do is we first retrieve tendocuments from the web database and this is of course pseudo-feedback. And then we're going to feed thatmixture model to this ten document set. And these are the wordslearned using this approach. This is the probability of a word givenby the feedback model in both cases. So in both cases you can see the highest probability words include the veryrelevant words to the query. So airport security, for example, these query words still show up as highprobabilities in each case naturally, because they occur frequentlyin the top ranked documents. But we also see beverage,alcohol, bomb, terrorist, etc. So these are relevant to this topic,and they, if combined with original query, can helpus much more accurately on documents. And also they can help us bring updocuments that only mention some of these other words, maybe, for example,just airport and then bomb, for example. So this is how pseudo-feedback works. It shows that this model really works andpicks up some related words to the query. What's also interesting is that ifyou look at the two tables here and you compare them,then you'll see, in this case, when lambda is set to a small value,then we'll see some common words here. And that means, well,we don't use the background model often. Remember, lambda confuses the probabilityof using background model to generate the text. If we don't rely much on background model, we still have to use this topic modelto account for the common words. Whereas if we set lambdato a very high value, we will use the background modelvery often to explain these words. Then there's no burden onexpanding those common words in the feedback documentsby the topic model. So as a result, the topic modelhere is very discriminative. It contains all the relevantwords without common words. So this can be added to the originalquery to achieve feedback. So to summarize, in this lecture we have talked aboutthe feedback in language model approach. In general,feedback is to learn from examples. These examples can be assumed examples,can be pseudo-examples, like assume the top ten documentsthat are assumed to be relevant. They could be based on user interactions, like feedback based on clickthroughs orimplicit feedback. We talked about the three majorfeedback scenarios, relevance feedback, pseudo feedback, and implicit feedback. We talked about how to use Rocchio todo feedback in vector space model and how to use query model estimation forfeedback in language model. And we briefly talked aboutthe mixture model and the basic idea. There are many other methods. For example, the relevance model is a very effectivemodel for estimating query model. So you can read more about thesemethods in the references that are listed at the end of this lecture. So there are two additional readings here. The first one is a book thathas a systematic review and discussion of language models forinformation retrieval. And the second one is a important research paper that's about relevancebased language models, and it's a very effective wayof computing query model. [MUSIC]
[SOUND]There are many more of the Munster learning algorithmsthan the regression based approaches and they generally attempt to directthe optimizer retrieval method. Like a MAP or nDCG. Note that the optimization object orfunction that we have seen on the previous slide is not directlyrelated to the retrieval measure. By maximizing the prediction of one or zero, we don't necessarily optimizethe ranking of those documents. One can imagine that ourprediction may not be too bad. And let's say both are around 0.5. So it's kind of in the middle of zero andone for the two documents. But the ranking can be wrong, so we mighthave a larger value for E2 and then E1. So that won't be good fromretrieval perspective, even though function, it's not bad. In contrast, we might have anothercase where we predicted the values, or around the 0.9, it said. And by the objective function,the error would be larger. But if we didn't get the orderof the two documents correct, that's actually a better result. So these new, more advanced approacheswill try to correct that problem. Of course, then the challenge isthat the optimization problem will be harder to solve. And then, researchers have posedmany solutions to the problem, and you can read more of the references atthe end, know more about these approaches. Now, these learning rankedapproaches after the general. So there accounts would be be appliedwith many other ranking problems, not just the retrieval problem. So some people will gowith recommender systems, computational advertising,or summarization and there are many others that you canprobably encounter in your applications.. To summarize this lecture wehave talked about using machine learning to combine much morefeatures including ranking results. Actually the use of machine learning in information retrieval hasstarted since many decades ago. So for example, the Rocchio feedbackapproach that we talked about earlier was a machine learning approachprior to relevance feedback. But the most recent use of machinelearning has been driven by some changes in the environment ofapplications of retrieval systems. First, it's mostly freedom ofavailability of a lot of training data in the form of critical, such asthey are more available than before. So the data can provide a lot ofuseful knowledge about relevance and machine learning methods can beapplied into a leverage list. Secondly, it's also freedom bythe need for combining many features, and this is not only justbecause there are more features available on the web that canbe naturally used for improved scoring. It's also because by combining them,we can improve the robustness of ranking, so this is desired forcombating spams. Modern search engines all use somekind of machine learning techniques to combine many featuresto optimize ranking and this is a major feature of thesecommercial engines such a Google or Bing. The topic of learning to rank is stillactive research topic in the community, and so we can expect to see new resultsin development in the next few years, perhaps. Here are some additional readingsthat can give you more information about how learning to rank at works andalso some advanced methods. [MUSIC]
﻿  Skip to content  [University of Illinois Urbana-Champaign](https://illinois.edu)  [ ](http://illinois.edu)  [Student Code](/)  ## Main Menu    * [Student Code Home](/ "Student Code home")   * [   Information ](/information/ "Information on previous codes, CCG, and the change process.")      * [Conference on Conduct Governance](/information/ccg/)     * [Procedure for Amending the Student Code](/information/procedure-for-amending/)     * [Changes from Previous Version (pdf)](/docs/2021-Proposed-Code-Changes.pdf)     * [Archives](/information/archives/)   * [Article 1 - Student Rights    and Responsibilities](/article1/ "Article 1 \(Student Rights and Responsibilities\)")      * [Part 1 - Student Rights](/article1/part1/1-101/)     * [Part 2 - General Responsibilities of Students](/article1/part2/1-201/)     * [Part 3 - Student Discipline](/article1/part3/1-301/)     * [Part 4 - Academic Integrity Policy and Procedure](/article1/part4/1-401/)     * [Part 5 - Class Attendance](/article1/part5/1-501/)     * [Part 6 - Educational Technologies](/article1/part6/1-601/)   * [Article 2 - General Policies    and Regulations](/article2/ "Article 2 \(General Policies and Regulations\)")      * [Part 1 - Medical Policies](/article2/part1/2-101/)     * [Part 2 - Housing Policies](/article2/part2/2-201/)     * [Part 3 - Registered Organizations and Organization Fund](/article2/part3/2-301/)     * [Part 4 - University Property and Facilities - In General](/article2/part4/2-401/)     * [Part 5 - Use of University Premises and Facilities](/article2/part5/2-501/)     * [Part 6 - Motor Vehicles and Bicycles](/article2/part6/2-601/)     * [Part 7 - Chancellor's Emergency Powers](/article2/part7/2-701/)     * [Part 8 - Missing Student Notification Policy](/article2/part8/2-801/)     * [Part 9 - Involuntary Withdrawal](/article2/part9/2-901/)   * [Article 3 - Academic Policies    and Regulations](/article3/ "Article 3 \(Academic Policies and Regulations\)")      * [Part 1 - Grades and Grading System](/article3/part1/3-101/)     * [Part 2 - Examinations](/article3/part2/intro/)     * [Part 3 - Registration, Course Changes, and Withdrawal](/article3/part3/3-301/)     * [Part 4 - Undergraduate Academic Recognition](/article3/part4/3-401/)     * [Part 5 - Registration Charges](/article3/part5/3-501/)     * [Part 6 - Student Records - Guidelines and Regulations Governing Access and Release](/article3/part6/3-601/)     * [Part 7 - Transcripts](/article3/part7/3-701/)     * [Part 8 - Graduation](/article3/part8/3-801/)     * [Part 9 - Residency Status Regulations](/article3/part9/)   *   # Student Code at Illinois  ## Main Content  ## Student Code 2021-2022  The regulations appearing here are those in effect at the most recent publication and are subject to change without notice. Unless otherwise noted, the rules stated in this Student Code apply to all undergraduate, graduate, and professional students enrolled at the University of Illinois Urbana- Champaign.  ### Preface  The Student Code is a collection of rules, regulations, policies, and procedures that apply to, or otherwise directly impact, students at the University of Illinois Urbana-Champaign. Although it is not an exhaustive list of such policies, it is the most expansive list available in a single document. It is divided into three articles:    * [Article 1 - Student Rights and Responsibilities](article1/)   * [Article 2 - General Policies and Regulations](article2/)   * [Article 3 - Academic Policies and Regulations](article3/)  Unless otherwise noted, the rules stated in this Student Code apply to all undergraduate, graduate, and professional students enrolled at the university. All students are expected to review this document, especially Article 1, prior to attending classes so that they may begin their work at Illinois with knowledge both of their rights as students and of their responsibilities as members of the academic community. A printed booklet containing only Article 1 is also available in the Office of the Dean of Students.  The specific changes that were made from the 2020-2021 publication to this year’s issue can be found on this website in the information section.  ### Print Versions  The Student Code is also available in two print versions: one containing all three articles and the other containing only Article 1 - Student Rights and Responsibilities. Both may be obtained at the [Office of the Dean of Students](https://odos.illinois.edu).  ### Digital Versions  The digital version of the code may be viewed using [Adobe Acrobat Reader](https://acrobat.adobe.com/us/en/acrobat/pdf-reader.html).    * [Full Student Code(pdf)](docs/2021-Student-Code.pdf)   * [Article 1 - Student Rights and Responsibilities(pdf)](docs/2021-Pocket-Code.pdf)  Top  ## Footer, Content Info  [ ](http://illinois.edu)  ### [Student Code](/)    * [University Administration](http://www.uillinois.edu/)   * [Urbana Campus](http://www.illinois.edu)   * [Chicago Campus](http://www.uic.edu)   * [Springfield Campus](http://www.uis.edu)  #### Student Code Links    * [Information](/information/)   * [Article 1 - Student Rights and Responsibilities](/article1/)   * [Article 2 - General Policies and Regulations](/article2/)   * [Article 3 - Academic Policies and Regulations](/article3/)  #### Quick Links    * [Procedure for Amending the Student Code](/information/procedure-for-amending/)   * [Translate](/translate/)  © 2021 University of Illinois Board of Trustees  [Web Privacy Notice](https://www.vpaa.uillinois.edu/resources/web_privacy)  About Cookies  
#  CS 410: Text Information Systems  ##  Course Description  The growth of “big data” created unprecedented opportunities to leverage computational and statistical approaches, which turn raw data into actionable knowledge that can support various application tasks. This is especially true for the optimization of decision making in virtually all application domains, such as health and medicine, security and safety, learning and education, scientific discovery, and business intelligence. This course covers general computational techniques for building intelligent text information systems to help users manage and make use of large amounts of text data in all kinds of applications.  Text data include all data in the form of natural language text (e.g., English text or Chinese text), including all web pages, social media data such as tweets, news, scientific literature, emails, government documents, and many other kinds of enterprise data. Text data play an essential role in our lives. Since we communicate using natural languages, we produce and consume a large amount of text data every day covering all kinds of topics. The explosive growth of text data makes it impossible for people to consume all the relevant text data in a timely manner.  The two main techniques to assist people in consuming, digesting, and making use of the text data are:    1. Text retrieval, which helps identify the most relevant text data to a particular problem from a large collection of text documents, thus avoiding processing a large number of non-relevant documents     2. Text mining, which helps users further analyze and digest the found relevant text data and extract actionable knowledge for finishing a task   This course covers both text retrieval and text mining, so as to provide you with the opportunity to see the complete spectrum of techniques used in building an intelligent text information system. Building on two MOOCs covering the same topic and including a course project, this course enables you to learn the basic concepts, principles, and general techniques in text retrieval and mining, as well as gain hands-on experience with using software tools to develop interesting text data applications.  ##  Course Goals and Objectives  Upon successful completion of this course, you will be able to:    * Explain all the basic concepts in text retrieval and text mining.     * Explain the main ideas behind the major models and algorithms for text retrieval and text mining.     * Explain how the major models and algorithms for text retrieval and text mining work.     * Explain how to implement some of the commonly used algorithms for text retrieval and text mining.     * Explain how to evaluate applications of text retrieval and text mining.   ##  Textbook  There is not a required textbook for this course, but there are several optional readings suggested in each week's overview page. All readings listed in the weekly overview pages are optional and are primarily from the following textbook:  Zhai, C. & Massung, S. (2016). _Text data management and analysis: A practical introduction to information retrieval and text mining._ ACM Book Series. Morgan & Claypool Publishers.  ##  Course Outline  **Week**  |  **Dates**  |  **Topics**      ---|---|---      **Week 1**  |  August 23 - August 29  |  Part of Speech tagging, syntactic analysis, semantic analysis, ambiguity, “bag of words” representation, push, pull, querying, browsing, probability ranking principle, relevance, vector space model, dot product, bit vector representation      **Week 2**  |  August 30 -September 5  |  Term frequency (TF), document frequency (DF), and inverse document frequency (IDF), TF transformation, pivoted length normalization, BM25, inverted index and postings, binary coding, unary coding, gamma-coding, d-gap, Zipf’s law      **Week 3**  |  September 6 -12  |  Cranfield evaluation methodology, precision and recall, average precision, mean average precision (MAP), geometric mean average precision (gMAP), reciprocal rank, mean reciprocal rank, F-measure , Normalized Discounted Cumulative Gain (nDCG), statistical significance test      **Week 4**  |  September 13 - 19  |  p(R=1|q,d), query likelihood, p(q|d), statistical and unigram language models, maximum likelihood estimate, background, collection, and document language models, smoothing of unigram language models, relation between query likelihood and TF-IDF weighting, linear interpolation (i.e., Jelinek-Mercer) smoothing, Dirichlet Prior smoothing      **Week 5**  |  September 20 - 26  |  Relevance feedback, pseudo-relevance feedback, implicit feedback, Rocchio feedback, Kullback-Leiber divergence (KL-divergence) retrieval function, mixture language model, scalability and efficiency, spams, crawler, focused crawling, and incremental crawling, Google File System (GFS), MapReduce, link analysis and anchor text, PageRank      **Week 6**  |  September 27 - October 3  |  Content-based filtering, collaborative filtering, Beta-Gamma threshold learning, linear utility , user profile, exploration-exploitation tradeoff, memory-based collaborative filtering, cold start      **Week 7**  |  October 4 - 10  |  Text representation (especially bag-of-words representation), context of a word, context similarity, paradigmatic relation, syntagmatic relation      **Week 8**  |  October 11 - 17  |  Entropy, conditional entropy, mutual information, topics, coverage of topic , language model, generative model, unigram language model, word distribution, background language model, parameters of a probabilistic model, likelihood, Bayes rule, maximum likelihood estimation, prior and posterior distributions, Bayesian estimation & inference, maximum a posteriori (MAP) estimate, prior model, posterior mode. **Exam 1**      **Week 9**  |  October 18 - 24  |  Mixture model, component model, constraints on probabilities, Probabilistic Latent Semantic Analysis (PLSA), Expectation-Maximization (EM) algorithm, E-step and M-step, hidden variables, hill climbing, local maximum, Latent Dirichlet Allocation (LDA)      **Week 10**  |  October 25 - October 31  |  Clustering, document clustering, term clustering, clustering bias, perspective of similarity, Hierarchical Agglomerative Clustering, k-Means, direction evaluation (of clustering), indirect evaluation (of clustering), text categorization, topic categorization, sentiment categorization, email routing , spam filtering, naïve Bayes classifier, smoothing      **Week 11**  |  November 1 - 7  |  Generative classifier vs. discriminative classifier, training data, logistic regression, K-Nearest Neighbor classifier, classification accuracy, precision, recall, F measure, macro-averaging, micro-averaging, opinion holder, opinion target, sentiment, opinion representation, sentiment classification, features, n-grams, frequent patterns, overfitting      **Week 12**  |  November 8 - 14  |  Text-based prediction, the “data mining loop”, context (of text data), contextual text mining, contextual probabilistic latent semantic analysis (CPLSA), views of a topic, coverage of topics, spatiotemporal trends of topics, event impact analysis, network-regularized topic modeling, NetPLSA, causal topics, iterative topic modeling with time series supervision      **Week 13**  |  November 15 - 21  |  Project work week.      **Week 14**  |  November 22 - 28  |  _Thanksgiving Break_      **Week 15**  |  November 29 -December 5  |  **Exam 2;** No new content - work on your final project!      **Week 16**  |  December 6 - 12  |  Final Project Presentation and Report Due Start of Finals Week      ##  Elements of This Course    * **Lecture videos.** Each week your instructor will teach you the concepts you need to know through a collection of short video lectures. You may either stream these videos for playback within the browser by clicking on their titles, or you can download each video for later offline playback by clicking the download icon. **The videos usually total 1.5 to 2 hours each week** , but you generally need to spend at least the same amount of time digesting the content in the videos. The actual amount of time needed to digest the content will naturally vary according to your background.     * **Quizzes.** Most weeks will include one for-credit quiz. You will have two attempts for each quiz, with your highest score used toward your final grade. Your top 10 quiz scores will be used to calculate your final grade (i.e., we will drop the two lowest quiz scores).     * **Exams** . This course will have two 1-hour exams. The exams are intended to test your understanding of the material you learn in the course and will contain questions similar to those seen in the weekly quizzes.     * **Programming Assignments.** The programming assignments for this course provide an opportunity for you to practice your programming skills and apply what you've learned in the course. Set aside about 2-4 hours each week to work on the programming assignment if you plan to finish it.     * **Tech Review (for 4-credit students).** It will require you to generate a short 1-2 page review on an interesting course-related cutting-edge technology topic not covered in any lecture.     * **Final Course Project.** There will also be one culminating project due at the end of course. It will require you to work in groups of at most three. You will build a tool using methods from the course to perform data analysis and also generate documentations and presentation.   ##  Grading  Your final grade will be calculated based on the activities listed in the table below. As a note, the grade on Coursera will NOT accurately reflect your grade in the course.  **Activity**  |  **Percent of Final Grade**      ---|---      Quizzes  |  25%      Programming Assignments  |  25%      Course Project  |  20%      Exam 1  |  15%      Exam 2  |  15%      **Letter Grade**  |  **Percent Needed**  |  **Letter Grade**  |  **Percent Needed**  |  **Letter Grade**  |  **Percent Needed**      ---|---|---|---|---|---      **A+**  |  95  |  **B+**  |  80  |  **C**  |  60      **A**  |  90  |  **B**  |  75  |  **D**  |  55      **A-**  |  85  |  **B-**  |  70  |  **F**  |  <55      Your final grade will be calculated based on the activities listed in the table below. Your official final course grade will be listed in [ Enterprise ](https://apps.uillinois.edu/selfservice/) . The course grade you see displayed in Coursera may not match your official final course grade.  ##  Additional Course Policies  ###  Student Code and Policies  A student at the University of Illinois at the Urbana‑Champaign campus is a member of a University community of which all members have at least the rights and responsibilities common to all citizens, free from institutional censorship; affiliation with the University as a student does not diminish the rights or responsibilities held by a student or any other community member as a citizen of larger communities of the state, the nation, and the world. See the [ University of Illinois Student Code ](http://studentcode.illinois.edu/index.html) for more information.  The CS department also maintains a policies handbook for graduate student. For more information, see the [ Graduate Student Handbook ](https://cs.illinois.edu/sites/default/files/images/CSGraduateStudentHandbook_web.15-16.pdf) .  Additionally, all Coursera learners are required to follow an [ Honor Code ](https://learner.coursera.help/hc/en-us/articles/209818863-Coursera-Honor- Code) and a [ Code of Conduct ](https://learner.coursera.help/hc/en- us/articles/208280036-Coursera-Code-of-Conduct) . Please review both of these items before commencing your studies.  ###  Academic Integrity  All students are expected to abide by [ the campus regulations on academic integrity found in the Student Code of Conduct ](http://admin.illinois.edu/policy/code/article1_part4_1-401.html) . These standards will be enforced and infractions of these rules will not be tolerated in this course. Sharing, copying, or providing any part of a homework solution or code is an infraction of the University’s rules on academic integrity. We will be actively looking for violations of this policy in homework and project submissions. Any violation will be punished as severely as possible with sanctions and penalties typically ranging from a failing grade on this assignment up to a failing grade in the course, including a letter of the offending infraction kept in the student's permanent university record.  Again, a good rule of thumb: _Keep every typed word and piece of code your own_ . If you think you are operating in a gray area, you probably are. If you would like clarification on specifics, please contact the course staff.  ###  Disability Accommodations  Students with learning, physical, or other disabilities requiring assistance should contact the instructor as soon as possible. If you’re unsure if this applies to you or think it may, please contact the instructor and [ Disability Resources and Educational Services (DRES) ](http://disability.illinois.edu/) as soon as possible. You can contact DRES at 1207 S. Oak Street, Champaign, via phone at (217) 333-1970, or via email at [ disability@illinois.edu ](mailto:disability@illinois.edu) .  ###  Late Policy  Late homework and homework by email will not be accepted by the TA or the instructors without prior instructor approval.  
This lecture is about the methods fortext categorization. So in this lecture we're going to discusshow to do text for categorization. First, there're many methods fortext categorization. In such a method the idea isto determine the category based on some rules thatwe design carefully to reflect the domain knowledge aboutthe category prediction problem. So for example, if you want to do topiccategorization for news articles you can say well, if the news article mentionsword like a game and sports three times. That we're going to say it's about sportsthings like that and this would allow us to deterministically decide which categorya document that should be put into. Now such a strategy would work wellif the following conditions hold. First the categories must be very welldefined and this allows the person to clearly decide the categorybased on some clear rules. A certainly the categories as half to be easy to distinguished atthe based on a surface features in text. So that means some officialfeatures like keywords or punctuations or whatever,you can easily identify in text to data. For example, if there is somespecial vocabulary that is known to only occur in a particular category. And that would be most effective becausewe can easily use such a vocabulary or padding of such a vocabularyto recognize this category. Now we also should havesufficient knowledge for designing these words, and so if that'sthe case then such a can be effective. And so it does have a in some domains andsometimes. However, in general, there are severalproblems with this approach. First off, because it's label intensiveit requires a lot of manual work. Obviously, we can't do this forall kinds of categorization problems. We have to do it from scratch fora different problem. problem because given the rules,what they need. So it doesn't scale up well. Secondly, it cannot handleuncertainty in rules, often the rules Aren't 100% reliable. Take for example looking atoccurrences of words in texts and trying to decide the topic. It's actually very hard tohave 100% correct rule. So for example you can say well,if it has game, sports, basketball Then forsure it's about sports. But one can also imagine some types ofarticles that mention these cures, but may not be exactly about sports oronly marginally touching sports. The main topic could be another topic,a different topic than sports. So that's one disadvantageof this approach. And then finally,the rules maybe inconsistent and this would lead to robustness. More specifically, and sometimes, theresults of categorization may be different that depending on whichrule to be applied. So as in that case that youare facing uncertainty. And you will also have to decidean order of applying the rules, or combination of resultsthat are contradictory. So all these are problemswith this approach. And it turns out that bothproblems can be solved or alleviated by using machine learning. So these machine learningmethods are more automatic. But, I still put automaticin quotation marks because they are not really completely automaticcause it still require many work. More specifically we have to usea human experts to help in two ways. First the human experts must annotatedata cells was category labels. And would tell the computer whichdocuments should receive which categories. And this is called training data. And then secondly, the human experts alsoneed to provide a set of features to represent each text object. That can potentially providea clue about the category. So, we need to provide some basicfeatures for the computers to look into. In the case of tax a naturalchoice would be the words. So, using each has a feature isa very common choice to start with, but of course there are othersophisticated features like phrases or even parts of ancients tags oreven syntax to the structures. So once human experts can provide thisthen we can use machine running to learn soft rules forcategorization from the training data. So, soft rules just means, we're going to get decided which categorywe should be assigned for a document, but it's not going to be use usinga rule that is deterministic. So we might use something similarto saying that if it matches games, sports many times,it's likely to be sports. But, we're not going to say exactly forsure but instead, we're going to use probabilities orweights. So that we can combinemuch more evidences. So, the learning process,basically is going to figure out which features are most useful forseparating different categories. And it's going to also figure out how tooptimally combine features to minimize errors of the categorizationof the training data. So the training data,as you can see here, is very important. It's the basis for learning. And then, the trained classifier can beapplied to a new text object to predict the most likely category. And that's to simulatethe prediction of what human Would assign to this text object. If the human were to make a judgement. So when we use machine learning fortext categorization we can also talk about the problem in the generalsetting of supervisement. So the set up is to learna classifier to map a value of X. Into a map of Y sohere X is all the text objects and Y is all the categories,a set of categories. So the class phi will takeany value in x as input and would generate a value in y as output. We hope that output y withthis right category for x. And here correct, of course,is judged based on the training data. So that's a general goal in machinelearning problems or supervised learning problems where you are given some examplesof input and output for a function. And then the computer'sgoing to figure out the, how the function behaveslike based on this examples. And then try to be ableto compute the values for future x's that when we have not seen. So in general all methodswould rely on discriminative features of text objects todistinguish different categories. So that's why these featuresare very important and they have to be provided by humans. And they will also combine multiplefeatures in a weight map with weights to be optimized to minimizeerrors on the training data. So after the learning processesoptimization problem. An objective function is often tiedinto the errors on the training data. Different methods tend to vary intheir ways of measuring the errors on the training data. They might optimizea different objective function, which is often also called a lossfunction or cost function. They also tend to vary in theirways of combining the features. So a linear combination forexample is simple, is often used. But they are not as powerfulas nonlinear combinations. But nonlinear modelsmight be more complex for training, so there are tradeoffs as well. But that would lead todifferent variations of many variations of these learning methods. So in general we can distinguish twokinds of classifiers at a high level. One is called generative classifiers. The other is calleddiscriminative classifiers. The generative classifiers try to learnwhat the data looks like in each category. So it attempts to model the jointdistribution of the data and the label x and y andthis can then be factored out to a product of whythe distribution of labels. And the joint probabilityof sorry the conditional probability of X given Y, so it's Y. So we first modelthe distribution of labels and then we model how the data isgenerate a particular label here. And once we can estimate these models, then we can compute this conditionalprobability of label given data based on the probabilityof data given label. And the label distributionhere by using the Bayes Rule. Now this is the most important thing,because this conditional probability of the label can then be used directlyto decide which label is most likely. So in such approaches objectivefunction is actually likelihood. And so,we model how the data are generated. So it only indirectlycaptures the training errors. But if we can model the datain each category accurately, then we can also classify accurately. One example is NaÃ¯ve Bayes classifier,in this case. The other kind of approachesare called discriminative classifies, and these classifies try to learnwhat features separate categories. So they direct or attack the problem ofcategorization for separation of classes. So sorry for the problem. So, these discriminativeclassifiers attempt to model the conditional probability of the labelgiven the data point directly. So, the objective function tendsto directly measure the errors of categorization on the training data. Some examples includea logistical regression, support vector machines,and k-nearest neighbors. We will cover some of these classifiersin detail in the next few lectures. [MUSIC]
[MUSIC] This lecture is abouta specific technique for Contextual Text Mining called ContextualProbabilistic Latent Semantic Analysis. In this lecture, we're going to continuediscussing Contextual Text Mining. And we're going to introduce ContextualProbablitistic Latent Semantic Analysis as exchanging of POS fordoing contextual text mining. Recall that in contextual text miningwe hope to analyze topics in text, in consideration of the context so that we can associate the topics with aproperty of the context were interesting. So in this approach, contextualprobabilistic latent semantic analysis, or CPLSA, the main idea is toexpress to the add interesting context variables into a generating model. Recall that before when we generatethe text we generally assume we'll start wIth some topics, andthen assemble words from some topics. But here, we're going to add contextvariables, so that the coverage of topics, and also the content of topicswould be tied in context. Or in other words, we're going to letthe context Influence both coverage and the content of a topic. The consequences that this will enableus to discover contextualized topics. Make the topics more interesting,more meaningful. Because we can then have topicsthat can be interpreted as specifically to a particularcontext that we are interested in. For example, a particular time period. As an extension of PLSA model, CPLSA does the following changes. Firstly it would model the conditionallikelihood of text given context. That clearly suggests that the generationof text would then depend on context, and that allows us to bringcontext into the generative model. Secondly, it makes two specificassumptions about the dependency of topics on context. One is to assume that depending onthe context, depending on different time periods or different locations, we assumethat there are different views of a topic or different versions of worddescriptions that characterize a topic. And this assumption allowsus to discover different variations of the same topicin different contexts. The other is that we assume the topiccoverage also depends on the context. That means depending on the time or location, we might covertopics differently. Again, this dependencywould then allow us to capture the association oftopics with specific contexts. We can still use the EM algorithm to solvethe problem of parameter estimation. And in this case, the estimated parameterswould naturally contain context variables. And in particular, a lot of conditional probabilitiesof topics given certain context. And this is what allows youto do contextual text mining. So this is the basic idea. Now, we don't have time tointroduce this model in detail, but there are references here that youcan look into to know more detail. Here I just want to explain the highlevel ideas in more detail. Particularly I want to explainthe generation process. Of text data that has contextassociated in such a model. So as you see here, we can assumethere are still multiple topics. For example, some topics might representa themes like a government response, donation Or the city of New Orleans. Now this example is in the contextof Hurricane Katrina and that hit New Orleans. Now as you can see weassume there are different views associated with each of the topics. And these are shown as View 1,View 2, View 3. Each view is a differentversion of word distributions. And these views are tiedto some context variables. For example, tied to the location Texas,or the time July 2005, or the occupation of the authorbeing a sociologist. Now, on the right side, now we assumethe document has context information. So the time is known to be July 2005. The location is Texas, etc. And such context information iswhat we hope to model as well. So we're not going to just model the text. And so one idea here is to modelthe variations of top content and various content. And this gives us different viewsof the water distributions. Now on the bottom you will see the themecoverage of top Coverage might also vary according to these contextbecause in the case of a location like Texas, people mightwant to cover the red topics more. That's New Orleans. That's visualized here. But in a certain time period, maybe Particular topic andwill be covered more. So this variation isalso considered in CPLSA. So to generate the searcher document Withcontext, with first also choose a view. And this view of course now couldbe from any of these contexts. Let's say, we have taken thisview that depends on the time. In the middle. So now, we will have a specificversion of word distributions. Now, you can see some probabilitiesof words for each topic. Now, once we have chosen a view, now the situation will be very similarto what happened in standard ((PRSA)) We assume we have got word distributionassociated with each topic, right? And then next, we will also choosea coverage from the bottom, so we're going to choose a particularcoverage, and that coverage, before is fixed in PLSA, andassigned to a particular document. Each document has just onecoverage distribution. Now here, because we consider context, sothe distribution of topics or the coverage of Topics can vary depending on thecontext that has influenced the coverage. So, for example,we might pick a particular coverage. Let's say in this case we pickeda document specific coverage. Now with the coverage andthese word distributions we can generate a document inexactly the same way as in PLSA. So what it means, we're going touse the coverage to choose a topic, to choose one of these three topics. Let's say we have picked the yellow topic. Then we'll draw a word from thisparticular topic on the top. Okay, sowe might get a word like government. And then next time we mightchoose a different topic, and we'll get donate, etc. Until we generate all the words. And this is basicallythe same process as in PLSA. So the main difference iswhen we obtain the coverage. And the word distribution,we let the context influence our choice So in other words we have extra switchesthat are tied to these contacts that will control the choices of different viewsof topics and the choices of coverage. And naturally the model we havemore parameters to estimate. But once we can estimate thoseparameters that involve the context, then we will be able to understandthe context specific views of topics, or context specific coverages of topics. And this is precisely what wewant in contextual text mining. So here are some simple results. From using such a model. Not necessary exactly the same model,but similar models. So on this slide you seesome sample results of comparing news articles about Iraq War andAfghanistan War. Now we have about 30 articles on Iraqwa,r and 26 articles on Afghanistan war. And in this case,the goal is to review the common topic. It's covered in both sets of articles and the differences of variations ofthe topic in each of the two collections. So in this case the context is explicitlyspecified by the topic or collection. And we see the results hereshow that there is a common theme that's corresponding toCluster 1 here in this column. And there is a common theme indicting thatUnited Nations is involved in both Wars. It's a common topic coveredin both sets of articles. And that's indicated by the highprobability words shown here, united and nations. Now if you know the background,of course this is not surprising and this topic is indeed veryrelevant to both wars. If you look at the column further andthen what's interesting's that the next two cells of worddistributions actually tell us collection specific variationsof the topic of United Nations. So it indicates that the Iraq War, United Nations was more involvedin weapons factions, whereas in the Afghanistan War it was more involvedin maybe aid to Northern Alliance. It's a different variation ofthe topic of United Nations. So this shows that bybringing the context. In this case different the walls ordifferent the collection of texts. We can have topical variationstied to these contexts, to review the differences of coverageof the United Nations in the two wars. Now similarly if you look atthe second cluster Class two, it has to do with the killing of people,and, again, it's not surprising if you knowthe background about wars. All the wars involve killing of people,but imagine if you are not familiarwith the text collections. We have a lot of text articles, and such a technique can reveal the commontopics covered in both sets of articles. It can be used to review common topicsin multiple sets of articles as well. If you look at of course inthat column of cluster two, you see variations of killing of peopleand that corresponds to different contexts And here is another example of results obtained from blog articlesabout Hurricane Katrina. In this case,what you see here is visualization of the trends of topics over time. And the top one shows justthe temporal trends of two topics. One is oil price, and one is aboutthe flooding of the city of New Orleans. Now these topics are obtained fromblog articles about Hurricane Katrina. And people talk about these topics. And end up teaching to some other topics. But the visualisation showsthat with this technique, we can have conditionaldistribution of time. Given a topic. So this allows us to plotthis conditional probability the curve is like what you're seeing here. We see that, initially, the twocurves tracked each other very well. But later we see the topic of New Orleanswas mentioned again but oil price was not. And this turns out to be the time period when another hurricane,hurricane Rita hit the region. And that apparently triggered morediscussion about the flooding of the city. The bottom curve showsthe coverage of this topic about flooding of the city by blockarticles in different locations. And it also shows some shift ofcoverage that might be related to people's migrating from the stateof Louisiana to Texas for example. So in this case we can see the time canbe used as context to review trends of topics. These are some additionalresults on spacial patterns. In this case it was aboutthe topic of government response. And there was some criticism aboutthe slow response of government in the case of Hurricane Katrina. And the discussion now iscovered in different locations. And these visualizations show the coveragein different weeks of the event. And initially it's coveredmostly in the victim states, in the South, but then graduallyspread into other locations. But in week four,which is shown on the bottom left, we see a pattern that's very similarto the first week on the top left. And that's when againHurricane Rita hit in the region. So such a technique would allowus to use location as context to examine their issues of topics. And of course the moralis completely general so you can apply this to anyother connections of text. To review spatial temporal patterns. His view found another applicationof this kind of model, where we look at the use of the model forevent impact analysis. So here we're looking at the researcharticles information retrieval. IR, particularly SIGIR papers. And the topic we are focusing onis about the retrieval models. And you can see the top words with highprobability about this model on the left. And then we hope to examinethe impact of two events. One is a start of TREC, forText and Retrieval Conference. This is a major evaluationsponsored by U.S. government, and was launched in 1992 oraround that time. And that is known to have made a impact on the topics of researchinformation retrieval. The other is the publication ofa seminal paper, by Croft and Porte. This is about a language modelapproach to information retrieval. It's also known to have made a highimpact on information retrieval research. So we hope to use this kind ofmodel to understand impact. The idea here is simply touse the time as context. And use these events to dividethe time periods into a period before. For the event andanother after this event. And then we can comparethe differences of the topics. The and the variations, etc. So in this case,the results show before track the study of retrieval models was mostly a vectorspace model, Boolean model etc. But the after Trec, apparently the study of retrieval modelshave involved a lot of other words. That seems to suggest somedifferent retrieval tasks, so for example, email was used inthe enterprise search tasks and subtopical retrieval was anothertask later introduced by Trec. On the bottom,we see the variations that are correlated with the propagation ofthe language model paper. Before, we have those classicprobability risk model, logic model, Boolean etc., but after 1998, we see clear dominance of languagemodel as probabilistic models. And we see words like language model,estimation of parameters, etc. So this technique here can use events ascontext to understand the impact of event. Again the technique is generals so you can use this to analyzethe impact of any event. Here are some suggested readings. The first is paper about simple staging ofpsi to label cross-collection comparison. It's to perform comparativetext mining to allow us to extract common topics sharedby multiple collections. And there are variationsin each collection. The second one is the mainpaper about the CPLSA model. Was a discussion of a lot of applications. The third one has a lot of detailsabout the special temporal patterns for the Hurricane Katrina example. [MUSIC]
[SOUND] So here are some specific examples of what we can't do today and part of speech tagging is stillnot easy to do 100% correctly. So in the example, he turned off thehighway verses he turned off the fan and the two offs actually have somewhata differentness in their active categories and also its very difficultto get a complete the parsing correct. Again, the example, a man saw a boywith a telescope can actually be very difficult to parsedepending on the context. Precise deep semanticanalysis is also very hard. For example, to define the meaning of own, precisely is very difficult inthe sentence, like John owns a restaurant. So the state of the off canbe summarized as follows. Robust and general NLP tends to be shallow whilea deep understanding does not scale up. For this reason in this course,the techniques that we cover are in general, shallow techniques foranalyzing text data and mining text data and they are generallybased on statistical analysis. So there are robust andgeneral and they are in the in category of shallow analysis. So such techniques havethe advantage of being able to be applied to any text data inany natural about any topic. But the downside is that, they don'tgive use a deeper understanding of text. For that, we have to rely ondeeper natural language analysis. That typically would requirea human effort to annotate a lot of examples of analysis that wouldlike to do and then computers can use machine learning techniques and learn fromthese training examples to do the task. So in practical applications, we generallycombine the two kinds of techniques with the general statistical andmethods as a backbone as the basis. These can be applied to any text data. And on top of that, we're going to usehumans to, and you take more data and to use supervised machine learningto do some tasks as well as we can, especially for those importanttasks to bring humans into the loop to analyze text data more precisely. But this course will coverthe general statistical approaches that generally,don't require much human effort. So they're practically,more useful that some of the deeper analysis techniques that require a lot ofhuman effort to annotate the text today. So to summarize,the main points we take are first NLP is the foundation for text mining. So obviously, the better wecan understand the text data, the better we can do text mining. Computers today are far from being ableto understand the natural language. Deep NLP requires common senseknowledge and inferences. Thus, only working forvery limited domains not feasible for large scale text mining. Shallow NLP based on statisticalmethods can be done in large scale and is the main topic of this course and they are generally applicableto a lot of applications. They are in some sense also,more useful techniques. In practice,we use statistical NLP as the basis and we'll have humans forhelp as needed in various ways. [MUSIC]
[SOUND] In general, we can use the empirical count of events in the observed datato estimate the probabilities. And a commonly used technique iscalled a maximum likelihood estimate, where we simply normalizethe observe accounts. So if we do that, we can see, we cancompute these probabilities as follows. For estimating the probability thatwe see a water current in a segment, we simply normalize the count ofsegments that contain this word. So let's first takea look at the data here. On the right side, you see a list of some,hypothesizes the data. These are segments. And in some segments you see both wordsoccur, they are indicated as ones for both columns. In some other cases only one will occur,so only that column has one and the other column has zero. And in all, of course, in some othercases none of the words occur, so they are both zeros. And for estimating these probabilities, wesimply need to collect the three counts. So the three counts are first,the count of W1. And that's the total number ofsegments that contain word W1. It's just as the ones in the column of W1. We can count how manyones we have seen there. The segment count is for word 2, and wejust count the ones in the second column. And these will give us the totalnumber of segments that contain W2. The third count is when both words occur. So this time, we're going to countthe sentence where both columns have ones. And then, so this would give usthe total number of segments where we have seen both W1 and W2. Once we have these counts,we can just normalize these counts by N, which is the total number of segments, and this will give us the probabilities thatwe need to compute original information. Now, there is a small problem,when we have zero counts sometimes. And in this case, we don't want a zeroprobability because our data may be a small sample and in general, we wouldbelieve that it's potentially possible for a [INAUDIBLE] to avoid any context. So, to address this problem,we can use a technique called smoothing. And that's basically to add somesmall constant to these counts, and so that we don't getthe zero probability in any case. Now, the best way to understand smoothingis imagine that we actually observed more data than we actually have, because we'llpretend we observed some pseudo-segments. I illustrated on the top,on the right side on the slide. And these pseudo-segments wouldcontribute additional counts of these words sothat no event will have zero probability. Now, in particular we introducethe four pseudo-segments. Each is weighted at one quarter. And these represent the four differentcombinations of occurrences of this word. So now each event,each combination will have at least one count or at least a non-zerocount from this pseudo-segment. So, in the actual segmentsthat we'll observe, it's okay if we haven't observedall of the combinations. So more specifically, you can seethe 0.5 here after it comes from the two ones in the two pseudo-segments,because each is weighted at one quarter. We add them up, we get 0.5. And similar to this,0.05 comes from one single pseudo-segment that indicatesthe two words occur together. And of course in the denominator we addthe total number of pseudo-segments that we add, in this case,we added a four pseudo-segments. Each is weighed at one quarter sothe total of the sum is, after the one. So, that's why in the denominatoryou'll see a one there. So, this basically concludesthe discussion of how to compute a these four syntagmatic relation discoveries. Now, so to summarize,syntagmatic relation can generally be discovered by measuring correlationsbetween occurrences of two words. We've introduced the threeconcepts from information theory. Entropy, which measures the uncertaintyof a random variable X. Conditional entropy, which measuresthe entropy of X given we know Y. And mutual information of X and Y,which matches the entropy reduction of X due to knowing Y, orentropy reduction of Y due to knowing X. They are the same. So these three concepts are actually veryuseful for other applications as well. That's why we spent some timeto explain this in detail. But in particular,they are also very useful for discovering syntagmatic relations. In particular,mutual information is a principal way for discovering such a relation. It allows us to have valuescomputed on different pairs of words that are comparable andso we can rank these pairs and discover the strongest syntagmaticfrom a collection of documents. Now, note that there is some relationbetween syntagmatic relation discovery and [INAUDIBLE] relation discovery. So we already discussed the possibilityof using BM25 to achieve waiting for terms in the context to potentiallyalso suggest the candidates that have syntagmatic relationswith the candidate word. But here, once we use mutual informationto discover syntagmatic relations, we can also represent the context withthis mutual information as weights. So this would give usanother way to represent the context of a word, like a cat. And if we do the same for all the words,then we can cluster these words or compare the similarity between thesewords based on their context similarity. So this provides yetanother way to do term weighting for paradigmatic relation discovery. And so to summarize this whole partabout word association mining. We introduce two basic associations,called a paradigmatic and a syntagmatic relations. These are fairly general, they applyto any items in any language, so the units don't have to be words,they can be phrases or entities. We introduced multiple statisticalapproaches for discovering them, mainly showing that purestatistical approaches are visible, are variable fordiscovering both kind of relations. And they can be combined toperform joint analysis, as well. These approaches can be appliedto any text with no human effort, mostly because they are basedon counting of words, yet they can actually discoverinteresting relations of words. We can also use different ways withdefining context and segment, and this would lead us to some interestingvariations of applications. For example, the context can be verynarrow like a few words, around a word, or a sentence, or maybe paragraphs,as using differing contexts would allows to discover different flavorsof paradigmatical relations. And similarly,counting co-occurrences using let's say, visual information to discoversyntagmatical relations. We also have to define the segment, andthe segment can be defined as a narrow text window or a longer text article. And this would give us differentkinds of associations. These discovery associations cansupport many other applications, in both information retrieval andtext and data mining. So here are some recommended readings,if you want to know more about the topic. The first is a book witha chapter on collocations, which is quite relevant tothe topic of these lectures. The second is an articleabout using various statistical measures todiscover lexical atoms. Those are phrases thatare non-compositional. For example,hot dog is not really a dog that's hot, blue chip is not a chip that's blue. And the paper has a discussion about sometechniques for discovering such phrases. The third one is a new paper on a unifiedway to discover both paradigmatical relations and a syntagmatical relations,using random works on word graphs. [SOUND]
[SOUND]So this is indeed a general idea ofthe Expectation-Maximization, or EM, Algorithm. So in all the EM algorithms weintroduce a hidden variable to help us solve the problem more easily. In our case the hidden variableis a binary variable for each occurrence of a word. And this binary variable wouldindicate whether the word has been generated from 0 sub d or 0 sub p. And here we show some possiblevalues of these variables. For example, for the it's from background,the z value is one. And text on the other hand. Is from the topic then it's zero forz, etc. Now, of course, we don't observe these zvalues, we just imagine they're all such. Values of z attaching to other words. And that's why we callthese hidden variables. Now, the idea that wetalked about before for predicting the word distribution thathas been used when we generate the word is it a predictor,the value of this hidden variable? And, so, the EM algorithm then,would work as follows. First, we'll initialize allthe parameters with random values. In our case,the parameters are mainly the probability. of a word, given by theta sub d. So this is an initial addition stage. These initialized values would allowus to use base roll to take a guess of these z values, sowe'd guess these values. We can't say for sure whethertextt is from background or not. But we can have our guess. This is given by this formula. It's called an E-step. And so the algorithm would then try touse the E-step to guess these z values. After that, it would then invokeanother that's called M-step. In this step we simply take advantageof the inferred z values and then just group words that are inthe same distribution like these from that ground including this as well. We can then normalize the countto estimate the probabilities or to revise our estimate of the parameters. So let me also illustratethat we can group the words that are believed to havecome from zero sub d, and that's text, mining algorithm,for example, and clustering. And we group them together to help us re-estimate the parametersthat we're interested in. So these will help usestimate these parameters. Note that before we just setthese parameter values randomly. But with this guess, we will havesomewhat improved estimate of this. Of course, we don't know exactlywhether it's zero or one. So we're not going to reallydo the split in a hard way. But rather we're going todo a softer split. And this is what happened here. So we're going to adjust the count bythe probability that would believe this word has been generatedby using the theta sub d. And you can see this,where does this come from? Well, this has come from here, right? From the E-step. So the EM Algorithm woulditeratively improve uur initial estimate of parameters by usingE-step first and then M-step. The E-step is to augment the datawith additional information, like z. And the M-step is to take advantage of the additional informationto separate the data. To split the data accounts andthen collect the right data accounts to re-estimate our parameter. And then once we have a new generation ofparameter, we're going to repeat this. We are going the E-step again. To improve our estimateof the hidden variables. And then that would lead to anothergeneration of re-estimated parameters. For the word distributionthat we are interested in. Okay, so, as I said,the bridge between the two is really the variable z, hidden variable,which indicates how likely this water is from the top waterdistribution, theta sub p. So, this slide has a lot of content andyou may need to. Pause the reader to digest it. But this basically capturesthe essence of EM Algorithm. Start with initial values thatare often random themself. And then we invoke E-step followedby M-step to get an improved setting of parameters. And then we repeated this, sothis a Hill-Climbing algorithm that would gradually improvethe estimate of parameters. As I will explain laterthere is some guarantee for reaching a local maximum ofthe log-likelihood function. So lets take a look at the computation fora specific case, so these formulas are the EM. Formulas that you see before, andyou can also see there are superscripts, here, like here, n,to indicate the generation of parameters. Like here for example we have n plus one. That means we have improved. From here to here we have an improvement. So in this setting we have assumed the twonumerals have equal probabilities and the background model is null. So what are the relevanceof the statistics? Well these are the word counts. So assume we have just four words,and their counts are like this. And this is our background model thatassigns high probabilities to common words like the. And in the first iteration,you can picture what will happen. Well first we initialize all the values. So here, this probability that we'reinterested in is normalized into a uniform distribution of all the words. And then the E-step would give us a guessof the distribution that has been used. That will generate each word. We can see we have differentprobabilities for different words. Why? Well, that's because these words havedifferent probabilities in the background. So even though the twodistributions are equally likely. And then our initial audition say uniformdistribution because of the difference in the background of the distribution,we have different guess the probability. So these words are believed tobe more likely from the topic. These on the other hand are less likely. Probably from background. So once we have these z values, we know in the M-step these probabilitieswill be used to adjust the counts. So four must be multiplied by this 0.33 in order to get the allocatedaccounts toward the topic. And this is done by this multiplication. Note that if our guess says thisis 100% If this is one point zero, then we just get the full countof this word for this topic. In general it's not goingto be one point zero. So we're just going to get some percentageof this counts toward this topic. Then we simply normalize these counts to have a new generationof parameters estimate. So you can see, compare this withthe older one, which is here. So compare this with this one andwe'll see the probability is different. Not only that, we also see some words that are believed to have come fromthe topic will have a higher probability. Like this one, text. And of course, this new generation ofparameters would allow us to further adjust the inferred latent variable orhidden variable values. So we have a new generation of values, because of the E-step based onthe new generation of parameters. And these new inferred valuesof Zs will give us then another generation of the estimateof probabilities of the word. And so on and so forth so this is whatwould actually happen when we compute these probabilitiesusing the EM Algorithm. As you can see in the last rowwhere we show the log-likelihood, and the likelihood is increasingas we do the iteration. And note that these log-likelihood isnegative because the probability is between 0 and 1 when you take a logarithm,it becomes a negative value. Now what's also interesting is,you'll note the last column. And these are the inverted word split. And these are the probabilitiesthat a word is believed to have come from one distribution, in thiscase the topical distribution, all right. And you might wonder whetherthis would be also useful. Because our main goal is toestimate these word distributions. So this is our primary goal. We hope to have a more discriminativeorder of distribution. But the last column is also bi-product. This also can actually be very useful. You can think about that. We want to use, is to for example is to estimate to what extent thisdocument has covered background words. And this, when we add this up or take the average we will kind of know towhat extent it has covered background versus content was that are notexplained well by the background. [MUSIC]
##  Course Deadlines  ###  **Quizzes**  **Assignment**  |  **Release Date**  |  **Hard Deadline**      ---|---|---      Quiz 1  |  First day of class  |  End of Week 7      Quiz 2  |  First day of class  |  End of Week 7      Quiz 3  |  First day of class  |  End of Week 7      Quiz 4  |  First day of class  |  End of Week 7      Quiz 5  |  First day of class  |  End of Week 7      Quiz 6  |  First day of class  |  End of Week 7      Quiz 7  |  First day of class  |  End of Week 13      Quiz 8  |  First day of class  |  End of Week 13      Quiz 9  |  First day of class  |  End of Week 13      Quiz 10  |  First day of class  |  End of Week 13      Quiz 11  |  First day of class  |  End of Week 13      Quiz 12  |  First day of class  |  End of Week 13      ###  **Programming Assignments**  **Assignment**  |  **Release Date**  |  **Due Date**  |  **Hard Deadline**      ---|---|---|---      Programming Assignment 1  |  Week 2  |  Sept 5 11:59 pm CDT  |  End of Week 7*      Programming Assignment 2.1  |  Week 2  |  Sept 12, 11:59 pm CDT  |  End of Week 7*      Programming Assignment 2.2  |  Week 2  |  Sept 19, 11:59 pm CDT  |  End of Week 7*      Programming Assignment 2.3  |  Week 2  |  Sept 26, 11:59 pm CDT  |  End of Week 7*      Programming Assignment 2.4  |  Week 2  |  Oct 3, 11:59 pm CDT  |  End of Week 7*      Programming Assignment 3  |  Week 2  |  Oct 24, 11:59 pm CDT  |  End of Week 13*      ###  **Course Project**  **Assignment**  |  **Release Date**  |  **Hard Deadline**      ---|---|---      Team Formation  |  TBA  |  Oct 18, 11:59 pm CDT      Project Proposal Submission  |  TBA  |  Oct 24, 11:59 pm CDT      Project Progress Report Submission  |  TBA  |  Nov 15, 11:59 pm CDT      Project Progress Report Peer Review  |  TBA  |  Nov 19, 11:59 pm CDT      Project Code and Documentation Submission  |  TBA  |  Dec 09, 11:59 pm CDT      Project Presentation Submission  |  TBA  |  Dec 09, 11:59 pm CDT      Project Code, Documentation, Presentation Peer Review  |  TBA  |  Dec 17, 11:59 pm CDT      ###  **Technology Review (4-credit students only)**  **Assignment**  |  **Release Date**  |  **Hard Deadline**      ---|---|---      Technology review submission  |  TBA  |  Nov 7, 11:59 pm CDT      ###  **Exams**  **Exam Name**  |  **Exam Start Date**  |  **Exam End Date**      ---|---|---      Exam 1  |  Mon., Oct 11, 9:00am CDT  |  Thu., Oct 14, 10:00pm CDT      Exam 2  |  Mon., Nov 29, 9:00am CDT  |  Sun. Dec 5, 10:00pm CDT      * dates are tentative and are subject to change   Please see the informational item about using ProctorU in Orientation module.  ##  Late Policy    * Unless otherwise specified, all assignments are **due at 11:59 p.m. US Central Time on the due date** . ( [ Time Zone Converter ](http://www.thetimezoneconverter.com/) )     * MPs submitted after the due date and before the hard deadline will receive a 50% penalty.     * The hard deadline is the deadline after which you will receive 0 points on assignments regardless how well you did on the assignment. No late submission will be accepted except under extremely rare non-academic circumstances (which usually require approval from the Dean's office).     * No assignment will be accepted after December 9th.   ##  Academic Calendar    * The Graduate College at the University of Illinois maintains a [ Graduate College Calendar ](https://urldefense.proofpoint.com/v2/url?u=http-3A__illinois.edu_calendar_list_3284-3Fcal-3D20120724-26skinId-3D4101&d=CwMFaQ&c=8hUWFZcy2Z-Za5rBPlktOQ&r=hwR2puiSzExHAjayhdTmeP2QBwCIF4_yCon7vCbLJeI&m=KxaHnwchPxVBZQZVG3yfQvwdKXDksRe21cFOPWqsqD8&s=JKGtWB4YQ8WHerFzrKjQQsXX8xY3BCsF7Svtt-FTpZM&e=) . The calendar includes important dates such as final exam dates, course registration and cancellation, and holidays.     * There is also a [ campus wide calendar ](https://urldefense.proofpoint.com/v2/url?u=http-3A__illinois.edu_calendar_list_557&d=CwMFaQ&c=8hUWFZcy2Z-Za5rBPlktOQ&r=hwR2puiSzExHAjayhdTmeP2QBwCIF4_yCon7vCbLJeI&m=KxaHnwchPxVBZQZVG3yfQvwdKXDksRe21cFOPWqsqD8&s=FnL6Qy3PV8bbPZBIWReFy22R4oRUzT25OBvSxGRzTiM&e=) available.     * The CS Department also sends reminders about upcoming deadlines. You will also receive the Graduate College newsletter in your Exchange email account.   
[SOUND] This lecture is a overview oftext retrieval methods. In the previous lecture, we introducedthe problem of text retrieval. We explained that the main problem is the design of ranking functionto rank documents for a query. In this lecture, we will give an overview of differentways of designing this ranking function. So the problem is the following. We have a query that hasa sequence of words and the document that's alsoa sequence of words. And we hope to define a function f that can compute a score basedon the query and document. So the main challenge you hear is withdesign a good ranking function that can rank all the relevant documentson top of all the non-relevant ones. Clearly, this means our functionmust be able to measure the likelihood that a documentd is relevant to a query q. That also means we have to havesome way to define relevance. In particular, in order toimplement the program to do that, we have to have a computationaldefinition of relevance. And we achieve this goal bydesigning a retrieval model, which gives usa formalization of relevance. Now, over many decades, researchers have designed manydifferent kinds of retrieval models. And they fall into different categories. First, one family of the modelsare based on the similarity idea. Basically, we assume that ifa document is more similar to the query than another document is, then we will say the first documentis more relevant than the second one. So in this case,the ranking function is defined as the similarity between the query andthe document. One well known example in thiscase is vector space model, which we will cover more indetail later in the lecture. A second kind of modelsare called probabilistic models. In this family of models, we follow a verydifferent strategy, where we assume that queries and documents are allobservations from random variables. And we assume there is a binaryrandom variable called R here to indicate whether a documentis relevant to a query. We then define the score of document withrespect to a query as a probability that this random variable R is equal to 1,given a particular document query. There are different casesof such a general idea. One is classic probabilistic model,another is language model, yet another is divergencefrom randomness model. In a later lecture, we will talk moreabout one case, which is language model. A third kind of model are basedon probabilistic inference. So here the idea is to associateuncertainty to inference rules, and we can then quantifythe probability that we can show that the queryfollows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to definea set of constraints that we hope a good retrieval function to satisfy. So in this case, the problem isto seek a good ranking function that can satisfy allthe desired constraints. Interestingly, although these differentmodels are based on different thinking, in the end, the retrieval functiontends to be very similar. And these functions tend toalso involve similar variables. So now let's take a look at the commonform of a state of the art retrieval model and to examine some of the commonideas used in all these models. First, these models are allbased on the assumption of using bag of words to represent text,and we explained this in the naturallanguage processing lecture. Bag of words representation remainsthe main representation used in all the search engines. So with this assumption,the score of a query, like a presidential campaign newswith respect to a document of d here, would be based on scores computedbased on each individual word. And that means the score woulddepend on the score of each word, such as presidential, campaign, and news. Here, we can see thereare three different components, each corresponding to how well thedocument matches each of the query words. Inside of these functions,we see a number of heuristics used. So for example, one factor thataffects the function d here is how many times does the wordpresidential occur in the document? This is called a term frequency, or TF. We might also denote asc of presidential and d. In general, if the word occursmore frequently in the document, then the value of thisfunction would be larger. Another factor is,how long is the document? And this is to use the document length forscoring. In general, if a term occurs in a long document many times,it's not as significant as if it occurred the same numberof times in a short document. Because in a long document, any termis expected to occur more frequently. Finally, there is this factorcalled document frequency. That is, we also want to look at howoften presidential occurs in the entire collection, and we call this documentfrequency, or df of presidential. And in some other models,we might also use a probability to characterize this information. So here, I show the probability ofpresidential in the collection. So all these are trying to characterizethe popularity of the term in the collection. In general, matching a rare term inthe collection is contributing more to the overall score thanmatching up common term. So this captures some of the main ideasused in pretty much older state of the art original models. So now, a natural question is,which model works the best? Now it turns out that manymodels work equally well. So here are a list ofthe four major models that are generally regarded asa state of the art original models, pivoted length normalization,BM25, query likelihood, PL2. When optimized,these models tend to perform similarly. And this was discussed in detail in thisreference at the end of this lecture. Among all these,BM25 is probably the most popular. It's most likely that this has been usedin virtually all the search engines, and you will also often see thismethod discussed in research papers. And we'll talk more about thismethod later in some other lectures. So, to summarize, the main points madein this lecture are first the design of a good ranking function pre-requires acomputational definition of relevance, and we achieve this goal by designingappropriate retrieval model. Second, many models are equally effective,but we don't have a single winner yet. Researchers are still active andworking on this problem, trying to find a trulyoptimal retrieval model. Finally, the state of the artranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF anddocument frequency of words. Such information is used inthe weighting function to determine the overall contribution of matchinga word and document length. These are often combined in interestingways, and we'll discuss how exactly they are combined to rankdocuments in the lectures later. There are two suggested additionalreadings if you have time. The first is a paper where you canfind the detailed discussion and comparison of multiplestate of the art models. The second is a book witha chapter that gives a broad review of different retrieval models. [MUSIC]
[MUSIC] This lecture is about the implementationof text retrieval systems. In this lecture we will discusshow we can implement a text retrieval method to build a search engine. The main challenge is tomanage a lot of text data and to enable a query to be answered veryquickly and to respond to many queries. This is a typical textretrieval system architecture. We can see the documents are firstprocessed by a tokenizer to get tokenized units, for example, words. And then, these words, ortokens, will be processed by a indexer that will create a index,which is a data structure for the search engine to useto quickly answer a query. And the query would be goingthrough a similar processing step. So the Tokenizer would beapprised of the query as well, so that the text can beprocessed in the same way. The same units would bematched with each other. The query's representation wouldthen be given to the Scorer, which would use the index to quicklyanswer user's query by scoring the documents and then ranking them. The results will be given to the user. And then the user can look at the resultsand provided us some feedback that can be explicit judgements of bothwhich documents are good, which documents are bad. Or implicit feedback such as so thatuser didn't have to do anything extra. End user will just look at the results,and skip some, andclick on some result to view. So these interacting signals can be usedby the system to improve the ranking accuracy by assuming that viewed documentsare better than the skipped ones. So a search engine system thencan be divided into three parts. The first part is the indexer, andthe second part is a Scorer that responds to the users query, andthe third part is a Feedback mechanism. Now typically, the Indexer isdone in the offline manner, so you can pre-process the correct data and to build the inventory index,which we will introduce in moment. And this data structure can then be usedby the online module which is a scorer to process a user's query dynamically andquickly generate search results. The feedback mechanism can be done onlineor offline, depending on the method. The implementation of the indexer andthe scorer is very standard, and this is the main topic of thislecture and the next few lectures. The feedback mechanism,on the other hand, has variations, it depends on which method is used. So that is usually done inalgorithms specific way. Let's first talk about the tokenizer. Tokernization is a normalized lexicalunits in through the same form, so that semantically similar wordscan be matched with each other. Now, in the language like English,stemming is often used and this will map all the inflectionalforms of words into the same root form. So for example, computer, computation, and computing can all be matchedto the root form compute. This way all these different forms ofcomputing can be matched with each other. Now normally, this is a good idea, to increase the coverage of documentsthat are matched up with this query. But it's also not always beneficial, because sometimes the subtlestdifference between computer and computation might still suggest thedifference in the coverage of the content. But in most cases,stemming seems to be beneficial. When we tokenize the text in some otherlanguages, for example Chinese, we might face some special challenges in segmentingthe text to find the word boundaries. Because it's not obviouswhere the boundary is as there's no space to separate them. So here of course, we have to use somelanguage specific processing techniques. Once we do tokenization, then we wouldindex the text documents and than it'll convert the documents and do some datastructure that can enable faster search. The basic idea is to precomputeas much as we can basically. So the most commonly used indexis call an Inverted index. And this has been usedin many search engines to support basic search algorithms. Sometimes the other indices, for example, document index might be needed in orderto support feedback, like I said. And these kind of techniquesare not really standard in that they vary a lot accordingto the feedback methods. To understand why we want to useinverted index it will be useful for you to think about how you wouldrespond to a single term query quickly. So if you want to use more time tothink about that, pause the video. So think about how you canpre process the text data so that you can quickly respondto a query with just one word. Where if you have thoughtabout that question, you might realize that wherethe best is to simply create the list of documents that matchevery term in the vocabulary. In this way, you can basicallypre-construct the answers. So when you see a term you can simply justto fetch the random list of documents for that term and return the list to the user. So that's the fastest way torespond to a single term here. Now the idea of the invert indexis actually, basically, like that. We're going to do pre-constructedsearch an index, that will allows us to quickly find all the documentsthat match a particular term. So let's take a look at this example. We have three documents here, and these are the documents that youhave seen in some previous lectures. Suppose that we want to createan inverted index for these documents. Then we want to maintain a dictionary, inthe dictionary we will have one entry for each term and we're going to storesome basic statistics about the term. For example, the number ofdocuments that match the term, or the total number of code orfrequency of the term, which means we would kind of duplicatethe occurrences of the term. And so, for example, news, this term occur in allthe three documents, so the count of documents is three. And you might also realize we needed thiscount of documents, or document frequency, for computing some statistics tobe used in the vector space model. Can you think of that? So what weighting heuristicwould need this count. Well, that's the idea, right,inverse document frequency. So, IDF is the property of a term,and we can compute it right here. So, with the document that count here,it's easy to compute the idea of, either at this time, orwith the old index, or. At random time when we see a query. Now in addition to these basic statistics, we'll also store all the documentsthat matched the news, and these entries are storedin the file called Postings. So in this case it matchedthree documents and we store information aboutthese three documents here. This is the document id,document 1 and the frequency is 1. The tf is one for news, in the seconddocument it's also 1, et cetera. So from this list, we can get allthe documents that match the term news and we can also know the frequencyof news in these documents. So, if the query has just one word,news, and we have easily look up to thistable to find the entry and go quicker into the postings to fetchall the documents that matching yours. So, let's take a look at another term. This time, let's take a lookat the word presidential. This would occur in only one document,document 3. So the document frequency is 1 butit occurred twice in this document. So the frequency count is two, andthe frequency count is used for some other reachable method wherewe might use the frequency to assess the popularity ofa term in the collection. Similarly we'll have a pointerto the postings here, and in this case,there is only one entry here because the term occurred in just one document andthat's here. The document id is 3 andit occurred twice. So this is the basicidea of inverted index. It's actually pretty simple, right? With this structure we can easily fetchall the documents that match a term. And this will be the basis forscoring documents for a query. Now sometimes we also want to storethe positions of these terms. So in many of these cases the termoccurred just once in the document. So there's only one position forexample in this case. But in this case, the term occurredtwice so there's two positions. Now the position information is veryuseful for the checking whether the matching of query terms isactually within a small window of, let's say, five words or ten words. Or, whether the matching of the two queryterms is, in fact, a phrase of two words. That this can all be checked quicklyby using the position from each. So, why is inverted index good forfast search? Well, we just talked about the possibilityof using the two answer single-term query. And that's very easy. What about the multiple term queries? Well let's first look at the somespecial cases of the Boolean query. A Boolean query is basicallya Boolean expression like this. So I want the value in the documentto match both term A and term B. So that's one conjunctive query. Or I want the web documentsto match term A or term B. That's a disjunctive query. But how can we answer sucha query by using inverted index? Well if you think a bit about it, it would be obvious becausewe have simply fetch all the documents that match term A and alsofetch all the documents that match term B. And then just take the intersectionto answer a query like A and B. Or to take the union toanswer the query A or B. So this is all very easy to answer. It's going to be very quick. Now what about the multi-termkeyword query? We talked about the vector space model forexample and we will do a match such query withdocument and generate the score. And the score is based onaggregated term weights. So in this case it's notthe Boolean query but the scoring can be actuallydone in similar way. Basically it's similar todisjunctive Boolean query. Basically, it's like A or B. We take the union of all the documentsthat match at least one query term and then we would aggregate the term weights. So this is a basic idea of using invertedindex for scoring documents in general. And we're going to talk aboutthis in more detail later. But for now, let's just look at the questionwhy is in both index, a good idea? Basically why is more efficient thansequentially just scanning documents. This is the obvious approach. You can just compute a score for eachdocument and then you can then sort them. And this is a straightforward method but this is going to be very slow imaginethe wealth, there's a lot of documents. If you do this then it will takea long time to answer your query. So the question now is why wouldthe invert index be much faster? Well it has to do is the worddistribution in text. So, here's some common phenomenaof word distribution in the text. There are some languages independentof patterns that seem to be stable. And these patterns are basicallycharacterized by the following pattern. A few words like the commonwords like the, a, or we occur very, very frequently in text. So they account fora large percent of occurrences of words. But most words would occur just rarely. There are many words that occur just once, let's say, in a document oronce in the collection. And there are many such. It's also true that the mostfrequent the words in one corpus they have to be rare in another. That means although the generalphenomenon is applicable, was observed in many cases thatexact words that are common may vary from context to context. So this phenomena is characterizedby what's called a Zipf's Law. This law says that the rank of a word multiplied by the frequency ofthe word is roughly constant. So formally if we use F(w)to denote the frequency, r(w) to denote the rank of a word. Then this is the formula. It basically says the same thing,just mathematical term. Where C is basically a constant andso, and there is also a parameter, alpha, that might be adjusted tobetter fit any empirical observations. So if I plot the wordfrequencies in sorted order, then you can see this more easily. The x axis is basically the word rank. This is r(w) andthe y axis is word frequency or F(w). Now this curve shows that the productof the two is roughly the constant. Now if you look at these words, we can seeThey can be separated into three groups. In the middle,it's the intermediary frequency words. These words tend to occurquite in a few documents, but they are not like thosemost frequent words. And they are also not very rare. So they tend to be often used in queries and they also tendto have high TF-IDF weights. These intermediate frequency words. But if you look at the leftpart of the curve, these are the highest frequency words. They are covered very frequently. They are usually words,like the, we, of Etc. Those words are very, very frequent andthey are in fact the two frequent to be discriminated, and they are generallynot very useful for retrieval. So they are often removed andthis is called the stop words removal. So you can use pretty much just the kindof words in the collection to kind of infer what words might be stop words. Those are basicallythe highest frequency words. And they also occupy a lot ofspace in the inverted index. You can imagine the posting entries forsuch a word would be very long. And then therefore, if you can remove such words you can savea lot of space in the inverted index. We also show the tail part,which has a lot of rare words. Those words don't occur very frequently,and there are many such words. Those words are actually very useful for search also, if a user happens tobe interested in such a topic. But because they're rare,it's often true that users aren't necessarilyinterested in those words. But retain them would allow us tomatch such a document accurately. They generally have very high IDF. So what kind of data structures shouldwe use to store inverted index? Well, it has two parts, right. If you recall, we have a dictionary andwe also have postings. The dictionary has modest size, althoughfor the web it's still going to be very large but compare it withpostings it's more distinct. And we also need to have fastrandom access to the entries because we're going to look upon the query term very quickly. So therefore, we'd prefer to keep sucha dictionary in memory if it's possible. If the collection is not very large,this is feasible, but if the collection is very largethen it's in general not possible. If the vocabulary size is very large,obviously we can't do that. So, in general that's how it goes. So the data structuresthat we often use for storing dictionary,it would be direct access. There are structures like hash table, or b-tree if we can't storeeverything in memory or use disk. And then try to build a structure thatwould allow it to quickly look up entries. For postings they are huge. And in general, we don't have to havedirect access to a specific entry. We generally would just look upa sequence of document IDs and frequencies for all the documentsthat matches the query term. So would read those entries sequentially. And therefore because it's large andwe generally have store postings on disc, they have to stay on disc and they wouldcontain information such as document IDs, term frequency orterm positions, etcetera. Now because they are very large,compression is often desirable. Now this is not only to save disc space,and this is of course one benefit of compression, it It'snot going to occupy that much space. But it's also to help improving speed. Can you see why? Well, we know that input andoutput would cost a lot of time. In comparison with the time taken by CPU. So, CPU is much faster butIO takes time and so by compressing the inverter index,opposing files will become smaller, and the entries, that we have the readings,and memory to process a query term, would be smaller, andthen, so we can reduce the amount of tracking IO andthat can save a lot of time. Of course, we have to then do moreprocessing of the data when we uncompress the data in the memory. But as I said CPU is fast. So over all we can still save time. So compression here is bothto save disc space and to speed up the loading of the index. [MUSIC]
[SOUND] So average precision is computer for just one. one query.But we generally experiment with many different queries and this is toavoid the variance across queries. Depending on the queries you use youmight make different conclusions. Right, soit's better then using more queries. If you use more queries then,you will also have to take the average of the averageprecision over all these queries. So how can we do that? Well, you can naturally. Think of just doing arithmetic mean as we always tend to, to think in, in this way. So, this would give us what's calleda "Mean Average Position", or MAP. In this case, we take arithmetic mean of all the averageprecisions over several queries or topics. But as I just mentioned inanother lecture, is this good? We call that. We talked about the different waysof combining precision and recall. And we conclude that the arithmeticmean is not as good as the MAP measure. But here it's the same. We can also think about the alternativeways of aggregating the numbers. Don't just automatically assume that,though. Let's just also take the arithmeticmean of the average position over these queries. Let's think about what'sthe best way of aggregating them. If you think about the different ways,naturally you will, probably be able to think aboutanother way, which is geometric mean. And we call this kind of average a gMAP. This is another way. So now, once you think aboutthe two different ways. Of doing the same thing. The natural question to ask is,which one is better? So. So, do you use MAP or gMAP? Again, that's important question. Imagine you are againtesting a new algorithm in, by comparing the ways your oldalgorithms made the search engine. Now you tested multiple topics. Now you've got the average precision forthese topics. Now you are thinking of lookingat the overall performance. You have to take the average. But which, which strategy would you use? Now first, you should also think about thequestion, well did it make a difference? Can you think of scenarios where usingone of them would make a difference? That is they would give differentrankings of those methods. And that also means depending onthe way you average or detect the. Average of these average positions. You will get different conclusions. This makes the questionbecoming even more important. Right?So, which one would you use? Well again, if you look atthe difference between these. Different ways of aggregatingthe average position. You'll realize in arithmetic mean,the sum is dominating by large values. So what does large value here mean? It means the query is relatively easy. You can have a high pres,average position. Whereas gMAP tends to beaffected more by low values. And those are the queries thatdon't have good performance. The average precision is low. So if you think about the,improving the search engine for those difficult queries,then gMAP would be preferred, right? On the other hand, if you just want to. Have improved a lot. Over all the kinds of queries orparticular popular queries that might be easy and you want to make the perfect andmaybe MAP would be then preferred. So again, the answer depends onyour users, your users tasks and their pref, their preferences. So the point that here is to thinkabout the multiple ways to solve the same problem, and then compare them,and think carefully about the differences. And which one makes more sense. Often, when one of them mightmake sense in one situation and another might make more sensein a different situation. So it's important to pick out underwhat situations one is preferred. As a special case of the mean averageposition, we can also think about the case where there was preciselyone rank in the document. And this happens often, for example,in what's called a known item search. Where you know a target page, let'ssay you have to find Amazon, homepage. You have one relevant document there,and you hope to find it. That's call a "known item search". In that case,there's precisely one relevant document. Or in another application,like a question and answering, maybe there's only one answer. Are there. So if you rank the answers, then your goal is to rank that oneparticular answer on top, right? So in this case, you can easilyverify the average position, will basically boil downto reciprocal rank. That is, 1 over r where r is the rankposition of that single relevant document. So if that document is rankedon the very top or is 1, and then it's 1 for reciprocal rank. If it's ranked at the,the second, then it's 1 over 2. Et cetera. And then we can also take a, a averageof all these average precision or reciprocal rank over a set of topics, and that would give us somethingcalled a mean reciprocal rank. It's a very popular measure. For no item search or, you know, an problem where you havejust one relevant item. Now again here, you can see thisr actually is meaningful here. And this r is basicallyindicating how much effort a user would have to make in orderto find that relevant document. If it's ranked on the top it's low effortthat you have to make, or little effort. But if it's ranked at 100then you actually have to, read presumably 100 documentsin order to find it. So, in this sense r is also a meaningfulmeasure and the reciprocal rank will take the reciprocal of r,instead of using r directly. So my natural question hereis why not simply using r? I imagine if you were to designa ratio to, measure the performance of a random system,when there is only one relevant item. You might have thought aboutusing r directly as the measure. After all,that measures the user's effort, right? But, think about if you take a averageof this over a large number of topics. Again it would make a difference. Right, for one single topic, using r or using 1 over r wouldn'tmake any difference. It's the same. Larger r with correspondsto a small 1 over r, right? But the difference would only show when,show up when you have many topics. So again, think about the average of MeanReciprocal Rank versus average of just r. What's the difference? Do you see any difference? And would, would this differencechange the oath of systems. In our conclusion. And this, it turns out that,there is actually a big difference, and if you think about it, if you want tothink about it and then, yourself, then pause the video. Basically, the difference is,if you take some of our directory, then. Again it will be dominatedby large values of r. So what are those values? Those are basically large values thatindicate that lower ranked results. That means the relevant itemsrank very low down on the list. And the sum that's also the averagethat would then be dominated by. Where those relevant documentsare ranked in, in ,in, in the lower portion of the ranked. But from a users perspective we caremore about the highly ranked documents. So by taking this transformationby using reciprocal rank. Here we emphasize more onthe difference on the top. You know, think aboutthe difference between 1 and the 2, it would make a big difference, in 1 overr, but think about the 100, and 1, and where and when won't make muchdifference if you use this. But if you use this there willbe a big difference in 100 and let's say 1,000, right. So this is not the desirable. On the other hand, a 1 and2 won't make much difference. So this is yet another case where theremay be multiple choices of doing the same thing and then you need to figureout which one makes more sense. So to summarize,we showed that the precision-recall curve. Can characterize the overallaccuracy of a ranked list. And we emphasized that the actualutility of a ranked list depends on how many top ranked resultsa user would actually examine. Some users will examine more. Than others. An average person uses a standard measurefor comparing two ranking methods. It combines precision and recall and it's sensitive to the rankof every random document. [MUSIC]
[SOUND]This lecture is about smoothingof language models. In this lecture, we're going to continue talking aboutthe probabilistic retrieval model. In particular,we're going to talk about the smoothing of language model in the querylikelihood retrieval method. So you have seen this slidefrom a previous lecture. This is the ranking functionbased on the query likelihood. Here, we assume that the independence ofgenerating each query word And the formula would look like the following wherewe take a sum of all the query words. And inside the sum there is a logof probability of a word given by the document or document image model. So the main task now is to estimate this document language model as wesaid before different methods for estimating this model would leadto different retrieval functions. So in this lecture, we're going tobe looking to this in more detail. So how do we estimate this language model? Well the obvious choice would bethe maximum likelihood estimate that we have seen before. And that is we're going to normalizethe word frequencies in the document. And estimate the probabilityit would look like this. This is a step function here. Which means all of the words that have the same frequency count willhave identical problem with it. This is another freedom to count,that has different probability. Note that for words that have notoccurred in the document here they will have 0 probability. So we know this is just like the modelthat we assume earlier in the lecture. Where we assume that the use ofthe simple word from the document to a formula to clear it. And there's no chance of assembling anyword that's not in the document and we know that's not good. So how do we improve this? Well in order to assigna none 0 probability to words that have not been observed inthe document, we would have to take away some probability mass from the wordsthat are observed in the document. So for example here, we have to take awaysome probability of the mass because we need some extra probability mass forthe words otherwise they won't sum to 1. So all these probabilities must sum to 1. So to make this transformation and toimprove the maximum likelihood estimated by assigning non zero probabilities towords that are not observed in the data. We have to do smoothing andsmoothing has to do with improving the estimate by consideringthe possibility that if the author had been asking to write more words for the document,the author might have written other words. If you think about this factorthen the a smoothed language model would be a more accurate thanthe representation of the actual topic. Imagine you have seen an abstractof a research article. Let's say this document is abstract. If we assume and see words in thisabstract that we have a probability of 0. That would mean there'sno chance of sampling a word outside the abstractof the formulated query. But imagine a user who is interestedin the topic of this subject. The user might actuallychoose a word that's not in that chapter to use as query. So obviously,if we has asked this author to write more author would have writtena full text of the article. So smoothing of the languagemodel is an attempt to try to recover the model forthe whole article. And then of course,we don't have knowledge about any words that are notobserved in the abstract. So that's why smoothing isactually a tricky problem. So let's talk a little more abouthow to smooth a language model. The key question here is, what probabilityshould be assigned to those unseen words? And there are many differentways of doing that. One idea here, that's very useful forretrieval is let the probability of unseen word be proportional to its probabilitygiven by a reference language model. That means if you don't observethe word in the dataset. We're going to assume that itsprobability is kind of governed by another reference languagemodel that we will construct. It will tell us which unseen wordswould have a higher probability. In the case of retrieval,a natural choice would be to take the collection language modelas the reference language model. That is to say, if you don'tobserve a word in the document, we're going to assume thatthe probability of this word would be proportional to the probabilityof word in the whole collection. So more formally, we'll be estimating the probabilityof a word key document as follows. If the word is seen inthe document then the probability would be this counted the maximumlikelihood estimate P sub c here. Otherwise, if the word is not seen in thedocument we're going to let probability be proportional to the probabilityof the word in the collection. And here the coefficient that offer is to control the amount of probabilitymass that we assign to unseen words. Obviously, all theseprobabilities must sum to 1, so alpha sub d is constrained in some way. So what if we plug in thissmoothing formula into our query likelihood ranking function? This is what we will get. In this formula, we have this as a sum over all the query words and those that we have written here as the sumof all the vocabulary, you see here. This is the sum of allthe words in the vocabulary, but not that we have a countof the word in the query. So in fact, we are just takinga sample of query words. This is now a commonway that we would use, because of its conveniencein some transformations. So this is as I said,this is sum of all the query words. In our smoothing method,we assume that the words that are not observed in the method would havea somewhat different form of probability. Name it's four, this foru. So we're going to do then,decompose the sum into two parts. One sum is over all the query wordsthat are matching the document. That means that in this sum, all the words have a non zero probabilityin the document. Sorry, it's the non zero countof the word in the document. They all occur in the document. And they also have to of coursehave a non zero count in the query. So these are the query wordsthat are matching the document. On the other hand, in this sum weare taking a sum of all the words that are not all query wasnot matching the document. So they occur in the querydue to this term, but they don't occur in the document. In this case, these words have this probability becauseof our assumption about the smoothing. That here, these seen wordshave a different probability. Now, we can go further byrewriting the second sum as a difference of two other sums. Basically, the first sum isthe sum of all the query words. Now, we know that the original sumis not over all the query words. This is over all the query words thatare not matched in the document. So here we pretend that theyare actually over all the query words. So we take a sum over all the query words. Obviously, this sum has extraterms that are not in this sum. Because, here we're takingsum over all the query words. There, it's not matched in the document. So in order to make them equal, we willhave to then subtract another sum here. And this is the sum over all the querywords that are matching in the document. And this makes sense, because herewe are considering all query words. And then we subtract the querythat was matched in the document. That would give us the query thatwas not matched in the document. And this is almost a reverseprocess of the first step here. And you might wonder whydo we want to do that. Well, that's because if we do this, then we have different formsof terms inside of these sums. So now, you can see in this sumwe have all the words matched, the query was matching the documentwith this kind of term. Here we have another sum over the same setof terms, matched query terms in document. But inside the sum, it's different. But these two sums can clearly be merged. So if we do that, we'll get another form of the formula that looks likebefore me at the bottom here. And note that this isa very interesting formula. Because here we combinethese two that all or some of the query words matching inthe document in the one sum here. And the other sum now isdecomposing into two parts. And these two partslook much simpler just, because these are the probabilitiesof unseen words. This formula is very interestingbecause you can see the sum is now over the match the query terms. And just like in the vector space model,we take a sum of terms that are in the intersection ofquery vector and the document vector. So it already looks a little bitlike the vector space model. In fact, there's even more similarityhere as we explain on this slide. [MUSIC]
This lecture is about Web Search. In this lecture,we're going to talk about one of the most important applications oftext retrieval, web search engines. So let's first look at somegeneral challenges and opportunities in web search. Now, many informationalretrieval algorithms had been developedbefore the web was born. So when the web was born,it created the best opportunity to apply those algorithms to major applicationproblem that everyone would care about. So naturally, there have to be somefurther extensions of the classical search algorithms to address newchallenges encountered in web search. So here are some general challenges. First, this is a scalability challenge. How to handle the size of the web and ensure completeness ofcoverage of all information. How to serve many users quickly andby answering all their queries. And so that's one major challenge and before the web was born the scalesearch was relatively small. The second problem is that there'sno quality information and there are often spams. The third challenge isDynamics of the Web. The new pages are constantly create andsome pages may be updated very quickly, so it makes it harder tokeep it indexed fresh. So these are some of the challengesthat we have to solve in order to deal with high quality web searching. On the other hand there are also someinteresting opportunities that we can leverage to include the search results. There are many additional heuristics,for example, using links that we canleverage to improve scoring. Now everything that we talked aboutsuch as the vector space model are general algorithms. They can be applied to any searchapplications, so that's the advantage. On the other hand, they also don't takeadvantage of special characteristics of pages or documents in the specificapplications, such as web search. Web pages are linked with each other,so obviously, the linking is somethingthat we can also leverage. So, because of these challenges andopportunities and there are new techniques that have been developed forweb search or due to need for web search. One is parallel indexing and searching and this is to addressthe issue of scalability. In particular, Google's imaging ofmap reduce is very influential and has been very helpful in that aspect. Second, there are techniquesthat are developing for addressing the problem of spams,so spam detection. We'll have to prevent those spampages from being ranked high. And there are also techniquesto achieve robust ranking. And we're going to use a lotof signals to rank pages, so that it's not easy to spam the searchengine with a particular trick. And the third line of techniques is link analysis and these are techniques that can allow us to improve such resultsby leveraging extra information. And in general in web searching,we're going to use multiple features for ranking not just for link analysis. But also exploring all kindsof crawls like the layout or anchor text that describesa link to another page. So, here's a picture showingthe basic search engine technologies. Basically, this is the web on the left andthen user on the right side and we're going to help this user to getthe access for the web information. And the first component is a Crawler thatwould crawl pages and then the second component is Indexer that would takethese pages create the inverted index. The third component there is a Retrieverand that would use inverted index to answer user's query by talkingto the user's browser. And then the search results will be givento the user and when the browser would show those results, it allowsthe user to interact with the web. So, we're going to talk abouteach of these components. First of all, we're going to talk aboutthe crawler, also called a spider or software robot that would do somethinglike crawling pages on the web. To build a toy crawler is relatively easy, because you just need to startwith a set of seed pages. And then fetch pages from the web andparse these pages and figure out new links. And then add them to the priority que andthen just explore those additional links. But to be able to real crawleractually is tricky and there are some complicated issuesthat we have to deal with. For example robustness,what if the server doesn't respond, what if there's a trap that generatesdynamically generated webpages that might attract your crawler tokeep crawling on the same side and to fetch dynamic generated pages? The results of this issueof crawling courtesy and you don't want to overload one particularserver with many crawling requests and you have to respect the robotexclusion protocol. You also need to handle differenttypes of files, there are images, PDF files,all kinds of formats on the web. And you have to alsoconsider URL extension, so sometimes those are CGI scripts andthere are internal references, etc, and sometimes you haveJavaScripts on the page and they also create challenges. And you ideally should also recognizeredundant pages because you don't have to duplicate those pages. And finally, you may be interestedin the discover hidden URLs. Those are URLs that may not be linkedto any page, but if you truncate the URL to a shorter path, you mightbe able to get some additional pages. So what are the Major Crawling Strategies? In general, Breadth-First is most common becauseit naturally balances the sever load. You would not keep probing a particularserver with many requests. Also parallel crawling is verynatural because this task is very easy to parallelize. And there is some variationsof the crawling task, and one interesting variationis called a focused crawling. In this case, we're going to crawl justsome pages about a particular topic. For example,all pages about automobiles, all right. And this is typically going tostart with a query, and then you can use the query to get someresults from a major search engine. And then you can start it with thoseresults and then gradually crawl more. The one channel in crawling, is you will find the newchannels that people created and people probably are creatingnew pages all the time. And this is very challenging ifthe new pages have not been actually linked to any old pages. If they are, then you can probably findthem by re-crawling the old pages, so these are also some interestingchallenges that have to be solved. And finally, we might face the scenarioof incremental crawling or repeated crawling, right. Let's say,if you want to build a web search engine, and you first crawl a lotof data from the web. But then,once you have cracked all the data, in the future you just needto crawl the updated pages. In general, you don't have tore-crawl everything, right? It's not necessary. So in this case, your goal is tominimize the resource overhead by using minimum resourcesto just the update pages. So, this is actually a veryinteresting research question here, and this is a open research question,in that there aren't many standard algorithms established yetfor doing this task. But in general, you can imagine,you can learn, from the past experience. So the two major factors thatyou have to consider are, first will this pagebe updated frequently? And do I have to quote this page again? If the page is a static page andthat hasn't being changed for months, you probably don't have to re-crawl iteveryday because it's unlikely that it will changed frequently. On the other hand, if it's a sports scorepage that gets updated very frequently and you may need to re-crawl it andmaybe even multiple times on the same day. The other factor to consider is,is this page frequently accessed by users? If it is, then it means thatit is a high utility page and then thus it's more important toensure such a page to refresh. Compared with another page that hasnever been fetched by any users for a year, then even though thatpage has been changed a lot then. It's probably not that necessary tocrawl that page or at least it's not as urgent as to maintain the freshnessof frequently accessed page by users. So to summarize, web search is one ofthe most important applications of text retrieval and there are some newchallenges particularly scalability, efficiency, quality information. There are also new opportunitiesparticularly rich link information and layout, etc. A crawler is an essential componentof web search applications and in general, you can find two scenarios. One is initial crawling andhere we want to have complete crawling of the web if you are doinga general search engine or focused crawling if you want to justtarget as a certain type of pages. And then, there is another scenario that'sincremental updating of the crawl data or incremental crawling. In this case,you need to optimize the resource, try to use minimum resourceto get the [INAUDIBLE] [MUSIC]
[SOUND]. This lecture is aboutthe future of web search. In this lecture, we're going to talkabout some possible future trends of web search and intelligent informationretrieval systems in general. In order to further improvethe accuracy of a search engine, it's important that to considerspecial cases of information need. So one particular trend could be tohave more and more specialized than customized search engines, and theycan be called vertical search engines. These vertical search engines can beexpected to be more effective than the current general search enginesbecause they could assume that users are a special group of users thatmight have a common information need, and then the search engine can becustomized with this ser, so, such users. And because of the customization,it's also possible to do personalization. So the search can be personalized, because we have a betterunderstanding of the users. Because of the restrictions with domain,we also have some advantages in handling the documents, because we canhave better understanding of documents. For example, particular words maynot be ambiguous in such a domain. So we can bypass the problem of ambiguity. Another trend we can expect to see, is the search engine willbe able to learn over time. It's like a lifetime learning orlifelong learning, and this is, of course, very attractive because that means thesearch engine will self-improve itself. As more people are using it, the searchengine will become better and better, and this is already happening, because the search engines can learnfrom the [INAUDIBLE] of feedback. More users use it, and the qualityof the search engine allows for the popular queries that are typed in bymany users allow it to become better, so this is sort of anotherfeature that we will see. The third trend might beto the integration of bottles of information access. So search, navigation, andrecommendation or filtering might be combined to form a full-fledgedinformation management system. And in the beginning of this course,we talked about push versus pull. These are different modes of informationaccess, but these modes can be combined. And similarly, in the pull mode, queryingand the browsing could also be combined. And in fact we're doing that basically,today, is the [INAUDIBLE] search endings. We are querying, sometimes browsing,clicking on links. Sometimes we've got someinformation recommended. Although most of the cases the informationrecommended is because of advertising. But in the future, you can imagineseamlessly integrate the system with multi-mode for information access, andthat would be convenient for people. Another trend is that we might see systems that try to go beyond the searchesto support the user tasks. After all, the reason why people wantto search is to solve a problem or to make a decision or perform a task. For example consumers might search for opinions about products inorder to purchase a product, choose a good product by, soin this case it would be beneficial to support the whole workflow of purchasinga product, or choosing a product. In this era, after the common searchengines already provide a good support. For example, you can sometimes look at thereviews, and then if you want to buy it, you can just click on the button to go theshopping site and directly get it done. But it does not provide a,a good task support for many other tasks. For example, for researchers, you might want to find the realm inthe literature or site of the literature. And then, there's no, not much support forfinishing a task such as writing a paper. So, in general, I think,there are many opportunities in the wait. So in the following few slides, I'llbe talking a little bit more about some specific ideas or thoughts that hopefully, can help you in imagining newapplication possibilities. Some of them might be already relevantto what you are currently working on. In general, we can think about anyintelligent system, especially intelligent information system, as we specifiedby these these three nodes. And soif we connect these three into a triangle, then we'll able to specifyan information system. And I call thisData-User-Service Triangle. So basically the three questions youask would be who are you serving and what kind of data are you are managing andwhat kind of service you provide. Right there, this would help usbasically specify in your system. And there are many different waysto connect them depending on how you connect them,you will have a different kind of systems. So let me give you some examples. On the top,you can see different kinds of users. On the left side, you can see differenttypes of data or information, and on the bottom,you can see different service functions. Now imagine you can connectall these in different ways. So, for example, you can connecteveryone with web pages, and the support search andbrowsing, what do you get? Well, that's web search, right? What if we connect UIUC employees withorganization documents or enterprise documents to support the search andbrowsing, but that's enterprise search. If you connect the scientistwith literature information to provide all kinds of service,including search, browsing, or alert of new random documents ormining analyzing research trends, or provide the task with support ordecision support. For example, we might be,might be able to provide a support for automatically generatingrelated work section for a research paper, andthis would be closer to task support. Right?So then we can imagine this wouldbe a literature assistant. If we connect the online shopperswith blog articles or product reviews then we can help these peopleto improve shopping experience. So we can provide, for example data miningcapabilities to analyze the reviews, to compare products, compare sentiment ofproducts and to provide task support or decision support to have themchoose what product to buy. Or we can connect customer servicepeople with emails from the customers, and, and we can imagine a systemthat can provide a analysis of these emails to find that the majorcomplaints of the customers. We can imagine a system wecould provide task support by automatically generatinga response to a customer email. Maybe intelligently attachalso a promotion message if appropriate, if they detect that that'sa positive message, not a complaint, and then you might take this opportunityto attach some promotion information. Whereas if it's a complaint,then you might be able to automatically generate somegeneric response first and tell the customer that he or she canexpect a detailed response later, etc. All of these are trying to helppeople to improve the productivity. So this shows thatthe opportunities are really a lot. It's just only restrictedby our imagination. So this picture shows the trendof the technology, and also, it characterizes the, intelligentinformation system in three angles. You can see in the center, there'sa triangle that connects keyword queries to search a bag of words representation. That means the current search enginesbasically provides search support to users and mostly modelusers based on keyword queries and sees the data throughbag of words representation. So it's a very simple approximation ofthe actual information in the documents. But that's what the current system does. It connects these three nodesin such a simple way, or it only provides a basic search functionand doesn't really understand the user, and it doesn't really understand thatmuch information in the documents. Now, I showed some trends to push eachnode toward a more advanced function. So think about the user node here, right? So we can go beyond the keyword queries,look at the user search history, and then further model the usercompletely to understand the, the user's task environment,task need context or other information. Okay, so this is pushing forpersonalization and complete user model. And this is a majordirection in research in, in order to build intelligentinformation systems. On the document side,we can also see, we can go beyond bag of words implementationto have entity relation representation. This means we'll recognize people's names,their relations, locations, etc. And this is already feasible withtoday's natural processing technique. And Google is the reasonthe initiative on the knowledge graph. If you haven't heard of it,it is a good step toward this direction. And once we can get to that level withoutinitiating robust manner at larger scale, it can enable the search engineto provide a much better service. In the future we would like to have knowledge representation where wecan add perhaps inference rules, and then the search engine wouldbecome more intelligent. So this calls forlarge-scale semantic analysis, and perhaps this is more feasible forvertical search engines. It's easier to make progressin the particular domain. Now on the service side, we see we need to go beyond the search ofsupport information access in general. So search is only one way to get accessto information as well recommender systems and push and pull so differentways to get access to random information. But going beyond access, we also need to help people digest theinformation once the information is found, and this step has to do with analysisof information or data mining. We have to find patterns orconvert the text information into real knowledge that canbe used in application or actionable knowledge that can be used fordecision making. And furthermore the knowledgewill be used to help a user to improve productivity in finishing a task,for example, a decision-making task. Right, so this is a trend. And, and, and so basically,in this dimension, we anticipate in the future intelligent informationsystems will provide intelligent and interactive task support. Now I should also emphasize interactivehere, because it's important to optimize the combined intelligence of the users andthe system. So we, we can get some helpfrom users in some natural way. And we don't have to assume the systemhas to do everything when the human, user, and the machine can collaborate inan intelligent way, an efficient way, then the combined intelligencewill be high and in general, we can minimize the user's overalleffort in solving problem. So this is the big picture of futureintelligent information systems, and this hopefully can provideus with some insights about how to make further innovationson top of what we handled today. [MUSIC]
[SOUND]This lecture is about how to use generative probabilisticmodels for text categorization. There are in general about two kindsof approaches to text categorization by using machine learning. One is by generating probabilistic models. The other is discriminative approaches. In this lecture, we're going totalk about the generative models. In the next lecture, we're going totalk about discriminative approaches. So the problem of text categorization is actually a very similarto document clustering. In that, we'll assume that each documentit belongs to one category or one cluster. The main difference is that inclustering we don't really know what are the predefined categories are,what are the clusters. In fact,that's the goal of text clustering. We want to find such clusters in the data. But in the case of categorization,we are given the categories. So we kind of havepre-defined categories and then based on these categories andtraining data, we would like to allocate a document to one of these categories orsometimes multiple categories. But because of the similarityof the two problems, we can actually get the documentclustering models for text categorization. And we understand how we canuse generated models to do text categorization fromthe perspective of clustering. And so, this is a slide that we've talkedabout before, about text clustering, where we assume there are multiple topicsrepresented by word distributions. Each topic is one cluster. So once we estimated such a model, we faced a problem of deciding whichcluster document d should belong to. And this question boils down to decidewhich theta i has been used to generate d. Now, suppose d has L wordsrepresented as xi here. Now, how can you computethe probability that a particular topic word distribution zeta i hasbeen used to generate this document? Well, in general, we use basewall to make this influence and you can see this prior information herethat we need to consider if a topic or cluster has a higher priorthen it's more likely that the document hasbeen from this cluster. And so, we should favor such a cluster. The other is a likelihood part,it's this part. And this has to do with whetherthe topic word of distribution can explain the contentof this document well. And we want to pick a topicthat's high by both values. So more specifically,we just multiply them together and then choose which topichas the highest product. So more rigorously,this is what we'd be doing. So we're going to choosethe topic that would maximize. This posterior probability at the topof a given document gets posterior because this one,p of the i, is the prior. That's our belief aboutwhich topic is more likely, before we observe any document. But this conditional probability here is the posterior probability of the topicafter we have observed the document of d. And base wall allows us to update thisprobability based on the prior and I have shown the details,below here you can see how the prior here is related tothe posterior, on the left-hand side. And this is related to howwell this word distribution explains the document here, andthe two are related in this way. So to find the topic that has the higher posterior probability here it'sequivalent to maximize this product as we have seen also,multiple times in this course. And we can then change the probabilityof document in your product of the probability of each word, andthat's just because we've made an assumption about independencein generating each word. So this is just something that youhave seen in document clustering. And we now can see clearly how wecan assign a document to a category based on the informationabout word distributions for these categories andthe prior on these categories. So this idea can be directlyadapted to do categorization. And this is precisely whata Naive Bayes Classifier is doing. So here it's most reallythe same information except that we're looking atthe categorization problem now. So we assume that if theta i represents category i accurately,that means the word distribution characterizes the content ofdocuments in category i accurately. Then, what we can do is preciselylike what we did for text clustering. Namely we're going to assigndocument d to the category that has the highest probabilityof generating this document. In other words, we're going to maximizethis posterior probability as well. And this is related to the prior and the [INAUDIBLE] as you haveseen on the previous slide. And so, naturally we can decompose this [INAUDIBLE] intoa product as you see here. Now, here, I change the notation sothat we will write down the product as product of all the wordsin the vocabulary, and even though the documentdoesn't contain all the words. And the product is still accuratelyrepresenting the product of all the words in the documentbecause of this count here. When a word,it doesn't occur in the document. The count would be 0, sothis time will just disappear. So if actively we'll just have the productover other words in the document. So basically, with Naive Bayes Classifier, we're going to score each category forthe document by this function. Now, you may notice that here it involvesa product of a lot of small probabilities. And this can cause and the four problem. So one way to solve the problem isthru take logarithm of this function, which it doesn't changes allthe often these categories. But will helps us preserve precision. And so, this is often the functionthat we actually use to score each category andthen we're going to choose the category that has the highestscore by this function. So this is called an Naive BayesClassifier, now the keyword base is understandable because we are applyinga base rule here when we go from the posterior probability of the topic toa product of the likelihood and the prior. Now, it's also called a naive becausewe've made an assumption that every word in the document is generatedindependently, and this is indeed a naive assumption because in reality they'renot generating independently. Once you see some word,then other words will more likely occur. For example,if you have seen a word like a text. Than that mixed category, they see more clustering more likely toappear than if you have not the same text. But this assumption allowsus to simplify the problem. And it's actually quite effective formany text categorization tasks. But you should know thatthis kind of model doesn't have to make this assumption. We could for example, assume thatwords may be dependent on each other. So that would make it a bigram analogymodel or a trigram analogy model. And of course you can even use a mixturemodel to model what the document looks like in each category. So in nature, they will be all usingbase rule to do classification. But the actual generating model fordocuments in each category can vary. And here, we just talk about verysimple case perhaps, the simplest case. So now the question is,how can we make sure theta i actually represents category i accurately? Now in clustering,we learned that this category i or what are the distributions forcategory i from the data. But in our case,what can we do to make sure this theta i represents indeed category i. Well if you think about the question, and you likely come up with the ideaof using the training data. Indeed in the textbook, we typically assume that thereis training data available and those are the documents that unknownto have generator from which category. In other words, these are the documentswith known categories assigned and of course human experts must do that. In here, you see that T1represents the set of documents that are known to havethe generator from category 1. And T2 represents the documentsthat are known to have been generated from category 2, etc. Now if you look at this picture,you'll see that the model here is really a simplifiedunigram language model. It's no longer mixed modal, why? Because we already know which distributionhas been used to generate which documents. There's no uncertainty here, there'sno mixing of different categories here. So the estimation problem ofcourse would be simplified. But in general,you can imagine what we want to do is estimate these probabilitiesthat I marked here. And what other probability is that we haveto estimate it in order to do relation. Well there are two kinds. So one is the prior,the probability of theta i and this indicates how populareach category is or how likely will it have observedthe document in that category. The other kind isthe water distributions and we want to know what words have highprobabilities for each category. So the idea then is to justuse observe the training data to estimate these two probabilities. And in general, we can do thisseparately for the different categories. That's just because these documentsare known to be generated from a specific category. So once we know that, it's in some sense irrelevant of whatother categories we are also dealing with. So now this is a statisticalestimation problem. We have observed somedata from some model and we want to guessthe parameters of this model. We want to take our bestguess of the parameters. And this is a problem that we have seenalso several times in this course. Now, if you haven't thoughtabout this problem, haven't seen life based classifier. It would be very useful foryou to pause the video for a moment and to think about how to solve this problem. So let me state the problem again. So let's just think about with category 1, we know there is one word of distributionthat has been used to generate documents. And we generate each word in the documentindependently, and we know that we have observed a set of n sub 1documents in the set of Q1. These documents have been allgenerated from category 1. Namely have been all generatedusing this same word distribution. Now the question is,what would be your guess or estimate of the probability ofeach word in this distribution? And what would be your guess ofthe entire probability of this category? Of course,this singular probability depends on how likely are you to seedocuments in other categories? So think for a moment, how do youuse all this training data including all these documents that are knownto be in these k categories, to estimate all these parameters? Now, if you spend some timeto think about this and it would help you understandthe following few slides. So do spend some time to make sure thatyou can try to solve this problem, or do you best to solve the problem yourself. Now if you have thought about andthen you will realize the following to it. First, what's the bases for estimating theprior or the probability of each category. Well this has to do with whether youhave observed a lot of documents form that category. Intuitively, you have seen a lotof documents in sports and very few in medical science. Then you guess is that the probabilityof the sports category is larger or your prior on the categorywould be larger. And what about the basis for estimatingthe probability of where each category? Well the same, and you'll be justassuming that words that are observed frequently in the documents that are knownto be generated from a category will likely have a higher probability. And that's just a maximumNaive Bayes made of. Indeed, that's what we can do, so thismade the probability of which category and to answer the question,which category is most popular? Then we can simply normalize,the count of documents in each category. So here you see N sub i denotesthe number of documents in each category. And we simply just normalize thesecounts to make this a probability. In other words, we make thisprobability proportional to the size of training intercept in each categorythat's a size of the set t sub i. Now what about the word distribution? Well, we do the same. Again this time we can do this foreach category. So let's say,we're considering category i or theta i. So which word has a higher probability? Well, we simply count the word occurrences in the documents that are knownto be generated from theta i. And then we put together allthe counts of the same word in the set. And then we just normalize thesecounts to make this distribution of all the words make allthe probabilities off these words to 1. So in this case, you're going to see thisis a proportional through the count of the word in the collection oftraining documents T sub i and that's denoted by c of w and T sub i. Now, you may notice that weoften write down probable estimate in the form of beingproportional for certain numbers. And this is often sufficient, because we have some constraintson these distributions. So the normalizer isdictated by the constraint. So in this case, it will be useful foryou to think about what are the constraints on thesetwo kinds of probabilities? So once you figure outthe answer to this question, and you will know how tonormalize these accounts. And so this is a good exercise towork on if it's not obvious to you. There is another issue inNaive Bayes which is a smoothing. In fact the smoothing is a general problemin older estimate of language morals. And this has to do with, what would happen if you haveobserved a small amount of data? So smoothing is an important techniqueto address that outsmarts this. In our case, the training data can besmall and when the data set is small when we use maximum likely estimator we oftenface the problem of zero probability. That means if an event is not observed then the estimatedprobability would be zero. In this case, if we have not seena word in the training documents for let's say, category i. Then our estimator would be zero for theprobability of this one in this category, and this is generally not accurate. So we have to do smoothing to makesure it's not zero probability. The other reason for smoothing is thatthis is a way to bring prior knowledge, and this is also generally true fora lot of situations of smoothing. When the data set is small, we tend to rely on some priorknowledge to solve the problem. So in this case our [INAUDIBLE] says thatno word should have zero probability. So smoothing allows us to injectthese to prior initial that no order has a real zero probability. There is also a third reason whichus sometimes not very obvious, but we explain that in a moment. And that is to help achievediscriminative weighting of terms. And this is also called IDF weighting, inverse document frequency weighting thatyou have seen in mining word relations. So how do we do smoothing? Well in general we add pseudocounts to these events, we'll make sure that no event has 0 count. So one possible way of smoothingthe probability of the category is to simply add a small non activeconstant delta to the count. Let's pretend that every categoryhas actually some extra number of documents represented by delta. And in the denominator we also adda k multiplied by delta because we want the probability to some to 1. So in total we've added delta k timesbecause we have a k categories. Therefore in this sum,we have to also add k multiply by delta as a total pseudocountthat we add up to the estimate. Now, it's interesting to thinkabout the influence of that data, obvious data is a smoothingparameter here. Meaning that the larger data is andthe more we will do smoothing and that means we'll morerely on pseudocounts. And we might indeed ignore the actualcounts if they are delta is set to infinity. Imagine what would happen if thereare approaches positively to infinity? Well, we are going to say every categoryhas an infinite amount of documents. And then there's no distinction to them soit become just a uniform. What if delta is 0? Well, we just go back to the originalestimate based on the observed training data to estimate to estimatethe probability of each category. Now we can do the same forthe word distribution. But in this case,sometimes we find it useful to use a nonuniform seudocount forthe word. So here you'll see we'll adda pseudocounts to each word and that's mule multipliedby the probability of the word given by a backgroundlanguage model, theta sub b. Now that background model ingeneral can be estimated by using a logic collection of tests. Or in this case we will use the wholeset of all the training data to estimate this background language model. But we don't have to use this one, we can use larger test data thatare available from somewhere else. Now if we use such a backgroundlanguage model that has pseudocounts, we'll find that some words willreceive more pseudocounts. So what are those words? Well those are the common wordsbecause they get a high probability by the background average model. So the pseudocounts added forsuch words will be higher. Real words on the other handwill have smaller pseudocounts. Now this addition of backgroundmodel would cause a nonuniform smoothing of these word distributions. We're going to bring the probability ofthose common words to a higher level, because of the background model. Now this helps make the differenceof the probability of such words smaller across categories. Because every category has some helpfrom the background four words, and I get the, a,which have high probabilities. Therefore, it's not always soimportant that each category has documents that contain a lotof occurrences of such words or the estimate is more influencedby the background model. And the consequence is thatwhen we do categorization, such words tend not to influencethe decision that much as words that have small probabilitiesfrom the background language model. Those words don't get some helpfrom the background language model. So the difference would be primary becauseof the differences of the occurrences in the training documentsin different categories. We also see another smoothing parametermu here, which controls the amount of smoothing and just like a delta does forthe other probability. And you can easily understand why weadd mu to the denominator, because that represents the sum of all the pseudocountsthat we add for all the words. So view is also a nonnegative constant and it's [INAUDIBLE] set to control smoothing. Now there are some interestingspecial cases to think about as well. First, let's think about when muapproaches infinity what would happen? Well in this casethe estimate would approach to the background language model we'llattempt to the background language model. So we will bring every word distributionto the same background language model and that essentially remove the differencebetween these categories. Obviously, we don't want to do that. The other special case is the thingabout the background model and suppose, we actually setthe two uniform distribution. And let's say,1 over the size of the vocabulary. So each one has the same probability,then this smoothing formula is going to be very similar to the oneon the top when we add delta. It's because we're going to adda constant pseudocounts to every word. So in general, in Naive Bayes categorization wehave to do such a small thing. And then once we have these probabilities, then we can compute the score foreach category. For a document and then choose the category where it wasthe highest score as we discussed earlier. Now, it's useful tofurther understand whether the Naive Bayes scoringfunction actually makes sense. So to understand that, and also tounderstand why adding a background model will actually achieve the effect of IDFweighting and to penalize common words. So suppose we have just two categories and we're going to score based ontheir ratio of probability, right? So this is the. Lets say this is our scoring function for two categories, right? So, this is a score of a document forthese two categories. And we're going to score basedon this probability ratio. So if the ratio is larger, then it means it's morelikely to be in category one. So the larger the score is the more likely the document is in category one. So by using Bayes' rule, we can write down this ratio as follows,and you have seen this before. Now, we generally take logarithm of thisratio, and to avoid small probabilities. And this would then give us thisformula in the second line. And here we see somethingreally interesting, because this is our scoring function fordeciding between the two categories. And if you look at this function,we'll see it has several parts. The first part here is actuallylog of probability ratio. And so this is a category bias. It doesn't really depend on the document. It just says which category is morelikely and then we would then favor this category slightly, right? So, the second part has a sumof all the words, right? So, these are the words thatare observed in the document but in general we can consider allthe words in the vocabulary. So here we're going tocollect the evidence about which category is more likely,right? So inside of the sum you can seethere is product of two things. The first, is a count of the word. And this count of the word serves asa feature to represent the document. And this is what we cancollect from document. The second part isthe weight of this feature, here it's the weight on which word, right? This weight tells us towhat extent observing this word helps contribute in our decision to put this document in category one. Now remember,the higher the scoring function is, the more likely it's in category one. Now if you look at this ratio, basically,sorry this weight it's basically based on the ratio of the probability of theword from each of the two distributions. Essentially we're comparingthe probability of the word from the two distributions. And if it's a higher according to theta 1, then according to theta 2,then this weight would be positive. And therefore it means whenwe observe such a word, we will say that it's morelikely to be from category one. And the more we observe such a word, the more likely the documentwill be classified as theta 1. If, on the other hand,the probability of the word from theta 1 is smaller than the probabilityof the word from theta 2, then you can see thatthis word is negative. Therefore, this is negative evidence forsupporting category one. That means the more weobserve such a word, the more likely the documentis actually from theta 2. So this formula now makes a little sense,right? So we're going to aggregate allthe evidence from the document, we take a sum of all the words. We can call this the features that we collected from the documentthat would help us make the decision. And then each feature hasa weight that tells us how does this feature support category one orjust support category two. And this is estimated as the log ofprobability ratio here in naÃ¯ve Bayes. And then finally we havethis constant of bias here. So that formula actuallyis a formula that can be generalized to accommodatemore features and that's why I have introducesome other symbols here. To introduce beta 0 to denote the Bayesand fi to denote the each feature and beta sub i to denotethe weight on each feature. Now we do this generalisation,what we see is that in general we can representthe document by feature vector fi, here of course in this casefi is the count of a word. But in general, we can put any featuresthat we think are relevant for categorization. For example, document length or font size orcount of other patterns in the document. And then our scoring function can bedefined as a sum of a constant beta 0 and the sum of the featureweights of all the features. So if each f sub i is a featurevalue then we multiply the value by the corresponding weight,beta sub i, and we just take the sum. And this is the aggregate of all evidencethat we can collect from all these features. And of course there are parameters here. So what are the parameters? Well, these are the betas. These betas are weights. And with a proper setting of the weights,then we can expect such a scoring function to work well to classify documents,just like in the case of naive Bayes. We can clearly see naive Bayesclassifier as a special case of this general classifier. Actually, this general form is very closeto a classifier called a logistical regression, and this is actually oneof those conditional approaches or discriminative approachesto classification. And we're going to talk moreabout such approaches later, but here I want you to note thatthere is a strong connection, a close connection betweenthe two kinds of approaches. And this slide shows how naive Bayesclassifier can be connected to a logistic regression. And you can also see that indiscriminative classifiers that tend to use moregeneral form on the bottom, we can accommodate morefeatures to solve the problem. [MUSIC]
This lecture is about thetextual representation. In this lecture, we are going to discuss textualrepresentation, and discuss how naturallanguage processing can allow us to represent textin many different ways. Let's take a look at thisexample sentence again. We can represent this sentencein many different ways. First, we can always represent such a sentenceas a string of characters. This is true forall the languages when we store themin the computer. When we store a naturallanguage sentence as a string of characters, we have perhaps the most generalway of representing text since we always use this approach torepresent any text data. But unfortunately, usingsuch a representation will not help us to do semantic analysis, which is often needed for many applicationsof text mining. The reason is because we'renot even recognizing words. So as a string, we're going to keepall the spaces and these ASCII symbols. We can perhaps count what's the most frequent characterin English text, or the correlationbetween those characters, but we can't reallyanalyze semantics. Yet, this is the mostgeneral way of representing text because we can use this to represent anynatural language text. If we try to do a little bit more naturallanguage processing by doing word segmentation, then we can obtain arepresentation of the same text, but in the form of asequence of words. So here we see thatwe can identify words like a dog is chasing etc. Now with this levelof representation, we certainly can doa lot of things, and this is mainly becausewords are the basic units of human communicationin natural language, so they are very powerful. By identifying words, we can for example easily count what are the most frequent words in this document or inthe whole collection etc. These words can be used to form topics when we combinerelated words together, and some words are positive, some words negative, so we can also do sentiment analysis. So representing text dataas a sequence of words opens up a lot of interestinganalysis possibilities. However, this level of representation is slightlyless general than string of characters because insome languages such as Chinese, it's actually notthat easy to identify all the word boundariesbecause in such a language, you see text as a sequence of characters withno space in between. So you'll have to rely on some special techniquesto identify words. In such a language,of course then, we might make mistakesin segmenting words. So the sequence ofwords representation is not as robust asstring of characters. But in English, it's very easy to obtain this levelof representation, so we can do that all the time. Now, if we go further to do naturallylanguage processing, we can add a part of speech tags. Now once we do that, we can count, for example, the most frequentnouns or what kind of nouns are associated withwhat kind of verbs etc. So this opens up a little bit moreinteresting opportunities for further analysis. Note that I use a plus signhere because by representing text as a sequenceof part of speech tags, we don't necessarily replace the original wordsequence written. Instead, we add this as an additional way ofrepresenting text data, so that now the data isrepresented as both a sequence of words and a sequenceof part of speech tags. This enriches therepresentation of text data, and thus also enablesmore interesting analysis. If we go further, then we'llbe pausing the sentence often to obtaina syntactic structure. Now this of course, further open upa more interesting analysis of, for example, the writing styles orcorrecting grammar mistakes. If we go further forsemantic analysis, then we might be able torecognize dog as an animal, and we also can recognizea boy as a person, and playground as a location. We can further analyzetheir relations, for example, dog is chasing the boy andthe boy is on the playground. Now this will addmore entities and relations throughentity relation recreation. At this level, then we can do even moreinteresting things. For example, now wecan count easily the most frequent person that's mentioning this whole collectionof news articles, or whenever youmention this person, you also tend to see mentioningof another person etc. So this is a veryuseful representation, and it's also related to the knowledge graph thatsome of you may have heard of that Google is doing as a more semantic way ofrepresenting text data. However, it's also less robustthan sequence of words or even syntactical analysisbecause it's not always easy to identify all the entities withthe right types, and we might make mistakes, and relations areeven harder to find, and we might make mistakes. So this makes this level ofrepresentation less robust, yet it's very useful. Now if we move furtherto logical condition, then we can have predicatesand even inference rules. With inference rules, we can infer interesting derivedfacts from the text, so that's very useful. But unfortunately,at this level of representation is even less robust and we can make mistakes and we can't do that all the time forall kinds of sentences. Finally, speech acts wouldadd a yet another level of repetition of the intentof saying this sentence. So in this case, it might be a request. So knowing that wouldallow us to analyze even more interestingthings about this observer or the authorof this sentence. What's the intentionof saying that? What's scenarios? What kindof actions would be made? So this is another level of analysis that wouldbe very interesting. So this picture showsthat if we move down, we generally seemore sophisticated natural language processingtechniques to be used. Unfortunately,such techniques would require more human effort, and they are less accurate. That means there are mistakes. So if we add an texts that are at the levels that are representing deeperanalysis of language, then we have totolerate the errors. So that also means it'sstill necessary to combine such deep analysis withshallow analysis based on, for example, sequence of words. On the right side, you'll see the arrow pointsdown to indicate that. As we go down, we are representationof text is closer to knowledge representationin our mind, and need for solvinga lot of problems. Now this is desirable because as we can represent text atthe level of knowledge, we can easily extractthe knowledge. That's the purposeof text-mining. So there is a trade-off here between doinga deeper analysis that might have errors but would give us direct knowledge thatcan be extracted from text. Doing shallow analysis, which is more robust butwouldn't actually give us the necessary deeperrepresentation of knowledge. I should also say thattext data are generated by humans and are meant tobe consumed by humans. So as a result, intext data analysis, text-mining humans playa very important role, they are always in the loop. Meaning that we should optimize the collaboration ofhumans and computers. So in that sense, it's okay that computersmay not be able to have compute accuratelyrepresentation of text data, and the patternsthat are extracted from text data can beinterpreted by humans, and humans canguide the computers to do more accurate analysisby annotating more data, by providing featuresto guide a machine learning programs to makethem work more effectively.
[SOUND]>> This lecture is about topic mining andanalysis. We're going to talk about itsmotivation and task definition. In this lecture we're going to talkabout different kind of mining task. As you see on this road map,we have just covered mining knowledge about language,namely discovery of word associations such as paradigmatic andrelations and syntagmatic relations. Now, starting from this lecture, we'regoing to talk about mining another kind of knowledge, which is content mining, and trying to discover knowledge aboutthe main topics in the text. And we call that topic mining andanalysis. In this lecture, we're going to talk aboutits motivation and the task definition. So first of all,let's look at the concept of topic. So topic is something that weall understand, I think, but it's actually not thateasy to formally define. Roughly speaking, topic is the mainidea discussed in text data. And you can think of this as a theme orsubject of a discussion or conversation. It can also have different granularities. For example,we can talk about the topic of a sentence. A topic of article,aa topic of paragraph or the topic of all the research articlesin the research library, right, so different grand narratives of topicsobviously have different applications. Indeed, there are many applications thatrequire discovery of topics in text, and they're analyzed then. Here are some examples. For example, we might be interestedin knowing about what are Twitter users are talking about today? Are they talking about NBA sports, or are they talking about someinternational events, etc.? Or we are interested inknowing about research topics. For example, one might be interested inknowing what are the current research topics in data mining, and how are theydifferent from those five years ago? Now this involves discovery of topicsin data mining literatures and also we want to discover topics intoday's literature and those in the past. And then we can make a comparison. We might also be also interested inknowing what do people like about some products like the iPhone 6,and what do they dislike? And this involves discoveringtopics in positive opinions about iPhone 6 andalso negative reviews about it. Or perhaps we're interested in knowingwhat are the major topics debated in 2012 presidential election? And all these have to do with discoveringtopics in text and analyzing them, and we're going to talk about a lotof techniques for doing this. In general we can view a topic assome knowledge about the world. So from text data we expect todiscover a number of topics, and then these topics generally providea description about the world. And it tells us something about the world. About a product, about a person etc. Now when we have some non-text data, then we can have more context foranalyzing the topics. For example, we might know the timeassociated with the text data, or locations where the textdata were produced, or the authors of the text, orthe sources of the text, etc. All such meta data, or context variables can be associatedwith the topics that we discover, and then we can use these context variableshelp us analyze patterns of topics. For example, looking at topics over time,we would be able to discover whether there's a trending topic, orsome topics might be fading away. Soon you are looking at topicsin different locations. We might know some insights aboutpeople's opinions in different locations. So that's why miningtopics is very important. Now, let's look at the tasksof topic mining and analysis. In general, it would involve firstdiscovering a lot of topics, in this case, k topics. And then we also would like to know, whichtopics are covered in which documents, to what extent. So for example, in document one, wemight see that Topic 1 is covered a lot, Topic 2 andTopic k are covered with a small portion. And other topics,perhaps, are not covered. Document two, on the other hand,covered Topic 2 very well, but it did not cover Topic 1 at all, and it also covers Topic k to some extent,etc., right? So now you can see thereare generally two different tasks, or sub-tasks, the first is to discover ktopics from a collection of text laid out. What are these k topics? Okay, major topics in the text they are. The second task is to figure outwhich documents cover which topics to what extent. So more formally,we can define the problem as follows. First, we have, as input,a collection of N text documents. Here we can denote the textcollection as C, and denote text article as d i. And, we generally also need to haveas input the number of topics, k. But there may be techniques that canautomatically suggest a number of topics. But in the techniques that we willdiscuss, which are also the most useful techniques, we often need tospecify a number of topics. Now the output would then be the ktopics that we would like to discover, in order as theta subone through theta sub k. Also we want to generate the coverage oftopics in each document of d sub i And this is denoted by pi sub i j. And pi sub ij is the probabilityof document d sub i covering topic theta sub j. So obviously for each document, we havea set of such values to indicate to what extent the document covers,each topic. And we can assume that theseprobabilities sum to one. Because a document won't be able to cover other topics outside of the topicsthat we discussed, that we discovered. So now, the question is, how do we definetheta sub i, how do we define the topic? Now this problem has notbeen completely defined until we define what is exactly theta. So in the next few lectures, we're going to talk aboutdifferent ways to define theta. [MUSIC]
[SOUND]This lecture is about probabilistic andlatent Semantic Analysis or PLSA. In this lecture we're going to introduceprobabilistic latent semantic analysis, often called PLSA. This is the most basic topic model,also one of the most useful topic models. Now this kind of modelscan in general be used to mine multiple topics from text documents. And PRSA is one of the most basictopic models for doing this. So let's first examine this powerin the e-mail for more detail. Here I show a sample article which isa blog article about Hurricane Katrina. And I show some simple topics. For example government response,flood of the city of New Orleans. Donation and the background. You can see in the article we usewords from all these distributions. So we first for example see there'sa criticism of government response and this is followed by discussion of floodingof the city and donation et cetera. We also see backgroundwords mixed with them. So the overall of topic analysis hereis to try to decode these topics behind the text, to segment the topics,to figure out which words are from which distribution and to figure out first,what are these topics? How do we know there's a topicabout government response. There's a topic about a flood in the city. So these are the tasksat the top of the model. If we had discovered thesetopics can color these words, as you see here,to separate the different topics. Then you can do a lot of things,such as summarization, or segmentation, of the topics,clustering of the sentences etc. So the formal definition of problem ofmining multiple topics from text is shown here. And this is after a slide that youhave seen in an earlier lecture. So the input is a collection, the numberof topics, and a vocabulary set, and of course the text data. And then the output is of two kinds. One is the topic category,characterization. Theta i's. Each theta i is a word distribution. And second, it's the topic coverage foreach document. These are pi sub i j's. And they tell us which document it covers. Which topic to what extent. So we hope to generate these as output. Because there are many usefulapplications if we can do that. So the idea of PLSA isactually very similar to the two component mixture modelthat we have already introduced. The only difference is that weare going to have more than two topics. Otherwise, it is essentially the same. So here I illustrate how we can generatethe text that has multiple topics and naturally in all cases of Probabilistic modelling would wantto figure out the likelihood function. So we would also ask the question, what's the probability of observinga word from such a mixture model? Now if you look at this picture and compare this with the picturethat we have seen earlier, you will see the only difference isthat we have added more topics here. So, before we have just one topic,besides the background topic. But now we have more topics. Specifically, we have k topics now. All these are topics that we assumethat exist in the text data. So the consequence is that our switch forchoosing a topic is now a multiway switch. Before it's just a two way switch. We can think of it as flipping a coin. But now we have multiple ways. First we can flip a coin to decidewhether we're talk about the background. So it's the background lambdasub B versus non-background. 1 minus lambda sub B givesus the probability of actually choosing a non-background topic. After we have made this decision, we have to make another decision tochoose one of these K distributions. So there are K way switch here. And this is characterized by pi,and this sum to one. This is just the difference of designs. Which is a little bit more complicated. But once we decide which distribution touse the rest is the same we are going to just generate a word by using one ofthese distributions as shown here. So now lets look at the questionabout the likelihood. So what's the probability of observinga word from such a distribution? What do you think? Now we've seen thisproblem many times now and if you can recall, it's generally a sum. Of all the different possibilitiesof generating a word. So let's first look at how the word canbe generated from the background mode. Well, the probability that the word isgenerated from the background model is lambda multiplied by the probabilityof the word from the background mode. Model, right. Two things must happen. First, we have to havechosen the background model, and that's the probability of lambda,of sub b. Then second, we must have actuallyobtained the word w from the background, and that's probabilityof w given theta sub b. Okay, so similarly, we can figure out the probability ofobserving the word from another topic. Like the topic theta sub k. Now notice that here'sthe product of three terms. And that's because of the choiceof topic theta sub k, only happens if two things happen. One is we decide not totalk about background. So, that's a probabilityof 1 minus lambda sub B. Second, we also have to actually choosetheta sub K among these K topics. So that's probability of theta sub K,or pi. And similarly, the probability ofgenerating a word from the second. The topic and the first topicare like what you are seeing here. And so in the end the probability of observingthe word is just a sum of all these cases. And I have to stress again this is a veryimportant formula to know because this is really key to understanding all the topicmodels and indeed a lot of mixture models. So make sure that you reallyunderstand the probability of w is indeed the sum of these terms. So, next,once we have the likelihood function, we would be interested inknowing the parameters. All right, so to estimate the parameters. But firstly, let's put all these together to have thecomplete likelihood of function for PLSA. The first line shows the probability of aword as illustrated on the previous slide. And this is an importantformula as I said. So let's take a closer look at this. This actually commands allthe important parameters. So first of all we see lambda sub b here. This represents a percentageof background words that we believe exist in the text data. And this can be a known valuethat we set empirically. Second, we see the backgroundlanguage model, and typically we also assume this is known. We can use a large collection of text, or use all the text that we have availableto estimate the world of distribution. Now next in the next stop this formula. [COUGH] Excuse me. You see two interestingkind of parameters, those are the most important parameters. That we are. So one is pi's. And these are the coverageof a topic in the document. And the other is word distributionsthat characterize all the topics. So the next line,then is simply to plug this in to calculatethe probability of document. This is, again, of the familiarform where you have a sum and you have a count ofa word in the document. And then log of a probability. Now it's a little bit morecomplicated than the two component. Because now we have more components,so the sum involves more terms. And then this line is justthe likelihood for the whole collection. And it's very similar, just accounting formore documents in the collection. So what are the unknown parameters? I already said that there are two kinds. One is coverage,one is word distributions. Again, it's a useful exercise foryou to think about. Exactly how manyparameters there are here. How many unknown parameters are there? Now, try and think out that question will help youunderstand the model in more detail. And will also allow you to understandwhat would be the output that we generate when use PLSA to analyze text data? And these are preciselythe unknown parameters. So after we have obtainedthe likelihood function shown here, the next is to worry aboutthe parameter estimation. And we can do the usual think,maximum likelihood estimator. So again, it's a constrained optimizationproblem, like what we have seen before. Only that we have a collection of text andwe have more parameters to estimate. And we still have two constraints,two kinds of constraints. One is the word distributions. All the words must have probabilitiesthat's sum to one for one distribution. The other is the topiccoverage distribution and a document will have to coverprecisely these k topics so the probability of covering eachtopic that would have to sum to 1. So at this point though it's basicallya well defined applied math problem, you just need to figure outthe solutions to optimization problem. There's a function with many variables. and we need to just figureout the patterns of these variables to make the functionreach its maximum. >> [MUSIC]
##  Course Communication  Important announcements will be posted on the [ Campuswire ](https://campuswire.com/c/G0A3AA370/feed) forum, so please check it out periodically. This is also your go-to place to ask clarifying questions on course content, quizzes, programming assignments, and your final project. Before you post, check if someone has already asked your question. It is highly recommended to check the Campuswire forum at least twice a day.  **mcs-support@illinois.edu** \- this is your go-to place for non-content related, administrative questions you may have for course staff, the program and technical issues with the platform. We are committed to answering your questions with 24 - 48 hours during the working week.  ###  Course Staff Response Turnaround Time  The course staff will attempt to ensure your question is answered within 24-48 hours of it being posted. This does not mean you should post questions at the last minute (e.g., right before a deadline).  ###  Office Hours  The course staff will host weekly office hours via Zoom. To check the weekly schedule and to attend office hours, please refer to the [ Live Events ](https://www.coursera.org/learn/cs-410/office-hours) page.  ###  Zoom  If you plan to attend the office hours, please sign up for a free [ Zoom ](https://zoom.us) account and download the Zoom client application. For Zoom training and support, please refer to the end of this page.  ###  Slack  Slack offers instant messaging and collaboration when you want to connect with the class. As a student, you need to sign up for a free Slack account at [ **https://mcs-dsstudents.slack.com/signup** ](https://mcs- dsstudents.slack.com/signup) . Make sure that when you register, use your @illinois.edu email address. Other emails will NOT work.  If you have any issues with setting up your Slack account, please send an inquiry email to mcs-support@illinois.edu.  Once you have a Slack account, you can join the slack channel for the course by searching **#cs-410-text-info-syst** .  ##  Course Help Resources  ###  For Course Content Issues and Discussions  If you have questions about any lecture videos or assignments, the course [ Campuswire ](https://campuswire.com/c/G0A3AA370/feed "Campuswire") forum is your first go-to place. You can search to see if someone has already posted a similar question. If not, you can post your own thread in the forum.  ###  For MCS-DS Program Support  The MCS-DS Support Team of teaching and operational specialists from both the University of Illinois and Coursera are dedicated to providing you with program support help with any question you have from course enrollment to letters of course completion. You can reach this team at mcsds- support@illinois.edu.  ###  For Coursera Platform Technical Support  If you have a technical issue, the [ Coursera Help Center ](https://learner.coursera.help/) is a great place to start. The Help Center has detailed information on:    * [ Account setup ](https://learner.coursera.help/hc/en-us/sections/201906856)    * [ Video troubleshooting and downloads ](https://learner.coursera.help/hc/en-us/sections/201895863-Videos)    * [ Assignments ](https://learner.coursera.help/hc/en-us/sections/201906966-Quizzes-assignments)    * [ Peer reviews ](https://learner.coursera.help/hc/en-us/sections/201895903-Peer-reviewed-assignments)    * [ Degree student features ](https://learner.coursera.help/hc/en-us/sections/202534946-Degrees-on-Coursera)    * [ Coursera policies ](https://learner.coursera.help/hc/en-us/sections/201906986-Coursera-policies)  ###  For Coursera Platform Urgent Technical Support  If you need to speak to someone for urgent technical support, you are very welcome to use our chat support. This support is offered 24 hours a day 7 days per week. As a degree student, you can access chat support on _any_ page of the [ Course Help Center ](https://learner.coursera.help) .  Remember that Coursera chat support does not always know that you are a degree student. Please **let them know that** **you are a degree student for the MCS- DS program** as soon as you start a chat. That way we can ensure that your problem is solved as quickly as possible.  ###  For Zoom Training and Support    * [ Zoom Help Center ](https://support.zoom.us/hc/en-us?flash_digest=86e0e3b6563491c56c79abe58c35b007b47d28ec) : Zoom help center offers the most comprehensive resources on Zoom. You can type in issues and search for a solution.     * [ Zoom Video Tutorials ](https://support.zoom.us/hc/en-us/articles/206618765-Zoom-Video-Tutorials) : Video tutorials that can be taken at your own pace     * [ Zoom Technical Webinars ](https://support.zoom.us/hc/en-us/articles/203945009-Zoom-Technical-Webinars) : Hosted once every month     * [ Zoom Weekly Training Webinars ](https://support.zoom.us/hc/en-us/articles/206080966-Weekly-Zoom-Training-Webinars) : Free to register and join. This is a 90-minute user onboarding session with live Q&A. If you cannot attend, you can also view the latest recording.     * Zoom 24/7 phone support: Phone dial-in +1.888.799.9666 ext 2 or +1.650.397.6096 ext 2     * [ Zoom 24/7 Live Chat ](https://zoom.us/) : The live chat feature is available to the lower bottom on any Zoom pages. You will see a pop-up button **Help** . Click on the **Help** button and type in the question you have. After typing in your question, you will see the live chat option to chat with a Zoom agent.     * [ Zoom Ticket ](https://support.zoom.us/hc/en-us/requests/new) : Submit a ticket to Zoom for non-urgent account or technical issues.   
[SOUND] This lecture is about thevector space retrieval model. We're going to givean introduction to its basic idea. In the last lecture, we talked aboutthe different ways of designing a retrieval model, which would giveus a different arranging function. In this lecture, we're going totalk about a specific way of designing a ramping function calleda vector space retrieval model. And we're going to give a briefintroduction to the basic idea. Vector space model is a special case of similarity based modelsas we discussed before. Which means we assume relevanceis roughly similarity, between the document and the query. Now whether is this assumptionis true is actually a question. But in order to solve the search problem, we have to convert the vague notionof relevance into a more precise definition that can be implementedwith the program analogy. So in this process,we have to make a number of assumptions. This is the first assumptionthat we make here. Basically, we assume that if a documentis more similar to a query than another document. Then the first document will be assumed itwill be more relevant than the second one. And this is the basis forranking documents in this approach. Again, it's questionable whether this isreally the best definition for randoms. As we will see later thereare other ways to model randoms. The basic idea of vectors for base retrieval model is actuallyvery easy to understand. Imagine a high dimensional space whereeach dimension corresponds to a term. So here I issue a three dimensionalspace with three words, programming, library and presidential. So each term here defines one dimension. Now we can consider vectors in this,three dimensional space. And we're going to assumethat all our documents and the query will be placedin this vector space. So for example, on document mightbe represented by this vector, d1. Now this means this documentprobably covers library and presidential, butit doesn't really talk about programming. What does this mean in termsof representation of document? That just means we're going to look atour document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is onlythe vector root condition of the document. Of course,the document has all information. For example, the orders ofwords are [INAUDIBLE] model and that's because we assume thatthe [INAUDIBLE] of words will [INAUDIBLE]. So with this presentationyou can really see d1 simply suggests a [INAUDIBLE] library. Now this is different from anotherdocument which might be recommended as a different vector, d2 here. Now in this case, the document thatcovers programming and library, but it doesn't talk about presidential. So what does this remind you? Well you can probably guess the topicis likely about program language and the library is software lab library. So this shows that by usingthis vector space reproduction, we can actually capture the differencesbetween topics of documents. Now you can also imaginethere are other vectors. For example,d3 is pointing into that direction, that might be a presidential program. And in fact we can place allthe documents in this vector space. And they will be pointingto all kinds of directions. And similarly, we're going to place our query alsoin this space, as another vector. And then we're going to measure thesimilarity between the query vector and every document vector. So in this case for example, we can easily see d2 seems to bethe closest to this query vector. And therefore,d2 will be rendered above others. So this is basically the mainidea of the vector space model. So to be more precise, vector space model is a framework. In this framework,we make the following assumptions. First, we represent a document andquery by a term vector. So here a term can be any basic concept. For example, a word or a phrase oreven n gram of characters. Those are just sequence ofcharacters inside a word. Each term is assumed that willbe defined by one dimension. Therefore n terms in our vocabulary,we define N-dimensional space. A query vector would consistof a number of elements corresponding to the weightson different terms. Each document vector is also similar. It has a number of elements andeach value of each element is indicating the weight ofthe corresponding term. Here, you can see,we assume there are N dimensions. Therefore, they are N elements each corresponding to the weighton the particular term. So the relevance in this case will be assumed to be the similaritybetween the two vectors. Therefore, our ranking functionis also defined as the similarity between the query vector anddocument vector. Now if I ask you to write a programto implement this approach in a search engine. You would realize thatthis was far from clear. We haven't said a lot of things in detail, therefore it's impossible to actuallywrite the program to implement this. That's why I said, this is a framework. And this has to be refinedin order to actually suggest a particular ranking functionthat you can implement on a computer. So what does this framework not say? Well, it actually hasn't said many things that would be required in orderto implement this function. First, it did not say how we should defineor select the basic concepts exactly. We clearly assumethe concepts are orthogonal. Otherwise, there will be redundancy. For example, if two synonyms or somehowdistinguish it as two different concepts. Then they would be definingtwo different dimensions and that would clearly cause redundancy here. Or all the emphasizing ofmatching this concept, because it would be as ifyou match the two dimensions when you actually matchedone semantic concept. Secondly, it did not say how weexactly should place documents and the query in this space. Basically that show you some examplesof query and document vectors. But where exactly should the vector fora particular document point to? So this is equivalent to howto define the term weights? How do you compute the loseelement values in those vectors? This is a very important question, because term weight in the query vectorindicates the importance of term. So depending on how you assign the weight, you might prefer some termsto be matched over others. Similarly, the total word inthe document is also very meaningful. It indicates how well the termcharacterizes the document. If you got it wrong then you clearlydon't represent this document accurately. Finally, how to define the similaritymeasure is also not given. So these questions must be addressedbefore we can have a operational function that we can actuallyimplement using a program language. So how do we solve these problems is the main topic of the next lecture. [MUSIC]
[SOUND] This lecture is about the inverted index construction. In this lecture, we will continuethe discussion of system implementation. In particular, we're going to discusshow to construct the inverted index. The construction of the inverted indexis actually very easy if the dataset is very small. It's very easy to construct a dictionaryand then store the postings in a file. The problem is that when our datais not able to fit to the memory then we have to use somespecial method to deal with it. And unfortunately, in most retrievalapplications the dataset will be large. And they generally cannot beloaded into memory at once. And there are many approaches tosolve that problem, and sorting-based method is quite common andworks in four steps as shown here. First, you collect the local termID,documentID and frequency tuples. Basically you will locate the termsin a small set of documents. And then once you collect those accountsyou can sort those count based on terms. So that you will be able to locala partial inverted index and these are called rounds. And then you write them intoa temporary file on the disk and then you merge in step 3. Do pairwise merging of these runs, untilyou eventually merge all the runs and generate a single inverted index. So this is an illustration of this method. On the left you see some documents and on the right we have a term lexicon anda document ID lexicon. These lexicons are to map string-basedrepresentations of document IDs or terms into integer representations or map back from integers tothe stream representation. The reason why we want our interestusing integers to present these IDs is because integersare often easier to handle. For example,integers can be used as index for array, and they are also easy to compress. So this is one reason why we tendto map these strings into integers, so that we don't have tocarry these strings around. So how does this approach work? Well, it's very simple. We're going to scan thesedocuments sequentially and then parse the documents andcount the frequencies of terms. And in this stage we generally sortthe frequencies by document IDs, because we process eachdocument sequentially. So we'll first encounter allthe terms in the first document. Therefore the document IDsare all ones in this case. And this will be followed by document IDstwo and they are natural results in this only just because we processthe data in a sequential order. At some point,we will run out of memory and that would have to writethem into the disc. Before we do that we 're going to sortthem, just use whatever memory we have. We can sort them and then this timewe're going to sort based on term IDs. Note that here,we're using the term IDs as a key to sort. So all the entries that share the sameterm would be grouped together. In this case,we can see all the IDs of documents that match term 1 wouldbe grouped together. And we're going to write this intothat this is a temporary file. And would that allows you touse the memory to process and makes a batch of documents. And we're going to do that forall the documents. So we're going to write a lot oftemporary files into the disc. And then the next stage iswe do merge sort basically. We're going to merge them andthen sort them. Eventually, we will geta single inverted index, where the entries are sortedbased on term IDs. And on the top, we're going to seethese are the older entries for the documents that match term ID 1. So this is basically, how we can dothe construction of inverted index. Even though the data cannot beall loaded into the manner. Now, we mention earlier thatbecause of hostings are very large, it's desirable to compress them. So let's now take a little bithow we compressed inverted index. Well the idea of compression in general,is for leverage skewed distributions of values. And we generally have to usevariable-length encoding, instead of the fixed-lengthencoding as we use by default in a program manager like C++. And so how can we leveragethe skewed distributions of values to compress these values? Well in general, we will use fewbits to encode those frequent words at the cost of using longerbit string code those rare values. So in our case, let's think about howwe can compress the TF, tone frequency. Now, if you can picture whatthe inverted index look like, and you will see in post things,there are a lot of tone frequencies. Those are the frequencies ofterms in all those documents. Now, if you think about it, what kindof values are most frequent there? You probably will be able to guessthat small numbers tend to occur far more frequently than large numbers. Why? Well, think about the distribution ofwords and this is to do the sip of slopes, and many words occur just rarely sowe see a lot of small numbers. Therefore, we can use fewer bits forthe small, but highly frequent integers and that's cost of using more bits forlarger integers. This is a trade off of course. If the values are distributed to uniform,then this won't save us any space, but because we tend to see many smallvalues, they are very frequent. We can save on average even thoughsometimes when we see a large number we have to use a lot of bits. What about the document IDsthat we also saw in postings? Well they are not distributedin the skewed way. So how can we deal with that? Well it turns out that we canuse a trick called a d-gap and that is to store the differenceof these term IDs. And we can imagine if a term hasmatched that many documents then there will be longest of document IDs. So when we take the gap, and we take thedifference between adjacent document IDs, those gaps will be small. So again, see a lot of small numbers. Whereas if a term occurredin only a few documents, then the gap would be large,the large numbers would not be frequent. So this creates some skewed distribution, that would allow us tocompress these values. This is also possible becausein order to uncover or uncompress these document IDs,we have to sequentially process the data. Because we stored the difference andin order to recover the exact document ID we have to firstrecover the previous document ID. And then we can add the difference tothe previous document ID to restore the current document ID. Now this was possible because we onlyneeded to have sequential access to those document IDs. Once we look up the term, we look up allthe document IDs that match the term, then we sequentially process them. So it's very natural,that's why this trick actually works. And there are many different methods forencoding. So binary code is a commonly usedcode in just any program language. We use basically fixed glance in coding. Unary code, gamma code, anddelta code are all possibilities and there are many other possibilities. So let's look at someof them in more detail. Binary coding is reallyequal length coding, and that's a property forrandomly distributed values. The unary coding is a variablelength in coding method. In this case, integer this 1 will be encoded as x -1, 1 bit followed by 0. So for example, 3 will be encoded as 2,1s followed by 0, whereas 5 will be encoded as 4,1s, followed by 0, etc. So now you can imagine how many bits do wehave to use for a large number like 100? So how many bits do you have touse exactly for a number like 100? Well exactly, we have to use 100 bits. So it's the same number of bitsas the value of this number. So this is very inefficient if youwere likely to see some large numbers. Imagine if you occasionally see a numberlike 1,000, you have to use 1,000 bits. So this only works well if youare absolutely sure that there will be no large numbers, mostly veryoften you see very small numbers. Now, how do you decode this code? Now since these are variablelength encoding methods, you can't just count how many bits andthen just stop. You can't say 8-bits or 32-bits,then you will start another code. They are variable length, soyou will have to rely on some mechanism. In this case for unary, you can seeit's very easy to see the boundary. Now you can easily see 0 wouldsignal the end of encoding. So you just count up how many 1s youhave seen and at the end you hit 0. You have finished one number,you will start another number. Now we just saw that unarycoding is too aggressive. In rewarding small numbers, and if you occasionally can see a verybig number, it would be a disaster. So what about some otherless aggressive method? Well gamma coding's one of them and in this method we can use unary coding for a transform form of that. So it's 1 plus the floor of log of x. So the magnitude of this value ismuch lower than the original x. So that's why we can affordusing unary code for that. And so first I have the unary code forcoding this log of x. And this would be followed bya uniform code or binary code. And this basically the same uniform code,and binary code are the same. And we're going to use this coder to codethe remaining part of the value of x. And this is basically preciselyx-1 to the floor of log of x So the unary code are basicallycalled the flow of log of x, well add one there and here. But the remaining partwe'll be using uniform code through actually code the difference between the x and this 2 to the log of x. And it's easy to show that for this difference we only need to use up to this many bits andthe floor of log of x bits. And this is easy to understand, if the difference is too large, then wewould have a higher floor of log of x. So here are some examples forexample, 3 is is encoded as 101. The first two digits are the unary code. So this isn't for the value 2, 10 encodes 2 in unary coding. And so that means the floor of log of x is 1,because we won't actually use unary codes. In code 1 plus the flow of log of x, since this is two then we know thatthe flow of log of x is actually 1. So that 3 is still larger than 2 to the 1. So the difference is 1, andthe 1 is encoded here at the end. So that's why we have 101 for 3. Now similarly 5 is encoded as 110,followed by 01. And in this case the unary code in code 3. And so this is a unary code 110 andso the flow of log of x is 2. And that means we're going tocompute a difference between 5 and the 2 to the 2 and that's 1. And so we now have again 1 at the end. But this time we're going to use 2 bits, because with this levelof flow of log of x. We could have more numbers a 5, 6, 7 theywould all share the same prefix here, 110. So in order to differentiate them, we have to use 2 bits inthe end to differentiate them. So you can imagine 6 would be 10 herein the end instead of 01 after 10. It's also true that the form ofa gamma code is always the first odd number of bits, andin the center there is a 0. That's the end of the unary code. And before that or on the left sideof this 0, there will be all 1s. And on the right side of this 0,it's binary coding or uniform coding. So how can you decode such code? Well you again first do unary coding. Once you hit 0, you have got the unarycode and this also tell you how many bits you have to read furtherto decode the uniform code. So this is how you candecode a gamma code. There is also a delta code that'sbasically the same as a gamma code except that you replace the unaryprefix with the gamma code. So that's even lessconservative than gamma code in terms of wording the small integers. So that means, it's okay if youoccasionally see a large number. It's okay with delta code. It's also fine with the gamma code,it's really a big loss for unary code. And they are all operating of course, at different degrees of favoring short orfavoring small integers. And that also means they would beappropriate for a sorting distribution. But none of them is perfect forall distributions. And which method works the best wouldhave to depend on the actual distribution in your dataset. For inverted index compression, people have found that gammacoding seems to work well. So how to uncompress inverted index? I will just talk about this. Firstly, you decodethose encoded integers. And we just I think discussed the how wedecode unary coding and gamma coding. What about the document IDs thatmight be compressed using d-gap? Well, we're going to dosequential decoding so supposed the encoded I list is x1,x2, x3 etc. We first decode x1 to obtainthe first document ID, ID1. Then we can decode x2, which is actually the difference betweenthe second ID and the first one. So we have to add the decodervalue of x2 to ID1 to recover the value of the ID atthis secondary position. So this is where you cansee the advantages of converting document IDs to integers. And that allows us to dothis kind of compression. And we just repeat until wedecode all the documents. Every time we use the document ID inthe previous position to help to recover the document ID in the next position. [MUSIC]
[MUSIC] This lecture is about how to evaluatethe text retrieval system when we have multiple levels of judgements. In this lecture, we will continuethe discussion of evaluation. We're going to look at how toevaluate a text retrieval system, when we have multiplelevels of judgements. So far we have talked aboutthe binary judgements, that means a document is judged asbeing relevant or not relevant. But earlier, we also talk aboutthe relevance as a medal of degrees. So we often can distinguishvery high relevant documents, those are very useful documents,from moderately relevant documents. They are okay, they are useful perhaps. And further from now, we're addingthe documents, those are not useful. So imagine you can have ratings forthese pages. Then, you would havemultiple levels of ratings. For example, here I show example of threelevels, 3 for relevant, sorry 3 for very relevant, 2 for marginally relevant,and 1 for non-relevant. Now, how do we evaluate the searchengine system using these judgements? Obvious that the map doesn't work, averageof precision doesn't work, precision, and recall doesn't work,because they rely on binary judgements. So let's look at some top rankedresults when using these judgements. Imagine the user would be mostlycare about the top ten results here. And we marked the rating levels,or relevance levels, for these documents as shown here,3, 2, 1, 1, 3, etcetera. And we call these gain. And the reason why we call itthe gain is because the measure that we are infusing is called the NDCGnormalized or accumulated gain. So this gain, basically,can measure how much a gain of random information a user can obtain bylooking at each document, right? So looking at the first document,the user can gain 3 points. Looking at the non-relevant documentuser would only gain 1 point. Looking at the moderator ormarginally relevant, document the user would get 2 points,etcetera. So, this gain to each of the measures isa utility of the document from a user's perspective. Of course, if we assume the userstops at the 10 documents and we're looking at the cutoff at 10,we can look at the total gain of the user. And what's that? Well, that's simply the sum of these,and we call it the Cumulative Gain. So if the user stops after the position 1,that's just a 3. If the user looks at another document,that's a 3+2. If the user looks at the more documents,then the cumulative gain is more. Of course this is at the cost ofspending more time to examine the list. So cumulative gain givesus some idea about how much total gain the user would have ifthe user examines all these documents. Now, in NDCG, we also have another letterhere, D, discounted cumulative gain. So, why do we want to do discounting? Well, if you look at this cumulative gain,there is one deficiency, which is it did not consider the rankposition of these documents. So for example, looking at this sum here, and we only know there is 1highly relevant document, 1 marginally relevant document,2 non-relevant documents. We don't really carewhere they are ranked. Ideally, we want these two to be rankedon the top which is the case here. But how can we capture that intuition? Well we have to say, well this is 3 hereis not as good as this 3 on the top. And that means the contributionof the gain from different positions has to beweighted by their position. And this is the idea of discounting,basically. So we're going to to say, well, the firstone does not need to be discounted because the user can be assumedthat will always see this document. But the second one,this one will be discounted a little bit because there's a small possibilitythat the user wouldn't notice it. So we divide this gain bya weight based on the position. So log of 2,2 is the rank position of this document. And when we go to the third position,we discounted even more, because the normalizer is log of 3,and so on and so forth. So when we take such a sum that a lowerranked document would not contribute that much as a highly ranked document. So that means if you, for example,switch the position of this, let's say this position, and this one, and thenyou would get more discount if you put, for example very relevantdocument here as opposed to here. Imagine if you put the 3 here,then it would have to be discounted. So it's not as good as ifyou we would put the 3 here. So this is the idea of discounting. Okay, so now at this point that we havegot a discounted cumulative gain for measuring the utility of this rankedlist with multiple levels of judgements. So are we happy with this? Well, we can use this to rank systems. Now, we still need to do a little bit more in order to make this measurecomparable across different topics. And this is the last step, and by the way,here we just show the DCG at 10, so this is the total sum of DCG,all these 10 documents. So the last step is called N,normalization. And if we do that,then we'll get the normalized DCG. So how do we do that? Well, the idea here is we'regoing to normalize DCG by the ideal DCG at the same cutoff. What is the ideal DCG? Well, this is the DCG of an ideal ranking. So imagine if we have 9 documents inthe whole collection rated 3 here. And that means in total wehave 9 documents rated 3. Then our ideal rank lister would have putall these 9 documents on the very top. So all these would have to be 3 andthen this would be followed by a 2 here. Because that's the best we coulddo after we have run out of the 3. But all these positions would be 3. Right? So this would our ideal ranked list. And then we had computed the DCG forthis ideal rank list. So this would be given by thisformula that you see here. And so this ideal DCG would thenbe used as the normalizer DCG. So here. And this idea of DCG wouldbe used as a normalizer. So you can imagine now,normalization essentially is to compare the actual DCG with the best DCGyou can possibly get for this topic. Now why do we want to do this? Well, by doing this we'll map the DCGvalues into a range of 0 through 1. So the best value, or the highest value,for every query would be 1. That's when your rank list is,in fact, the ideal list but otherwise, in general,you will be lower than one. Now, what if we don't do that? Well, you can see, this transformation,or this normalization, doesn't really affect the relativecomparison of systems for just one topic, because this idealDCG is the same for all the systems, so the ranking of systems based ononly DCG would be exactly the same as if you rank them basedon the normalized DCG. The difference however iswhen we have multiple topics. Because if we don't do normalization, different topics will havedifferent scales of DCG. For a topic like this one,we have 9 highly relevant documents, the DCG can get really high,but imagine in another case, there are only two very relevant documentsin total in the whole collection. Then the highest DCG thatany system could achieve for such a topic would not be very high. So again, we face the problem ofdifferent scales of DCG values. When we take an average, we don't want the average to bedominated by those high values. Those are, again, easy queries. So, by doing the normalization,we can have avoided the problem, making all the queries contributeto equal to the average. So, this is a idea of NDCG, it's used for measuring a rank list based on multiplelevel of relevance judgements. In a more general way thisis basically a measure that can be applied to any ranked taskwith multiple level of judgements. And the scale of the judgementscan be multiple, can be more than binary not only more than binary theycan be much multiple levels like 1, 0, 5 oreven more depending on your application. And the main idea of this measure,just to summarize, is to measure the total utilityof the top k documents. So you always choose a cutoff andthen you measure the total utility. And it would discount the contributionfrom a lowly ranked document. And then finally, it would do normalization to ensure comparability across queries. [MUSIC]
[SOUND] So I showed you how we rewrite the query like holder which is a function intoa form that looks like the formula of this slide after if we makethe assumption about the smoothing, the language model based onthe collection language model. Now if you look at this rewriting,it will actually give us two benefits. The first benefit is it helps us betterunderstand this ranking function. In particular, we're going to show thatfrom this formula we can see smoothing with the collection language model wouldgive us something like a TF-IDF weighting and length normalization. The second benefit is thatit also allows us to compute the query like holder more efficiently. In particular we see thatthe main part of the formula is a sum over the matchof the query terms. So this is much better than if wetake a sum over all the words. After we smooth the document the damagemodel we essentially have non zero problem for all the words. So this new form of the formula ismuch easier to score or to compute. It's also interesting to note that the last term here is actuallyindependent of the document. Since our goal is torank the documents for the same query we can ignore this term forranking. Because it's going to be the same forall the documents. Ignoring it wouldn't affectthe order of the documents. Inside the sum, we also see that each matched queryterm would contribute a weight. And this weight actually is very interesting because itlooks like a TF-IDF weighting. First we can already see it hasa frequency of the word in a query just like in the vector space model. When we take a thought product, we see the word frequency inthe query to show up in such a sum. And so naturally this part wouldcorrespond between the vector element from the documented vector. And here indeed we can see it actually encodes a weight that has similarin factor to TF-IDF weight. I'll let you examine it, can you see it? Can you see which part is capturing TF? And which part isa capturing IDF weighting? So if want you can pausethe video to think more about it. So have you noticed that this P subseen is related to the term frequency in the sense that if a word occursvery frequently in the document, then the s made through probabilityhere will tend to be larger. So this means this term is reallydoing something like a TF weight. Now have you also noticed thatthis term in the denominator is actually achieving the factor of IDF? Why, because this is the popularityof the term in a collection. But it's in the denominator, so if theprobability in the collection is larger then the weight is actually smaller. And this means a popular term. We actually have a smaller weight and thisis precisely what IDF weighting is doing. Only that we now havea different form of TF and IDF. Remember IDF has a logarithmof documented frequency. But here we have something different. But intuitively itachieves a similar effect. Interestingly, we also have somethingrelated to the length of libation. Again, can you see which factor is relatedto the document length in this formula? What I just say is that this termis related to IDF weighting. This collection probability,but it turns out that this term here is actually relatedto document length normalization. In particular, F of sub d mightbe related to document length. So it encodes how much probabilitymass we want to give to unseen worlds. How much smoothing do we want to do? Intuitively, if a document is long, then we need to do less smoothing becausewe can assume that data is large enough. We probably have observed all the wordsthat the author could have written. But if the document is short then r ofsub t could be expected to be large. We need to do more smoothing. It's likey there are words that havenot been written yet by the author. So this term appears to paralyzethe non document in that other sub D would tend to be longerthan or larger than for a long document. But note that alpha sub dalso occurs here and so this may not actually be necessaryparalyzing long documents. The effect is not so clear yet. But as we will see later, when weconsider some specific smoothing methods, it turns out that they doparalyze long documents. Just like in TF-IDF weighting and document length normalizationformula in the vector space model. So, that's a very interestingobservation because it means we don't even have to think aboutthe specific way of doing smoothing. We just need to assume that if we smoothwith this collection memory model, then we would have a formula thatlooks like TF-IDF weighting and documents length violation. What's also interesting that we havevery fixed form of the ranking function. And see we have not heuristicallyput a logarithm here. In fact, you can think about whywe would have a logarithm here. You look at the assumptions thatwe have made, it would be clear it's because we have used a logarithmof query like for scoring. And we turned the product into a sumof logarithm of probability, and that's why we have this logarithm. Note that if only want to heuristicallyimplement a TF weighting and IDF weighting, we don't necessaryhave to have a logarithm here. Imagine if we drop this logarithm,we would still have TF and IDF weighting. But what's nice with problem risk modelingis that we are automatically given the logarithm function here. And that's basically a fixed formof the formula that we did not really have to heuristically design,and in this case if you try to drop the logarithm the model probably won'twork as well as if you keep the logarithm. So a nice property of problem riskmodeling is that by following some assumptions and the probability ruleswe'll get a formula automatically. And the formula would havea particular form like in this case. And if we heuristically designthe formula we may not necessarily end up having such a specific formula. So to summarize, we talked about the needfor smoothing the document imaging model. Otherwise it would give zero probabilityfor unseen words in the document, and that's not good forstoring a query with such an unseen word. It's also necessary, in general,to improve the accuracy of estimating the model representthe topic of this document. The general idea of smoothing in retrievalis to use the connecting memory model to, to give us some clue about which unseenwords should have a higher probability. That is, the probability of an unseenword is assumed to be proportional to its probability in the collection. With this assumption, we've shown that wecan derive a general ranking formula for query likelihood that haseffect of TF-IDF weighting and document length normalization. We also see that, through some rewriting, the scoring of such a ranking functionis primarily based on sum of weights on matched query terms,just like in the vector space model. But, the actual rankingfunction is given us automatically by the probability rules andassumptions that we have made. And like in the vector space modelwhere we have to heuristically think about the form of the function. However, we still need to addressthe question how exactly we should smooth the document and the model. How exactly we shoulduse the reference and model based on the connectionto adjust the probability of the maximum micro is made of andthis is the topic of the next batch. [MUSIC]
[SOUND] This lecture is about the Web Indexing. In this lecture, we will continuetalking about the Web Search and we're going to talk about howto create a Web Scale Index. So once we crawl the web,we've got a lot of web pages. The next step is to use the indexerto create the inverted index. In general, we can use the sameinformation retrieval techniques for creating an index and that is what wetalked about in previous lectures, but there are there are newchallenges that we have to solve. For web scale indexing, and the two mainchallenges are scalability and efficiency. The index would be so large, that it cannot actually fit intoany single machine or single disk. So we have to store the dataon virtual machines. Also, because the data is solarge, it's beneficial to process the data in parallel, sothat we can produce index quickly. Now to address these challenges,Google has made a number of innovations. One is the Google File System that'sa general File system, that can help programmers manage files storedon a cluster of machines. The second is MapReduce. This is a general software framework forsupporting parallel computation. Hadoop is the most well known opensource implementation of MapReduce. Now used in many applications. So, this is the architectureof the google file system. It uses a very simple centralized management mechanism to manageall the specific locations of. Files, soit maintains the file namespace and look up a table to know whereexactly each file is stored. The application client will thentalk to this GFS master, and that obtains specific locations ofthe files they want to process. And once the GFS file kind obtainedthe specific location about the files, then the application client can talkto the specific servers whether data actually sits directly, soyou can avoid involving other node. In the network. So when this file system storesthe files on machines, the system also with great fixed sizes of chunks, sothe data files are separated into. Many chunks. Each chunk is 64 MB, so it's pretty big. And that's appropriate forlarge data processing. These chunks are replicatedto ensure reliability. So this is something that the programmerdoesn't have to worry about, and it's all taken careof by this file system. So from the application perspective, the programmer would see thisas if it's a normal file. And the programmer doesn't have toknow where exactly it is stored and can just invoke high level. Operators to process the file. And another feature is that the datatransfer is directly between application and chunk servers. So it's efficient in this sense. On top of the Google file system, Google also proposed MapReduce as a generalframework for parallel programming. Now, this is very useful to supporta task like building inverted index. And so, this framework is, Hiding a lot of low-levelfeatures from the program. As a result, the programmer can makea minimum effort to create an application that can be run a largecluster in parallel. So some of the low level detailsare hidden in the framework including the specific and network communications orload balancing or where the task are executed. All these details are hiddenfrom the programmer. There is also a nice feature whichis the built in fault tolerance. If one server is broken, the server is down, andthen some tasks may not be finished. Then the MapReduce mapper will knowthat the task has not been done. So it automatically dispatches a taskon other servers that can do the job. And therefore, again the programdoesn't have to worry about that So here's how MapReduce works. The input data would be separatedinto a number of key value pairs. Now what exactly is in the valuewould depend on the data and it's actually a fairly general frameworkto allow you to just partition the data into different parts and each partcan be then processed in parallel. Each key value pair would be andsend it to a map function. The program was right the map function,of course. And then the map function willprocess this Key Value pair and then generate a number ofother Key Value pairs. Of course, the new key is usuallydifferent from the old key that's given to the map as input. And these key value pairsare the output of the map function and all the outputs of all the mapfunctions would be then collected, and then there will be forthe sorting based on the key. And the result is that,all the values that are associated with the same key will bethen grouped together. So now we've got a pair of of a key andseparate values attached to this key. So this would then be sentto a reduce function. Now, of course, each reduce functionwill handle a different key, so we will send these output values to multiple reduce functionseach handling a unique key. A reduce function would thenprocess the input, which is a key in a set of values to produceanother set of key values as the output. So these output values wouldbe then corrected together to form the final output. And so, this is the generalframework of MapReduce. Now the programmer only needs to writethe Map function and the Reduce function. Everything else is actually takencare of by the MapReduce framework. So you can see the program reallyonly needs to do minimum work. And with such a framework, the input datacan be partitioned into multiple parts, which is processing parallel first by map,and then being the process afterwe reach the reduced stage. The much more reduced if I'm[INAUDIBLE] can also further process the different keys andtheir associated values in parallel. So it achieves some, it achieves the purpose of parallelprocessing of a large data set. So let's take a look at a simple example. And that's Word Counting. The input is containing words, and the output that we want to generate isthe number of occurrences of each word. So it's the Word Count. We know this kind of countingwould be useful to, for example, assess the popularity of a word ina large collection and this is useful for achieving a factor of IDF wading forsearch. So how can we solve this problem? Well, one natural thought is that,well this task can be done in parallel by simply countingdifferent parts of the file in parallel, and then in the end we justcombine all the counts. And that's precisely the idea ofwhat we can do with MapReduce. We can parallelize onlines in this input file. So more specifically, we can assumethe input to each map function is a key value pair that represents the linenumber and the string on that line. So the first line, forexample, has a key of one and that is another word by word andjust the four words on that line. So this key value pair wouldbe sent to a Map Function. The Map Function then would justcount the words in this line. And in this case,of course there are only four words. Each world gets a count of one and these are the output that you see hereon this slide from this map function. So the map function is reallyvery simple if you look at what the pseudocode lookslike on the right side, you see it simply needs to iterateall the words and this line. And then just collect the function which means it would then send the wordand the count to the collector. The collector would then try tosort all these key value pairs from different Map Functions, right? So the function is very simple andthe programmer specifies this function as a way toprocess each part of the data. Of course, the second line will behandled by a different Map Function which we will produce a single output. Okay, now the output from the mapfunctions will be then and send it to a collector and the collectorwould do the internal grouping or sorting. So at this stage, you can see,we have collected a match for pairs. Each pair is a word andits count in a line. So, once we see all these pairs. Then we can sort them based on the key,which is the word. So we will collect all the countsof a word, like bye here, together. And similarly, we do that for other words. Like Hadoop, Hello, etc. So each word now is attached toa number of values, a number of counts. And these counts represent the occurrencesto solve this word in different lights. So now we have got a new pair of a key anda set of values, and this pair will then be fed into reducefunction, so the reduce function now would have to finish the job of countingthe total occurrences of this word. Now, it has all ready got allthese puzzle accounts, so all it needs to do issimply to add them up. So the reduce function hereis very simple, as well. You have a counter, andthen iterate all the other words. That you'll see in this array. And that,you just accumulate accounts, right? And then finally, you output the P andthe proto account. And that's precisely what we want asthe output of this whole program. So you can see,this is all ready very similar to. To building an Invert index. And if you think about it,the output here is index. And we have already got a dictionary,basically. We have got the count. But what's missing isthe document the specific frequency counts of wordsin those documents. So we can modify this slightly toactually be able to index in parallel, so here's one way to do that. So in this case, we can assume the inputfrom Map Function is a pair of a key which denotes the document ID,and the value denoting the screen for that document,so it's all the words in that document. And so, the map function would dosomething very similar to what we have seen in the word campaign example. It simply groups all the counts ofthis word in this document together. And it would then generatea set of key value pairs. Each key is a word, and the value is the count of this word inthis document plus the document ID. Now, you can easily see why we need toadd document ID here, because later in inverted index, we would like tokeep this formation, so the Map Function should keep track of it, and this can thenbe sent to the reduce function later. Now similarly another document D2can be processed in the same way. So in the end, again, there is a sortingmechanism that would group them together. And then we will have just a key,like a java, associated with all the documentsthat match this key. Or all the documents where java occurred. And the counts, sothe counts of java in those documents. And this will be collected together. And this will be, sofed into the reduce function. So now you can see the reduce functionhas already got input that looks like an inverted index entry. So it's just the word and allthe documents that contain the word and the frequencies of the wordin those documents. So all you need to do issimply to concatenate them into a continuous chunk of data. And this can be donewritten to a file system. So basically the reduce functionis going to do very minimal. Work. And so, this is a pseudo-code for [INAUDIBLE] that's construction. Here we see two functions,procedure Map and procedure Reduce. And a programmer would specify these twofunctions to program on top of map reduce. And you can see basically theyare doing what I just described. In the case of map, it's going to count the occurrences of a wordusing the AssociativeArray. And it would output all the countstogether with the document ID here. So, this is the reduce function,on the other hand, simply concatenates all the inputthat it has been given, and then put them together asone single entry for this key. So this is a very simpleMapReduce function, yet it would allow us to construct an invertedindex at very large scale, and the data can be processedby different machines. And program doesn't have totake care of the details. So this is how we can do parallelindex construction for web search. So to summarize, web scale indexing requires somenew techniques that go beyond the. Standard traditional indexing techniques. Mainly, we have to storeindex on multiple machines. And this is usually done by using a filingsystem, like a Google file system. But this should be through a file system. And secondly, it requires creatingan index an parallel, because it's so large and takes long time to createan index for all the documents. So if we can do it in parallel,it will be much faster and this is done by usingthe MapReduce framework. Note that both the GFS andMapReduce frameworks are very general, so they can also supportmany other applications. [MUSIC]
[MUSIC] This lecture is aboutthe Recommender Systems. So far we have talked about a lotof aspects of search engines. We have talked about the problemof search and ranking problem, different methods for ranking,implementation of search engine and how to evaluate a search engine, etc. This is important because we knowthat web search engines are by far the most important applicationsof text retrieval. And they are the most useful toolsto help people convert big raw text data into a small setof relevant documents. Another reason why we spend somany lectures on search engines, is because many techniques used in searchengines are actually also very useful for Recommender Systems,which is the topic of this lecture. And so, overall, the two systemsare actually well connected. And there are many techniquesthat are shared by them. So this is a slide thatyou have seen before, when we talked about the twodifferent modes of text access. Pull and the Push. And we mentioned that recommendersystems are the main systems to serve users in the Push Mode, where the systemswill take the initiative to recommend the information to the user orpushes information to the user. And this often workswell when the user has stable information needin the system has a good. So a Recommender System is sometimescalled a filtering system and it's because recommending usefulitems to people is like discarding or filtering out the the useless articles, and soin this sense they are kind of similar. And in all the cases the systemmust make a binary decision and usually there's a dynamic sourceof information items, and that you have some knowledgeabout the users' interest. And then the system would make a decision about whether this item isinteresting to the user, and then if it's interesting then the systemwould recommend the article to the user. So the basic filtering question here isreally will this user like this item? Will U like item X? And there are two ways to answer thisquestion, if you think about it. And one is look at what items U likes and then we can see if X isactually like those items. The other is to look at who likes X,and we can see if this user looks like a one of those users,or like most of those users. And these strategies can be combined. If we follow the first strategy and look at item similarity in the caseof recommending text objects, then we're talking about a content-basedfiltering or content-based recommendation. If we look at the second strategy, then,it's to compare users and in this case we're user similarity and the techniqueis often called collaborative filtering. So, let's first look atthe content-based filtering system. This is what the system would look like. Inside the system, there will bea Binary Classifier that would have some knowledge about the user's interests, andthis is called a User Interest Profile. It maintains this profile to keeptrack of all users interests, and then there is a utility functionto guide the user to make decision a nice plan utilityfunction in the moment. It helps the system decidewhere to set the threshold. And then the accepted documents willbe those that have passed the threshold according to the classified. There should be also an initializationmodule that would take a user's input, maybe from a user's specified keywords orchosen category, etc., and this would be to feed intothe system with the initiator's profile. There is also typically a learningmodule that would learn from users' feedback over time. Now note that in this case typicalusers information is stable so the system would have a lot moreopportunities to observe the users. If the user has taken a recommended item,has viewed that, and this a signal to indicate thatthe recommended item may be relevant. If the user discarded it,no, it's not relevant. And so such feedback can be a long termfeedback, and can last for a long time. And the system can collect a lot ofinformation about the user's interest and this then can then be usedto improve the classify. Now what's the criteria forevaluating such a system? How do we know this filteringsystem actually performs well? Now in this case we cannot use the rankingevaluation measures like a map because we can't afford waiting fora lot of documents and then rank the documents tomake a decision for the users. And so the system must makea decision in real time in general to decide whether the item isabove the threshold or not. So in other words, we're tryingto decide on absolute relevance. So in this case, one common user strategy is to usea utility function to evaluate the system. So here, I show linear utility function. That's defined as for example threemultiplied the number of good items that you delivered, minus two multiplied by thenumber of bad items that you delivered. So in other words, we could kind of just treat this as almost in a gambling game. If you delete one good item,let's say you win three dollars, you gain three dollars but if you delivera bad one you will lose two dollars. And this utility functionbasically kind of measures how much money you are get bydoing this kind of game, right? And so it's clear that if you wantto maximize this utility function, this strategy should be deliveredas many good articles as possible, and minimize the delivery of bad articles. That's obvious, right? Now one interesting question here ishow should we set these coefficients? I just showed a three andnegative two as possible coefficients. But one can ask the question,are they reasonable? So what do you think? Do you think that's a reasonable choice? What about the other choices? So for example, we can have 10 andminus 1, or 1, minus 10. What's the difference? What do you think? How would this utility function affectthe systems' threshold of this issue. Right, you can think ofthese two extreme cases. (10, -1) + (1, -10), which one doyou think would encourage this system to over do it and which one wouldencourage this system to be conservative? If you think about it you will see thatwhen we get a bigger award for delivering our good document you incur only a smallpenalty for delivering a bad one. Intuitively, you would beencouraged to deliver more. And you can try to deliver more inhope of getting a good one delivered. And then we'll get a big reward. So on the other hand,if you choose (1,-10), you really don't get such a big prizeif you deliver a good document. On the other hand, you will havea big loss if you deliver a bad one. You can imagine that, the system would be very reluctantto deliver a lot of documents. It has to be absolutelysure that it's not. So this utility function has to bedesigned based on a specific application. The three basic problems in content-basedfiltering are the following, first, it has to makea filtering decision. So it has to be a binary decision maker,a binary classifier. Given a text document anda profile description of the user, it has to say yes or no, whether thisdocument should be deleted or not. So that's a decision module, andit should be an initialization module as you have seen earlier andthis will get the system started. And we have to initialize the systembased on only very limited text exclusion orvery few examples from the user. And the third model isa learning model which you have, has to be able to learn from limitedrelevance judgements, because we counted them from the user about theirpreferences on the deliver documents. If we don't deliver documentto the user we'll never be able to know whetherthe user likes it or not. And we had accumulate a lot of documentseven then from entire history. All these modules will have to beoptimized to maximize the utility. So how can we deal with such a system? And there are many different approaches. Here we're going to talk abouthow to extend a retrieval system, a search engine for information filtering. Again, here's why we've spent a lot oftime talking about the search engines. Because it's actually not very hardto extend the search engine for information filtering. So here's the basic idea forextending a retrieval system for information filtering. First, we can reuse a lot ofretrieval techniques to do scoring. Right, so we know how to scoredocuments against queries, etc. We're going to match the similaritybetween profile text description and a document. And then we can use a score threshold forthe filtering decision. We do retrieval and then we kind of findthe scores of documents and then we'll apply a threshold to see whether thedocument is passing the threshold or not. And if it's passing the threshold, we're going to say it's relevant andwe're going to deliver it to the user. Another component that we have to add is,of course, to learn from the history, and we had used is the traditional feedbacktechniques to learn to improve scoring. And we know rock hill can be using forscoring improvement. And, but we have to develop a newapproaches to learn how to accept this. And we need to set it initially and then we have to learn how toupdate the threshold over time. So here's what the systemmight look like if we just generalize the vector-space model forfiltering problems, right? So you can see the document vector couldbe fed into a scoring module which already exists in a search enginethat implements a vector-space model. And the profile will be treatedas a query essentially, and then the profile vector can be matched withthe document vector to generate the score. And then this score would be fed into athresholding module that would say yes or no, and then the evaluation would be basedon the utility for the filtering results. If it says yes and then the documentwould be sent to the user. And then user could give some feedback. The feedback information would beused to both adjust the threshold and to adjust the vector representation. So the vector learning is essentiallythe same as query modification or feedback in the case of search. The threshold of learningis a new component and that we need to talka little bit more about. [MUSIC]
[SOUND]. So, as we explained the different text representation tends toenable different analysis. In particular,we can gradually add more and more deeper analysis resultsto represent text data. And that would open up a moreinteresting representation opportunities andalso analysis capacities. So, this table summarizeswhat we have just seen. So the first column showsthe text representation. The second visualizes the generalityof such a representation. Meaning whether we can do thiskind of representation accurately for all the text data or only some of them. And the third column showsthe enabled analysis techniques. And the final column shows someexamples of application that can be achieved through thislevel of representation. So let's take a look at them. So as a stream text can only be processedby stream processing algorithms. It's very robust, it's general. And there was still some interestingapplications that can be down at this level. For example, compression of text. Doesn't necessarily need toknow the word boundaries. Although knowing word boundariesmight actually also help. Word base repetition is a veryimportant level of representation. It's quite general and relatively robust, indicating theywere a lot of analysis techniques. Such as word relation analysis,topic analysis and sentiment analysis. And there are many applications that canbe enabled by this kind of analysis. For example, thesaurus discovery hasto do with discovering related words. And topic andopinion related applications are abounded. And there are, for example, people might be interesting in knowing the majortopics covered in the collection of texts. And this can be the casein research literature. And scientists want to know what are themost important research topics today. Or customer service people might want toknow all our major complaints from their customers by mining their e-mail messages. And business intelligencepeople might be interested in understanding consumers' opinions abouttheir products and the competitors' products to figure out what are thewinning features of their products. And, in general, there are many applications that can be enabled bythe representation at this level. Now, moving down, we'll see we cangradually add additional representations. By adding syntactical structures,we can enable, of course, syntactical graph analysis. We can use graph mining algorithmsto analyze syntactic graphs. And some applications are relatedto this kind of representation. For example, stylistic analysis generally requiressyntactical structure representation. We can also generatethe structure based features. And those are features that might help usclassify the text objects into different categories by looking at the structuressometimes in the classification. It can be more accurate. For example,if you want to classify articles into different categories correspondingto different authors. You want to figure out which ofthe k authors has actually written this article, then you generally needto look at the syntactic structures. When we add entities and relations, then we can enable other techniquessuch as knowledge graph and answers, or information network andanswers in general. And this analysis enableapplications about entities. For example, discovery of all the knowledge andopinions about real world entities. You can also use this level representation to integrate everything aboutanything from scaled resources. Finally, when we add logical predicates, that would enable large inference,of course. And this can be very useful for integrating analysis ofscattered knowledge. For example,we can also add ontology on top of the, extracted the information from text,to make inferences. A good of example of application in thisenabled by this level of representation, is a knowledge assistant for biologists. And this program that can help a biologistmanage all the relevant knowledge from literature about a research problem suchas understanding functions of genes. And the computer can make inferences about some of the hypothesis thatthe biologist might be interesting. For example,whether a gene has a certain function, and then the intelligent program can read theliterature to extract the relevant facts, doing compiling andinformation extracting. And then using a logic system toactually track that's the answers to researchers questioning about whatgenes are related to what functions. So in order to supportthis level of application we need to go as far aslogical representation. Now, this course is covering techniquesmainly based on word based representation. And these techniques are general and robust and that's more widelyused in various applications. In fact, in virtually all the text miningapplications you need this level of representation and then techniques thatsupport analysis of text in this level. But obviously all these otherlevels can be combined and should be combined in order to supportthe sophisticated applications. So to summarize,here are the major takeaway points. Text representation determines whatkind of mining algorithms can be applied. And there are multiple ways torepresent the text, strings, words, syntactic structures, entity-relationgraphs, knowledge predicates, etc. And these differentrepresentations should in general be combined in real applicationsto the extent we can. For example, even if we cannotdo accurate representations of syntactic structures, we can statethat partial structures strictly. And if we can recognize some entities,that would be great. So in general we want todo as much as we can. And when different levelsare combined together, we can enable a richer analysis,more powerful analysis. This course however focuseson word-based representation. Such techniques have also severaladvantage, first of they are general and robust, so they are applicableto any natural language. That's a big advantage overother approaches that rely on more fragile natural languageprocessing techniques. Secondly, it does not requiremuch manual effort, or sometimes, it does notrequire any manual effort. So that's, again, an important benefit, because that means that you can applyit directly to any application. Third, these techniques are actuallysurprisingly powerful and effective form in implications. Although not all of courseas I just explained. Now they are very effectivepartly because the words are invented by humans as basicallyunits for communications. So they are actually quite sufficient forrepresenting all kinds of semantics. So that makes this kind of word-basedrepresentation all so powerful. And finally, such a word-basedrepresentation and the techniques enable by such a representation can be combinedwith many other sophisticated approaches. So they're not competing with each other. [MUSIC]
[MUSIC] This lecture is about topic mining andanalysis. We're going to talk aboutusing a term as topic. This is a slide that you haveseen in a earlier lecture where we define the task oftopic mining and analysis. We also raised the question, how dowe exactly define the topic of theta? So in this lecture, we're going tooffer one way to define it, and that's our initial idea. Our idea here is defininga topic simply as a term. A term can be a word or a phrase. And in general,we can use these terms to describe topics. So our first thought is justto define a topic as one term. For example, we might have termslike sports, travel, or science, as you see here. Now if we define a topic in this way, we can then analyze the coverageof such topics in each document. Here for example, we might want to discover to whatextent document one covers sports. And we found that 30% of the contentof document one is about sports. And 12% is about the travel, etc. We might also discover documenttwo does not cover sports at all. So the coverage is zero, etc. So now, of course,as we discussed in the task definition for topic mining and analysis,we have two tasks. One is to discover the topics. And the second is to analyze coverage. So let's first thinkabout how we can discover topics if we representeach topic by a term. So that means we need to mine ktopical terms from a collection. Now there are, of course,many different ways of doing that. And we're going to talk abouta natural way of doing that, which is also likely effective. So first of all, we're going to parse the text data inthe collection to obtain candidate terms. Here candidate terms can be words orphrases. Let's say the simplest solution isto just take each word as a term. These words then become candidate topics. Then we're going to design a scoringfunction to match how good each term is as a topic. So how can we design such a function? Well there are many thingsthat we can consider. For example, we can use pure statisticsto design such a scoring function. Intuitively, we would like tofavor representative terms, meaning terms that can representa lot of content in the collection. So that would mean we wantto favor a frequent term. However, if we simply use the frequencyto design the scoring function, then the highest scored termswould be general terms or functional terms like the, etc. Those terms occur very frequently English. So we also want to avoid havingsuch words on the top so we want to penalize such words. But in general, we would like to favorterms that are fairly frequent but not so frequent. So a particular approach could be basedon TF-IDF weighting from retrieval. And TF stands for term frequency. IDF stands for inverse document frequency. We talked about some of these ideas in the lectures aboutthe discovery of word associations. So these are statistical methods, meaning that the function isdefined mostly based on statistics. So the scoring functionwould be very general. It can be applied to any language,any text. But when we apply such a approachto a particular problem, we might also be able to leveragesome domain-specific heuristics. For example, in news we might favortitle words actually general. We might want to favor titlewords because the authors tend to use the title to describethe topic of an article. If we're dealing with tweets,we could also favor hashtags, which are invented to denote topics. So naturally, hashtags can be goodcandidates for representing topics. Anyway, after we have this designscoring function, then we can discover the k topical terms by simply pickingk terms with the highest scores. Now, of course, we might encounter situation where thehighest scored terms are all very similar. They're semantically similar, orclosely related, or even synonyms. So that's not desirable. So we also want to have coverage overall the content in the collection. So we would like to remove redundancy. And one way to do that isto do a greedy algorithm, which is sometimes called a maximalmarginal relevance ranking. Basically, the idea is to go downthe list based on our scoring function and gradually take termsto collect the k topical terms. The first term, of course, will be picked. When we pick the next term, we'regoing to look at what terms have already been picked and try to avoidpicking a term that's too similar. So while we are consideringthe ranking of a term in the list, we are also consideringthe redundancy of the candidate term with respect to the termsthat we already picked. And with some thresholding,then we can get a balance of the redundancy removal andalso high score of a term. Okay, soafter this that will get k topical terms. And those can be regarded as the topicsthat we discovered from the connection. Next, let's think about how we're goingto compute the topic coverage pi sub ij. So looking at this picture,we have sports, travel and science and these topics. And now suppose you are give a document. How should we pick out coverageof each topic in the document? Well, one approach can be to simplycount occurrences of these terms. So for example, sports might have occurredfour times in this this document and travel occurred twice, etc. And then we can just normalize thesecounts as our estimate of the coverage probability for each topic. So in general, the formula wouldbe to collect the counts of all the terms that represent the topics. And then simply normalize them sothat the coverage of each topic in the document would add to one. This forms a distribution of the topicsfor the document to characterize coverage of different topics in the document. Now, as always,when we think about idea for solving problem, we have to askthe question, how good is this one? Or is this the best wayof solving problem? So now let's examine this approach. In general,we have to do some empirical evaluation by using actual data sets andto see how well it works. Well, in this case let's takea look at a simple example here. And we have a text document that'sabout a NBA basketball game. So in terms of the content,it's about sports. But if we simply count thesewords that represent our topics, we will find that the word sportsactually did not occur in the article, even though the contentis about the sports. So the count of sports is zero. That means the coverage of sportswould be estimated as zero. Now of course,the term science also did not occur in the document andit's estimate is also zero. And that's okay. But sports certainly is not okay becausewe know the content is about sports. So this estimate has problem. What's worse, the term travelactually occurred in the document. So when we estimate the coverageof the topic travel, we have got a non-zero count. So its estimated coveragewill be non-zero. So this obviously is also not desirable. So this simple example illustratessome problems of this approach. First, when we count whatwords belong to to the topic, we also need to consider related words. We can't simply just countthe topic word sports. In this case, it did not occur at all. But there are many related wordslike basketball, game, etc. So we need to countthe related words also. The second problem is that a wordlike star can be actually ambiguous. So here it probably meansa basketball star, but we can imagine it might alsomean a star on the sky. So in that case, the star might actuallysuggest, perhaps, a topic of science. So we need to deal with that as well. Finally, a main restriction of thisapproach is that we have only one term to describe the topic, so it cannotreally describe complicated topics. For example, a very specializedtopic in sports would be harder to describe by using just a word orone phrase. We need to use more words. So this example illustratessome general problems with this approach of treating a term as topic. First, it lacks expressive power. Meaning that it can only representthe simple general topics, but it cannot represent the complicated topicsthat might require more words to describe. Second, it's incompletein vocabulary coverage, meaning that the topic itselfis only represented as one term. It does not suggest what otherterms are related to the topic. Even if we're talking about sports,there are many terms that are related. So it does not allow us to easilycount related terms to order, conversion to coverage of this topic. Finally, there is this problemof word sense disintegration. A topical term orrelated term can be ambiguous. For example,basketball star versus star in the sky. So in the next lecture,we're going to talk about how to solvethe problem with of a topic. [MUSIC]
This lecture is about that Latent Dirichlet Allocation or LDA. In this lecture, we are going to continue talking about topic models. In particular, we are going to talk about some extension of PLSA, and one of them is LDA or Latent Dirichlet Allocation. So the plan for this lecture is to cover two things. One is to extend the PLSA with prior knowledge and that would allow us to have in some sense a user-controlled PLSA, so it doesn't apply to they just listen to data, but also would listen to our needs. The second is to extend the PLSA as a generative model, a fully generative model. This has led to the development of Latent Dirichlet Allocation or LDA. So first, let's talk about the PLSA with prior knowledge. Now in practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. The standard PLSA is going to blindly listen to the data by using maximum [inaudible]. We are going to just fit data as much as we can and get some insight about data. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. For example, we might expect to see retrieval models as a topic in information retrieval or we also may be interesting in certain aspects, such as battery and memory, when looking at opinions about a laptop because the user is particularly interested in these aspects. A user may also have knowledge about topic coverage and we may know which topic is definitely not covering which document or is covering the document. For example, we might have seen those tags, topic tags assigned to documents. And those tags could be treated as topics. If we do that then a document account will be generated using topics corresponding to the tags already assigned to the document. If the document is not assigned a tag, we're going to say there is no way for using that topic to generate document. The document must be generated by using the topics corresponding to that assigned tags. So question is how can we incorporate such knowledge into PLSA. It turns out that there is a very elegant way of doing that and that would incorporate such knowledge as priors on the models. And you may recall in Bayesian inference, we use prior together with data to estimate parameters and this is precisely what would happen. So in this case, we can use maximum a posteriori estimate also called MAP estimate and the formula is given here. Basically, this is to maximize the posteriori distribution probability. And this is a combination of the likelihood of data and the prior. So what would happen is that we are going to have an estimate that listens to the data and also listens to our prior preferences. We can use this prior which is denoted as p of lambda to encode all kinds of preferences and the constraints. So for example, we can use this to encode the need of having precise background of the topic. Now this could be encoded as a prior because we can say the prior for the parameters is only a non-zero if the parameters contain one topic that is equivalent to the background language model. In other words, in other cases if it is not like that, we are going to say the prior says it is impossible. So the probability of that kind of models I think would be zero according to our prior. So now we can also for example use the prior to force particular choice of topic to have a probability of a certain number. For example, we can force document D to choose topic one with probability of one half or we can prevent topic from being used in generating document. So we can say the third topic should not be used in generating document D, we will set to the Pi zero for that topic. We can also use the prior to favor a set of parameters with topics that assign high probability to some particular words. In this case, we are not going to say it is impossible but we can just strongly favor certain kind of distributions and you will see example later. The MAP can be computed using a similar EM algorithm as we have used for the maximum likelihood estimate. With just some modifications, most of the parameters would reflect the prior preferences and in such an estimate if we use a special form of the prior code or conjugate the prior, then the functional form of the prior will be similar to the data. As a result, we can combine the two and the consequence is that you can basically convert the inference of the prior into the inference of having additional pseudo data because the two functional forms are the same and they can be combined. So the effect is as if we had more data and this is convenient for computation. It does not mean conjugate prior is the best way to define prior. So now let us look at the specific example. Suppose the user is particularly interested in battery life of a laptop and we are analyzing reviews. So the prior says that the distribution should contain one distribution that would assign high probability to battery and life. So we could say well there is distribution that is kind of concentrated on battery life and prior says that one of distributions should be very similar to this. Now if we use MAP estimate with conjugate prior, which is the original prior, the original distribution based on this preference, then the only difference in the EM is that when we re-estimate words distributions, we are going to add additional counts to reflect our prior. So here you can see the pseudo counts are defined based on the probability of words in a prior. So battery obviously would have high pseudo counts and similarly life would have also high pseudo counts. All the other words would have zero pseudo counts because their probability is zero in the prior and we see this is also controlled by a parameter mu and we are going to add a mu much by the probability of W given prior distribution to the connected accounts when we re-estimate this word distribution. So this is the only step that is changed and the change is happening here. And before we just connect the counts of words that we believe have been generated from this topic but now we force this distribution to give more probabilities to these words by adding them to the pseudo counts. So in fact we artificially inflated their probabilities. To make this distribution, we also need to add this many pseudo counts to the denominator. This is total sum of all the pseudo counts we have added for all the words This would make this a gamma distribution. Now this is intuitively very reasonable way of modifying EM and theoretically speaking, this works and it computes the MAP estimate. It is useful to think about the two specific extreme cases of mu. Now, [inaudible] the picture. Think about what would happen if we set mu to zero. Well that essentially to remove this prior. So mu in some sense indicates our strengths on prior. Now what would happen if we set mu to positive infinity. Well that is to say that prior is so strong that we are not going to listen to the data at all. So in the end, you see in this case we are going to make one of the distributions fixed to the prior. You see why? When mu is infinitive, we basically let this one dominate. In fact we are going to set this one to precise this distribution. So in this case, it is this distribution. And that is why we said the background language model is in fact a way to impose the prior because it would force one distribution to be exactly the same as what we give, that is background distribution. So in this case, we can even force the distribution to entirely focus on battery life. But of course this would not work well because it cannot attract other words. It would affect the accuracy of counting topics about battery life. So in practice, mu is set somewhere in between of course. So this is one way to impose a prior. We can also impose some other constraints. For example, we can set any parameters that will constantly include zero as needed. For example, we may want to set one of the Pi's to zero and this would mean we do not allow that topic to participate in generating that document. And this is only reasonable of course when we have prior analogy that strongly suggests this.
In this lecture we're going to talk about how to instantiatevector space model so that we can get veryspecific ranking function. So this is to continue the discussionof the vector space model, which is one particular approachto design a ranking function. And we're going to talk about howwe use the general framework of the the vector spacemodel as a guidance to instantiate the framework to derivea specific ranking function. And we're going to cover the symbolistinstantiation of the framework. So as we discussed inthe previous lecture, the vector space modelis really a framework. And this didn't say. As we discussed in the previous lecture,vector space model is really a framework. It does not say many things. So, for example, here it shows that it did not sayhow we should define the dimension. It also did not say how we placea document vector in this space. It did not say how we place a queryvector in this vector space. And, finally, it did not say how weshould measure the similarity between the query vector and the document vector. So you can imagine,in order to implement this model, we have to say specificallyhow we compute these vectors. What is exactly xi? And what is exactly yi? This will determine wherewe place a document vector, where we place a query vector. And, of course, we also need to say exactly whatshould be the similarity function. So if we can provide a definitionof the concepts that would define the dimensions and these xi's oryi's and namely weights of terms for queries and document, then we will beable to place document vectors and query vectors in this well defined space. And then,if we also specify similarity function, then we'll have a welldefined ranking function. So let's see how we can do that andthink about the instantiation. Actually, I would suggest you topause the lecture at this point, spend a couple minutes to think about. Suppose you are askedto implement this idea. You have come up with the idea of vectorspace model, but you still haven't figured out how to compute these vectors exactly,how to define the similarity function. What would you do? So, think for a couple of minutes,and then proceed. So, let's think about some simplest waysof instantiating this vector space model. First, how do we define the dimension? Well, the obvious choice is to use each word in our vocabularyto define the dimension. And show that there are Nwords in our vocabulary. Therefore, there are N dimensions. Each word defines one dimension. And this is basicallythe bag of words with Now let's look at how weplace vectors in this space. Again here, the simplest strategy is to use a Bit Vector to representboth the query and a document. And that means each element, xi and yi will be taking a valueof either zero or 1. When it's 1, it means the corresponding word ispresent in the document or in the query. When it's 0,it's going to mean that it's absent. So you can imagine if the usertypes in a few words in the query, then the query vector will onlyhave a few 1's, many, many zeros. The document vector,generally we have more 1's, of course. But it will also have many zeros sincethe vocabulary is generally very large. Many words don't reallyoccur in any document. Many words will only occasionallyoccur in a document. A lot of words will be absentin a particular document. So now we have placed the documents andthe query in the vector space. Let's look at how wemeasure the similarity. So, a commonly used similaritymeasure here is Dot Product. The Dot Product of twovectors is simply defined as the sum of the products of thecorresponding elements of the two vectors. So, here we see that it'sthe product of x1 and y1. So, here. And then, x2 multiplied by y2. And then, finally, xn multiplied by yn. And then, we take a sum here. So that's a Dot Product. Now, we can represent this in a moregeneral way using a sum here. So this is only one of the many differentways of measuring the similarity. So, now we see that we havedefined the dimensions, we have defined the vectors, and we havealso defined the similarity function. So now we finally have the simplestvector space model, which is based on the bit vector [INAUDIBLE] dot productsimilarity and bag of words [INAUDIBLE]. And the formula looks like this. So this is our formula. And that's actually a particular retrievalfunction, a ranking function right? Now we can finally implement thisfunction using a program language, and then rank the documents for query. Now, at this point you shouldagain pause the lecture to think about how we caninterpreted this score. So, we have gone through the processof modeling the retrieval problem using a vector space model. And then,we make assumptions about how we place vectors in the vector space, andhow do we define the similarity. So in the end, we've got a specificretrieval function shown here. Now, the next step is to think aboutwhether this retrieval function actually makes sense, right? Can we expect this functionto actually perform well when we used it to rank documents foruser's queries? So it's worth thinking about what isthis value that we are calculating. So, in the end, we'll get a number. But what does this number mean? Is it meaningful? So, spend a couple minutesto sort of think about that. And, of course, the general question here is do youbelieve this is a good ranking function? Would it actually work well? So, again,think about how to interpret this value. Is it actually meaningful? Does it mean something? This is related to how wellthe document matched the query. So, in order to assesswhether this simplest vector space model actually works well,let's look at the example. So, here I show some sample documents anda sample query. The query is news aboutthe presidential campaign. And we have five documents here. They cover different terms in the query. And if you look at these documents fora moment, you may realize that some documents are probably relevant, andsome others are probably not relevant. Now, if I asked you to rank thesedocuments, how would you rank them? This is basically our ideal ranking. When humans can examine the documents,and then try to rank them. Now, so think for a moment,and take a look at this slide. And perhaps by pausing the lecture. So I think most of you wouldagree that d4 and d3 are probably better than others because theyreally cover the query well. They match news,presidential and campaign. So, it looks like these documentsare probably better than the others. They should be ranked on top. And the other three d2, d1, andd5 are really not relevant. So we can also say d4 andd3 are relevant documents, and d1, d2 and d5 are non-relevant. So now let's see if our simplestvector space model could do the same, or could do something closer. So, let's first think abouthow we actually use this model to score documents. All right. Here I show two documents, d1 and d3. And we have the query also here. In the vector space model, of course wewant to first compute the vectors for these documents and the query. Now, I showed the vocabulary here as well. So these are the end dimensionsthat we'll be thinking about. So what do you think is the vector forthe query? Note that we're assumingthat we only use zero and 1 to indicate whether a term is absent orpresent in the query or in the document. So these are zero,1 bit vectors. So what do you think is the query vector? Well, the query has four words here. So for these four words,there will be a 1. And for the rest, there will be zeros. Now, what about the documents? It's the same. So d1 has two rows, news and about. So, there are two 1's here,and the rest are zeroes. Similarly, so now that we have the two vectors,let's compute the similarity. And we're going to use Do Product. So you can see when we use Dot Product, we just multiply the correspondingelements, right? So these two will be formal product, and these two willgenerate another product, and these two will generate yetanother product and so on, so forth. Now you can easily see if we do that,we actually don't have to care about these zeroes because whenever we havea zero the product will be zero. So when we take a sumover all these pairs, then the zero entries will be gone. As long as you have one zero,then the product would be zero. So, in the fact, we're justcounting how many pairs of 1 and 1. In this case, we have seen two,so the result will be 2. So what does that mean? Well, that means this number, orthe value of this scoring function, is simply the count of how many uniquequery terms are matched in the document. Because if a term is matched in thedocument, then there will be two one's. If it's not, then there willbe zero on the document side. Similarly, if the document has a term butthe term is not in the query, there will be a zero in the query vector. So those don't count. So, as a result,this scoring function basically measures how many unique queryterms are matched in a document. This is how we interpret this score. Now, we can also take a look at d3. In this case, you can see the resultis 3 because d3 matched to the three distinctive query words news, presidentialcampaign, whereas d1 only matched the two. Now in this case, this seemsreasonable to rank d3 on top of d1. And this simplest vectorspace model indeed does that. So that looks pretty good. However, if we examine this model indetail, we likely will find some problems. So, here I'm going to show allthe scores for these five documents. And you can easily verify they'recorrect because we're basically counting the number of unique queryterms matched in each document. Now note that this measureactually makes sense, right? It basically means if a documentmatches more unique query terms, then the document will beassumed to be more relevant. And that seems to make sense. The only problem is here we can note thatthere are three documents, d2, d3 and d4. And they tied with a 3 as a score. So, that's a problem because if you lookat them carefully, it seems that the d4 should be ranked above d3 because d3 only mentions the presidential once,but d4 mentioned it multiple times. In the case of d3,presidential could be an dimension. But d4 is clearly abovethe presidential campaign. Another problem is that d2 andd3 also have the same score. But if you look at the three wordsthat are matched, in the case of d2, it matched the news, about and campaign. But in the case of d3, it matched news,presidential and campaign. So intuitively this reads betterbecause matching presidential is more important than matching about, even though about andthe presidential are both in the query. So intuitively,we would like d3 to be ranked above d2. But this model doesn't do that. So that means this modelis still not good enough. We have to solve these problems. To summarize, in this lecture we talked about howto instantiate a vector space model. We mainly need to do three things. One is to define the dimension. The second is to decide how to placedocuments as vectors in the vector space, and to also place a query inthe vector space as a vector. And third is to definethe similarity between two vectors, particularly the query vector andthe document vector. We also talked about various simple wayto instantiate the vector space model. Indeed, that's probably the simplestvector space model that we can derive. In this case,we use each word to define the dimension. We use a zero, 1 bit vector torepresent a document or a query. In this case, we basically only careabout word presence or absence. We ignore the frequency. And we use the Dot Productas the similarity function. And with such a instantiation, we showed that the scoringfunction is basically to score a document based on the number of distinctquery words matched in the document. We also showed that such a simple vectorspace model still doesn't work well, and we need to improve it. And this is a topic that we'regoing to cover in the next lecture. [MUSIC]
[SOUND] This lecture is about how to do fastersearch by using invert index. In this lecture, we're going to continuethe discussion of system implementation. In particular, we're going to talk about how to supporta faster search by using invert index. So let's think about what a generalscoring function might look like. Now of course, the vector spacemodel is a special case of this, but we can imagine many other retrievalfunctions of the same form. So the form of thisfunction is as follows. We see this scoring functionof a document D and a query Q is defined asfirst a function of fa that adjustment a function thatwould consider two factors. That I'll assume here at the end, f sub d of d and f sub q of q. These are adjustment factorsof a document and a query, so they are at the level of a document andthe query. So and then inside of this function, we also see there'sanother function called h. So this is the main partof the scoring function and these as I just said ofthe scoring factors at the level of the whole document andthe query. For example, document [INAUDIBLE] and this aggregate punching wouldthen combine all these. Now inside this h function, there are functions thatwould compute the weights of the contribution ofa matched query term ti. So this g,the function g gives us the weight of a matched query term ti in document d. And this h function would thenaggregate all these weights. So for example,take a sum of all the matched query terms, but it can also be a product or it couldbe another way of aggregating them. And then finally, this adjustmentthe functioning would then consider the document level or query levelfactors to further adjust this score, for example, document [INAUDIBLE]. So, this general form would covermany state of [INAUDIBLE] functions. Let's look at how we can score documentswith such a function using virtual index. So, here's a general algorithmthat works as follows. First this query level and document level factors can bepre-computed in the indexing time. Of course, for the query we have tocompute it at the query time but for document, for example,document [INAUDIBLE] can be pre-computed. And then, we maintain a score accumulatorfor each document d to computer h. An h is an aggregation functionover all the matching query terms. So how do we do that? For each period term we're going todo fetch the inverted list from the invert index. This will give us all the documentsthat match this query term and that includes d1, f1 and so dn fn. So each pair is a document ID andthe frequency of the term in the document. Then for each entry d sub j andf sub j are particular match of the term in thisparticular document d sub j. We'll going to compute the functiong that would give us something like weight of this term, so we're computing the weight completion ofmatching this query term in this document. And then, we're going to updatethe score accumulator for this document andthis would allow us to add this to our accumulator that wouldincrementally compute function h. So this is basically a generalway to allow pseudo computer or functions of this form byusing the inbound index. Note that we don't have toattach any of document and that didn't match any query term. Well, this is why it's fast, we only need to process the documentsthat matched at least one query term. In the end, then we're going to adjustthe score the computer this function f sub a and then we can sort. So let's take a lookat a specific example. In this case, let's assume the scoringfunction is a very simple one, it just takes the sum of t f, the role oft f, the count of a term in the document. This simplification would helpshield the algorithm clearly. It's very easy to extend the computationto include other weights like the transformation of tf, or [INAUDIBLE]or IDF [INAUDIBLE]. So let's take a look at specific example,where the queries information security and it show some entries ofinvert index on the right side. Information occurred in four documents and their frequencies are also there,security occurred in three documents. So let's see how the arrows works, sofirst we iterate overall query terms and we fetch the first query then,what is that? That's information, right? And imagine we have all thesescore accumulators who score the, scores for these documents. We can imagine there will be other but then they will only beallocated as needed. So before we do any waiting of terms, we don't even need a score of. That comes actually we have these scoreaccumulators eventually allocating. So lets fetch the interest fromthe entity [INAUDIBLE] for information, that the first one. So these four accumulators obviouslywould be initialize as zeros. So, the first entry is d1 and 3, 3 is occurrences ofinformation in this document. Since our scoring function assume that thescore is just a sum of these raw counts. We just need to add a 3 to the scoreaccumulator to account for the increase of score due to matchingthis term information, a document d1. And then, we go to the next entry,that's d2 and 4 and then we add a 4 to the scoreaccumulator of d2. Of course, at this point, that we willallocate the score accumulator as needed. And so at this point, we allocatedthe d1 and d2, and the next one is d3, and we add one, we allocate anotherscore [INAUDIBLE] d3 and add one to it. And then finally,the d4 gets a 5, because the term information occurred fivetimes in this document. Okay, so this completes the processing ofall the entries in the invert index for information. It processed all the contributionsof matching information in this four documents. So now, our error will go tothe next that's security. So, we're going to fetch allthe inverted index entries for security. So, in this case, there are three entries, andwe're going to go through each of them. The first is d2 and 3 and that means security occur threehumps in d2 and what do we do? Well, we do exactly the same,as what we did for information. So, this time we're going to change thescore [INAUDIBLE] d2 since it's already allocated andwhat we do is we'll add 3 to the existing value which is a 4, sowe now get a 7 for d2. D2 score is increased because the matchthat falls the information and the security. Go to the next entry, that's d4 and1, so we would the score for d4 and again, we add 1 to d4 sod4 goes from 5 to 6. Finally, we process d5 and a 3. Since we have not yet allocated a scoreaccumulated for d5, at this point, we're going to allocate 1 for d5,and we're going to add a 3 to it. So, those scores, of the last rule,are the final scores for these documents. If our scoring function is justa simple some of TF values. Now, what if we, actually,would like to do form addition? Well, we going to do the [INAUDIBLE]at this point, for each document. So, to summarize this,all right, so you can see, we first process the informationdetermine query term information and we processed all the entriesin what index for this term. Then we process the security,all right, its worst think about what should be the order of processinghere when we can see the query terms? It might make a difference especiallyif we don't want to keep all the score accumulators. Let's say, we only want to keepthe most promising score accumulators. What do you think would bea good order to go through? Would you process a common term first orwould you process a rare term first? The answers is we just go to whoshould process the rare term first. A rare term would match a few documents,and then the score contribution would be higher,because the ideal value would be higher. And then, it allows us to attachthe most diplomacy documents first. So, it helps pruningsome non-promising ones, if we don't need somany documents to be returned to the user. So those are all heuristics forfurther improving the accuracy. Here you can also see how we canincorporate the idea of waiting, right? So they can [INAUDIBLE] when weincorporate [INAUDIBLE] when we process each query time. When we fetch the inverted index wecan fetch the document frequency and then we can compute IDF. Or maybe perhaps the IDF valuehas already been precomputed when we indexed the documents. At that time, we already computedthe IDF value that we can just fetch it, so all these can be done at this time. So that would mean when we processall the entries for information, these words would be adjusted by the sameIDF, which is IDF for information. So this is the basic idea of usinginverted index for fast research and it works well for all kinds offormulas that are of the general form. And this generally,the general form covers actually most state of art retrieval functions. So there are some tricks tofurther improve the efficiency, some general techniquesto encode the caching. This is we just store some results ofpopular queries, so that next time when you see the same query,you simply return the stored results. Similarly, you can also slow the listof inverted index in the memory for a popular term. And if the query term is popular likely, you will soon need to factor the invertedindex for the same term again. So keeping it in the memory would help,and these are general techniques for improving efficiency. We can also keep only the most promisingaccumulators because a user generally doesn't want to examine so many documents. We only need to return highqualities subset of documents that likely are ranked on the top. For that purpose,we can then prune the accumulators. We don't have to storeall the accumulators. At some point, we just keepthe highest value accumulators. Another technique is to do parallelprocessing and that's needed for really process in such a largedata set like the web data set. And you scale up tothe Web-scale really to have the special techniques youdo parallel processing and to distribute the storageof files on machines. So here is a list of some text retrievaltoolkits, it's not a complete list. You can find more informationat this URL on the bottom. And here, I listed your four here,Lucene's one of the most popular toolkits that can support a lot of applications andit has very nice support for applications. You can use it to build a searchengine application very quickly. The downside is that it's notthat easy to extend it, and the algorithms implemented they are alsonot the most advanced algorithms. Lemur or Indri is anothertoolkit that does not have such a nice support webapplication as Lucene but it has many advanced search algorithms andit's also easy to extend. Terrier is yet another toolkitthat also has good support for application capability andsome advanced algorithms. So that's maybe in between Lemur orLucene, or maybe rather combiningthe strands of both, so that's also useful tool kit. MeTA is a toolkit that we will use for the problem assignment andthis is a new toolkit that has a combination of both text retrievalalgorithms and text mining algorithms. And so talking models are implement theyare a number of text analysis algorithms implemented in the toolkit aswell as basic search algorithms. So to summarize all the discussionabout the System Implementation, here are the major takeaway points. Inverted index is the primary datastructure for supporting a search engine and that's the key to enablefaster response to a user's query. And the basic idea is to preprocessthe data as much as we can, and we want to do compressionwhen appropriate. So that we can save disk space andwe can speed up IO and processing of inverted index in general. We talked about how to construct theinvert index when the data can't fit into the memory. And then we talk about faster search usingthat index basically, what's we exploit the invective index to accumulate a scoresfor documents [INAUDIBLE] algorithm. And we exploit the Zipf's law toavoid the touching many documents that don't match any query term and this algorithm can actually supporta wide range of ranking algorithms. So these basic techniqueshave great potential for further scaling up using distributed filesystem, parallel processing, and caching. Here are two additional readings youcan take a look, if you have time and you are interested inlearning more about this. The first one is a classicaltextbook on the efficiency o inverted index andthe compression techniques. And how to,in general feel that the efficient any inputs of the space,overhead and speed. The second one is a newer textbook thathas a nice discussion of implementing and evaluating search engines. [MUSIC]
[SOUND]. This lecture is about some practicalissues that you would have to address in evaluation of text retrieval systems. In this lecture, we will continuethe discussion of evaluation. We'll cover some practicalissues that you have to solve in actual evaluation oftext retrieval systems. So, in order to createthe test collection, we have to create a set of queries. A set of documents anda set of relevance judgments. It turns out that each isactually challenging to create. First, the documents andqueries must be representative. They must represent the real queries andreal documents that the users handle. And we also have to use many queries and many documents in order toavoid a bias of conclusions. For the matching of relevantdocuments with the queries. We also need to ensure that there exists alot of relevant documents for each query. If a query has only one, that'sa relevant option we can actually then. It's not very informative tocompare different methods using such a query because there's notthat much room for us to see difference. So ideally, there should be morerelevant documents in the clatch but yet the queries also should representthe real queries that we care about. In terms of relevance judgments,the challenge is to ensure complete judgments of allthe documents for all the queries. Yet, minimizing human and fault, because we have to use humanlabor to label these documents. It's very labor intensive. And as a result, it's impossible toactually label all the documents for all the queries, especially consideringa giant data set like the web. So this is actually a major challenge,it's a very difficult challenge. For measures, it's also challenging,because we want measures that would accurately reflectthe perceived utility of users. We have to consider carefullywhat the users care about. And then design measures to measure that. If your measure is notmeasuring the right thing, then your conclusion would be misled. So it's very important. So we're going to talk abouta couple of issues here. One is the statistical significance test. And this also is a reason whywe have to use a lot of queries. And the question here is how sure canyou be that observe the difference doesn't simply result fromthe particular queries you choose. So here are some sample results ofaverage position for System A and System B into different experiments. And you can see in the bottom,we have mean average of position. So the mean, if you look at the meanaverage of position, the mean average of positions are exactly the samein both experiments, right? So you can see this is 0.20,this is 0.40 for System B. And again here it's also 0.20 and0.40, so they are identical. Yet, if you look at these exact averagepositions for different queries. If you look at these numbers in detail,you would realize that in one case, you would feel that you can trustthe conclusion here given by the average. In the another case, in the other case,you will feel that, well, I'm not sure. So, why don't you take a look at all thesenumbers for a moment, pause the media. So, if you look at the average,the mean average of position, we can easily, say that well,System B is better, right? So, after all it's 0.40 and this is twice as much as 0.20,so that's a better performance. But if you look at these two experiments,look at the detailed results. You will see that, we've been moreconfident to say that, in the case one, in experiment one. In this case. Because these numbers seem to beconsistently better for System B. Whereas in Experiment 2, we're not surebecause looking at some results like this, after System A is better andthis is another case System A is better. But yet if we look at only average,System B is better. So, what do you think? How reliable is our conclusion,if we only look at the average? Now in this case, intuitively,we feel Experiment 1 is more reliable. But how can we quantitatethe answer to this question? And this is why we need to dostatistical significance test. So, the idea of the statisticalsignificance test is basically to assess the variants acrossthese different queries. If there is a big variance, that means the results could fluctuatea lot according to different queries. Then we should believe that,unless you have used a lot of queries, the results might change if weuse another set of queries. Right, so this is then not so if you have c high variancethen it's not very reliable. So let's look at these resultsagain in the second case. So, here we show two differentways to compare them. One is a sign test wherewe just look at the sign. If System B is better than System A,we have a plus sign. When System A is better wehave a minus sign, etc. Using this case, if you see this,well, there are seven cases. We actually have four caseswhere System B is better. But three cases of System A is better,intuitively, this is almost like a random results,right? So if you just take a randomsample of you flip seven coins and if you use plus to denote the head andminus to denote the tail and that could easily be the results of justrandomly flipping these seven coins. So, the fact that the average islarger doesn't tell us anything. We can't reliably conclude that. And this can be quantitativelymeasured by a p value. And that basically means the probability that this result isin fact from a random fluctuation. In this case, probability is 1.0. It means it surely isa random fluctuation. Now in Willcoxan test,it's a non-parametric test, and we would be not onlylooking at the signs, we'll be also looking atthe magnitude of the difference. But we can draw a similar conclusion, where you say it's verylikely to be from random. To illustrate this, let's thinkabout that such a distribution. And this is called a now distribution. We assume that the mean is zero here. Lets say we started withassumption that there's no difference between the two systems. But we assume that because of randomfluctuations depending on the queries, we might observe a difference. So the actual difference mightbe on the left side here or on the right side here, right? So, and this curve kind of showsthe probability that we will actually observe values thatare deviating from zero here. Now, so if we look at this picture then,we see that if a difference is observed here, then the chance is very high that this isin fact a random observation, right? We can define a region oflikely observation because of random fluctuation andthis is that 95% of all the outcomes. And in this then the observed maystill be from random fluctuation. But if you observe a value in thisregion or a difference on this side, then the difference is unlikelyfrom random fluctuation. All right, so there's a very smallprobability that you are observe such a difference just becauseof random fluctuation. So in that case, we can then concludethe difference must be real. So System B is indeed better. So this is the idea ofStatical Significance Test. The takeaway message here is that youhave to use many queries to avoid jumping into a conclusion. As in this case,to say System B is better. There are many different ways of doingthis Statistical Significance Test. So now, let's talk about the otherproblem of making judgments and, as we said earlier,it's very hard to judge all the documents completely unless it'sa very small data set. So the question is,if we can afford judging all the documents in the collection,which is subset should we judge? And the solution here is Pooling. And this is a strategy that has been usedin many cases to solve this problem. So the idea of Pooling is the following. We would first choose a diverseset of ranking methods. These are Text Retrieval systems. And we hope these methods can help usnominate like the relevant documents. So the goal is to pick outthe relevant documents. We want to make judgements on relevantdocuments because those are the most useful documents from users perspectives. So then we're going to haveeach to return top-K documents. The K can vary from systems. But the point is to ask them to suggestthe most likely relevant documents. And then we simply combineall these top-K sets to form a pool of documents forhuman assessors. To judge, so imagine you have manysystems each were ten k documents. We take the top-K documents,and we form a union. Now, of course, there are manydocuments that are duplicated because many systems might have retrievedthe same random documents. So there will be some duplicate documents. And there are also unique documentsthat are only returned by one system. So the idea of having diverse set of ranking methods is toensure the pool is broad. And can include as many possiblerelevant documents as possible. And then, the users would,human assessors would make complete the judgments on this data set, this pool. And the other unjudged the documents areusually just assumed to be non relevant. Now if the pool is large enough,this assumption is okay. But if the pool is not very large,this actually has to be reconsidered. And we might use otherstrategies to deal with them and there are indeed othermethods to handle such cases. And such a strategy is generally okay for comparing systems thatcontribute to the pool. That means if you participatein contributing to the pool, then it's unlikely that itwould penalize your system because the problematicdocuments have all been judged. However, this is problematic for evaluating a new system that mayhave not contributed to the pool. In this case, a new system mightbe penalized because it might have nominated some read only documentsthat have not been judged. So those documents might beassumed to be non relevant. That's unfair. So to summarize the whole part of textualevaluation, it's extremely important. Because the problem is the empiricallydefined problem, if we don't rely on users, there's no way totell whether one method works better. If we have in the propertyexperiment design, we might misguide our research orapplications. And we might just draw wrong conclusions. And we have seen this isin some of our discussions. So make sure to get it right foryour research or application. The main methodology is the Cranfieldevaluation methodology. And they are the main paradigm used inall kinds of empirical evaluation tasks, not just a search engine variation. Map and nDCG are the two mainmeasures that you should definitely know about and they are appropriate forcomparing ranking algorithms. You will see them oftenin research papers. Precision at 10 documents is easierto interpret from user's perspective. So that's also often useful. What's not covered is some otherevaluation strategy like A-B Test. Where the system would mix two,the results of two methods, randomly. And then would showthe mixed results to users. Of course, the users don't seewhich result, from which method. The users would judge those results or click on those documents ina search engine application. In this case then, the search enginecan check or click the documents and see if one method has contributedmore through the click the documents. If the user tends to click on one,the results from one method, then it suggests thatmessage may be better. So this is what leverages the real usersof a search engine to do evaluation. It's called A-B Test andit's a strategy that is often used by the modern search engines orcommercial search engines. Another way to evaluate IR or textual retrieval is user studies andwe haven't covered that. I've put some references herethat you can look at if you want to know more about that. So, there are threeadditional readings here. These are three mini books aboutevaluation and they are all excellent in covering a broad review ofInformation Retrieval Evaluation. And it covers some of the thingsthat we discussed, but they also have a lot of others to offer. [MUSIC]
[SOUND] This lecture is about the specific smoothing methods for language modelsused in probabilistic retrieval model. In this lecture, we will continuethe discussion of language models for information retrieval, particularlythe query likelihood retrieval method. And we're going to talk about specificallythe smoothing methods used for such a retrieval function. So this is a slide from a previouslecture where we show that with a query likelihood ranking and smoothingwith the collection language model, we add up having a retrieval functionthat looks like the following. So this is the retrieval function based onthese assumptions that we have discussed. You can see it's a sum of allthe matching query terms, here. And inside its sum is the countof the term in the query and some weight for the term in the document. We have t of i, the f weight here, andthen we have another constant here in n. So clearly if we want to implement thisfunction using programming language, we still need to figureout a few variables. In particular, we're going to need toknow how to estimate the probability of a word exactly and how do we set alpha. So in order to answer this question,we have to think about very specific smoothing methods, andthat is main topic of this lecture. We're going to talk abouttwo smoothing methods. The first is simple linearinterpolation with a fixed coefficient. And this is also calleda Jelinek-Mercer smoothing. So the idea is actually very simple. This picture shows howwe estimate a document language model by usingmaximum likelihood estimate. That gives us word counts normalized bythe total number of words in the text. The idea of using this method is to maximize the probabilityof the observed text. As a result,if a word like network is not observed in the text, it's going to get0 probability, as shown here. So the idea of smoothing, then,is to rely on collection language model where this word is not going to havea zero probability to help us decide what nonzero probability shouldbe assigned to such a word. So we can note that network hasa nonzero probability here. So in this approach what we do is we doa linear interpolation between the maximum likelihood placement here andthe collection language model, and this is computed by the smoothing parameterlambda, which is between 0 and 1. So this is a smoothing parameter. The larger lambda is,the more smoothing we will have. So by mixing them together, we achieve the goal of assigning nonzeroprobabilities to a word like network. So let's see how it works forsome of the words here. For example, if we computethe smooth probability for text. Now the maximum likelihoodestimated gives us 10 over 100, and that's going to be here. But the collection probability is this. So we'll just combine themtogether with this simple formula. We can also see the word network,which used to have a zero probability, now is getting a non-zeroprobability of this value. And that's because the count isgoing to be zero for network here. But this part is nonzero, andthat's basically how this method works. Now if you think about this andyou can easily see now the alpha sub d in this smoothingmethod is basically lambda. Because that's remember the coefficientin front of the probability of the word given by the collectionlanguage model here. Okay, sothis is the first smoothing method. The second one is similar butit has a tie-in into the coefficient for linear interpolation. It's often called Dirichlet Prior,or Bayesian, Smoothing. So again here we face problemof zero probability for an unseen word like network. Again we will use the collectionlanguage model, but in this case, we're going to combine themin somewhat different ways. The formula first can be seen asa interpolation of the maximum likelihood estimate andthe collection language model as before, as in the J-M smoothing method. Only that the coefficient nowis not lambda, a fixed number, but a dynamic coefficient in this form, where mu is a parameter,it's a non-negative value. And you can see if weset mu to a constant, the effect is that a long document wouldactually get a smaller coefficient here. Because a long documentwill have longer lengths, therefore the coefficientis actually smaller. And so a long document would haveless smoothing, as we would expect. So this seems to make more sensethan a fixed coefficient smoothing. Of course,this part would be of this form so that the two coefficients would sum to 1. Now this is one way tounderstand this smoothing. Basically, it means it's a dynamiccoefficient interpolation. There is another way to understandthis formula which is even easier to remember, andthat's on this side. So it's easier to see how we can rewritethe smoothing method in this form. Now in this form we can easilysee what change we have made to the maximum likelihood estimate,which would be this part. So normalize the countby the document length. So in this form we can see what we did iswe add this to the count of every word. So what does this mean? Well, this is basically something relatedto the probability of the word in the collection. And we multiply that by the parameter mu. And when we combine thiswith the count here, essentially we are addingpseudocounts to the observed text. We pretend every word hasgot this many pseudocount. So the total count would bethe sum of these pseudocounts and the actual count ofthe word in the document. As a result, in total we wouldhave added this many pseudocounts. Why?Because if you take somewhat this one over all the words, then we'll see theprobability of the words would sum to 1, and that gives us just mu. So this is the total number ofpseudocounts that we added. And sothese probabilities would still sum to 1. So in this case, we can easilysee the method is essentially to add this as a pseudocount to this data. Pretend we actually augment the databy including some pseudo data defined by the collection language model. As a result, we have more counts is that the total counts fora word would be like this. And as a result, even if a word has zerocount here, let's say if we have zero count here, then it would still havenonzero count because of this part. So this is how this method works. Let's also take a look atsome specific example here. So for text again we willhave 10 as the original count that we actually observe, butwe also add some pseudocount. And so the probability oftext would be of this form. Naturally, the probability ofnetwork would be just this part. And so here you can also seewhat's alpha sub d here. Can you see it? If you want to think about it,you can pause the video. But you'll notice that thispart is basically alpha sub d. So we can see, in this case, alpha sub d does depend on the document, because this lengthdepends on the document, whereas in the linear interpolation, the J-M smoothing method,this is a constant. [MUSIC]
[SOUND] This lecture is about link analysis for web search. In this lecture, we're going to talkabout the web search and particularly, focusing on how to do link analysis anduse the results to improve search. The main topic of this lecture is to lookat the ranking algorithms for Web Search. In the previous lecture we talkedabout how to create index. Now that we have index, we want to seehow we can improve ranking of Pages. The web. Now standard IR models,can be also applied here. In fact,they are important building blocks, for, improve, for supporting web search. But they aren't sufficient. And mainly for the following reasons. First, on the web, we tend to havevery different information needs, for example, people might search fora webpage, or an entry page. And this is different fromthe traditional library search, where people are primarily interestedin collecting literature Information. So this kind of query is oftencalled a navigational queries. The purpose is to navigate intoa particular type of the page. So for such queries we might benefitfrom using link information. Secondly, documents have additionalinformation and on the web pages, are web format,there are a lot of other clues, such as the layout, the title,or link information again. So this has provided opportunity to use extra context information ofthe document to improve the scoring. And finally,information quality varies a lot. That means we have to considermany factors to improve the range in the algorithm. This would give us a more robust wayto rank pages, making it harder for any spammer to just manipulate the onesignal to improve the ranking of a page. So as a result, people have made a number of majorextensions to the ranking algorithms. One line is to exploitlinks to improve scoring. And that's the main topic of this lecture. People have also proposed algorithms toexploit the loudest, they are implicit. Feedback information the form ofclick throughs and that's of course in the category of feedback techniques andmachine all is often used there. In general in web search the rankingalgorithms are based on machine learning algorithms to combineall kinds of features. Many of them are based onthe standard of virtual models such as BM25 that we talked about [INAUDIBLE]to score different parts of documents or to provide additional featuresbased on content matching, but link informationis also very useful so they provide additional scoring signals. So let's look at links inmore detail on the web. So this is a snapshot of somepart of the web, let's say. So we can see there are many links thatlink the different pages together. And in this case, you can alsolook at the center here, there is a description of a link that's pointingto the document on the right side. Now, this description textis called anchor text. Now if you think about this text,it's actually quite useful because it provides some extradescription of that page be points with. So for example, if someone wantsto bookmark Amazon.com front page the person might say the biggestonline bookstore and then the link to Amazon, right? So, the description here after is verysimilar to what the user would type in the query box when they are looking foror such a page. And that's why it's very useful formanaging pages. Suppose someone types inthe query like online bookstore or biggest online bookstore. All right the query would matchthis anchor text in the page here. And then this actuallyprovides evidence for matching the page that's beingpointed to that is the Amazon. a entry page. So if you match anchor text thatdescribes an anchor to a page, actually that provides good evidence forthe elements of the page being pointed to. So anchor text is very useful. If you look at the bottom part of thispicture you can also see there are some patterns of some links and these linksmight indicate the utility of a document. So for example, on the right side you'll see thispage has received the many inlinks. Now that means many other pagesare pointing to this page. This shows that this page is quite useful. On the left side you can see thisis another page that points to many other pages. So this is a director pagethat would allow you to actually see a lot of other pages. So we can call the firstcase authority page and the second case half page, but this meansthe link information can help intuit. One is to provide extra text for matching. The other is to provide someadditional scores for the webpage to characterize how likely a page isa hub, how likely a page is a authority. So people then of course and proposedideas to leverage this link information. Google's PageRank which was the maintechnique that they used in early days is a good example andthat is an algorithm to capture page and popularity, basically to score authority. So the intuitions here are linksare just like citations in literature. Now think about one pagepointing you to another page, this is very similar to onepaper citing another paper. So, of course then,if a page is cited often, then we can assume this pageto be more useful in general. So that's a very good intuition. Now PageRank is essentially to takeadvantage of this Intuition to implement with the principal approach. Intuitively, it is essentially doingcitation counting or in link counting. It just improves the simpleidea in two ways. One it will consider indirect citations. So that means you don't just lookat how many in links you have. You also look at what are thosepages that are pointing to you. If those pages themselves have a lotof in-links, that means a lot. In some sense,you will get some credit from that. But if those pages thatare pointing to you are not being pointed to by other pages theythemselves don't have many in-links, then well, you don't get that much. So that's the idea ofgetting indirect citation. All right, so you can also understand this idea bylooking at again the research papers. If you're cited by let's say ten papers,and those ten papers are just workshop papers or some papersthat are not very influential, right? So although you've got ten in-links,and that's not as good as if you are cited by ten papers that themselveshave attracted a lot of other citations. And so in this case where we wouldlike to consider indirect links and page does that. The other idea is it'sgood to pseudo citations. Assume that basically every page is havinga number zero pseudo citation count. Essentially you are trying toimagine there are many virtual links that will link allthe pages together so that you actually get the pseudocitations from everyone. The reason why they want to do that. Is this will allow themto solve the problem elegantly with linear algebra technique. So, I think maybe the bestway to understand the PageRank is to thinkof this as through computer probability of random surfervisiting every webpage. [MUSIC]
[SOUND] There are some interesting challenges in threshold forthe learning the filtering problem. So here I show the historical data thatyou can collect in the filtering system, so you can see the scores andthe status of relevance. So the first one has a score of 36.5 andit's relevant. The second one is not relevant andit's separate. Of course, we have a lot of documents forwhich we don't know the status, because we have neverdelivered them to the user. So as you can see here, we only see the judgements ofdocuments delivered to the user. So this is not a random sample,so it's a sensitive data. It's kind of biased, so that createssome difficultly for learning. Secondly, there are in general very littlelabeled data and very few relevant data, so it's also challenging formachine learning approaches, typically they require more training data. And in the extreme case atthe beginning we don't even have any labeled data as well. The system there has to make a decision, so that's a very difficultproblem at the beginning. Finally, there is also this issue ofexploration versus exploitation tradeoff. Now, this means we also wantto explore the document space a little bit andto see if the user might be interested in documents thatwe have in data labeled. So in other words, we're going toexplore the space of user interests by testing whether the user might beinterested in some other documents that currently are not matchingthe user's interests so well. So how do we do that? Well, we could lower the thresholda little bit until we just deliver some near misses to the userto see what the user would respond, to see how the user wouldrespond to this extra document. And this is a tradeoff, because onthe one hand, you want to explore, but on the other hand,you don't want to really explore too much, because then you will overdeliver non-relevant Information. So exploitation means you wouldexploit what you learn about the user. Let's say you know the user isinterested in this particular topic, so you don't want to deviate that much, but if you don't deviate at all then you don'texploit so that's also are not good. You might miss opportunity to learnanother interest of the user. So this is a dilemma. And that's also a difficultyproblem to solve. Now, how do we solve these problems? In general, I think one can use theempirical utility optimization strategy. And this strategy is basically to optimizethe threshold based on historical data, just as you have seenon the previous slide. Right, so you can just computethe utility on the training data for each candidate score threshold. Pretend that, what if I cut at this point. What if I cut at the different scoringthreshold point, what would happen? What's utility? Since these are training data,we can kind of compute the utility, and we know that relevant status,or we assume that we know relevant status based onapproximation of click-throughs. So then we can just choose the thresholdthat gives the maximum utility on the training data. But this of course, doesn't account forexploration that we just talked about. And there is also the difficulty ofbiased training sample, as we mentioned. So, in general, we can only get the upperbound for the true optimal threshold, because the threshold mightbe actually lower than this. So, it's possible that this coulddiscarded item might be actually interesting to the user. So how do we solve this problem? Well, we generally, and as I said we can low with thisthreshold to explore a little bit. So here's on particular approachcalled beta-gamma threshold learning. So the idea is falling. So here I show a ranked list of all thetraining documents that we have seen so far, andthey are ranked by their positions. And on the y axis we show the utility,of course, this function depends on how you specify the coefficientsin the utility function, but we can then imagine, that depending on thecutoff position, we will have a utility. Suppose I cut at this position andthat would be a utility. For example,identify some cutting cutoff point. The optimal point,theta optimal, is the point when it will achieve the maximum utilityif we had chosen this as threshold. And there is also zero utility threshold. You can see at this cutoffthe utility is zero. What does that mean? That means if I lower the thresholda little bit, now I reach this threshold. The utility would be lower butit's still non-active at least, right? So it's not as high asthe optimal utility. But it gives us as a safe pointto explore the threshold, as I have explained, it's desirableto explore the interest of space. So it's desirable to lower the thresholdbased on your training there. So that means, in general, we want to setthe threshold somewhere in this range. Let's say we can use the alpha to control the deviation fromthe optimal utility point. So you can see the formula of thethreshold would be just the interpolation of the zero utility threshold andthe optimal utility threshold. Now, the question is,how should we set alpha? And when should we deviate morefrom the optimal utility point? Well, this can depend on multiple factors,and the one way to solve the problem is to encourage this thresholdmechanism to explore up to the zero point, andthat's a safe point, but we're not going to necessarily reachall the way to the zero point. Rather, we're going to use otherparameters to further define alpha and this specifically is as follows. So there will be a beta parameter tocontrol the deviation from the optimal threshold and this can be based on canbe accounting for the over-fitting to the training data let's say, and sothis can be just an adjustment factor. But what's more interestingis this gamma parameter. Here, and you can see in this formula, gamma is controlling the inference of the number of examplesin training that are set. So you can see in this formula as N whichdenotes the number of training examples becomes bigger, then it wouldactually encourage less exploration. In other words, when these verysmall it would try to explore more. And that just means if we have seen few examples we're not sure whether wehave exhausted the space of interest. So we need to explore but as we haveseen many examples from the user many that have we feel that weprobably don't have to explore more. So this gives us a beta gamma forexploration, right. The more examples we have seenthe less exploration we need to do. So the threshold would be closerto the optimal threshold so that's the basic idea of this approach. This approach actually has been workingwell in some evaluation studies, particularly effective. And also can work on arbitrary utilitywith the appropriate lower bound. And explicitly addressesthe exploration-exploitation tradeoff and it kind of uses the zero utilitythreshold point as a safeguard for exploration-exploitation tradeoff. We're not never going to explorefurther than the zero utility point. So if you take the analogy of gambling,and you don't want to risk on losing money. So it's a safe spend, reallyconservative strategy for exploration. And the problem is of course,this approach is purely heuristic and the zero utility lower boundary is alsooften too conservative, and there are, of course, more advance in machine learningapproaches that have been proposed for solving this problems andthis is their active research area. So to summarize, there are twostrategies for recommended systems or filtering systems, one is content based,which is looking at the item similarity, and the other is collaborative filteringthat was looking at the user similarity. We've covered content-basedfiltering approach. In the next lecture, we will talkabout the collaborative filtering. In content-based filtering system,we generally have to solve several problems relative tofiltering decision and learning, etc. And such a system can actually bebuilt based on a search engine system by adding a threshold mechanism andadding adaptive learning algorithm to allow the system to learn fromlong term feedback from the user. [MUSIC]
Office hours will be held by the instructor and TAs each week using Zoom. All times are given in US Central Time and using a 24-hour clock ( [ Time Zone Converter ](http://www.thetimezoneconverter.com/) ). See below for specific days and times.  **Instructor: Professor Cheng Zhai**  Tuesdays: 8-9 pm (CT)  **Weekly TA Office Hours:**  TA  |  Day  |  Time (CT)      ---|---|---      Junting  |  Monday  |  8 - 9 pm      Yunan  |  Wednesday  |  8 - 9 pm      Ziyuan  |  Thursday  |  8 - 9 pm      Richa  |  Friday  |  9 - 10 am      Yuxiang  |  Friday  |  8 - 9 pm      Yuchen  |  |      ###  **Office Hours Recordings:**  You can find the recordings here: [ https://www.coursera.org/learn/cs-410/resources/nOxcB ](https://www.coursera.org/learn/cs-410/resources/nOxcB)  
[SOUND] This lecture is about the word association mining and analysis. In this lecture,we're going to talk about how to mine associations of words from text. Now this is an example of knowledgeabout the natural language that we can mine from text data. Here's the outline. We're going to first talk aboutwhat is word association and then explain why discovering suchrelations is useful and finally we're going to talk about some generalideas about how to mine word associations. In general there are two wordrelations and these are quite basic. One is called a paradigmatic relation. The other is syntagmatic relation. A and B have paradigmatic relation if they can be substituted for each other. That means the two words thathave paradigmatic relation would be in the same semantic class,or syntactic class. And we can in generalreplace one by the other without affectingthe understanding of the sentence. That means we would stillhave a valid sentence. For example, cat and dog, these twowords have a paradigmatic relation because they are inthe same class of animal. And in general,if you replace cat with dog in a sentence, the sentence would still be a validsentence that you can make sense of. Similarly Monday andTuesday have paradigmatical relation. The second kind of relation iscalled syntagmatical relation. In this case, the two words that have thisrelation, can be combined with each other. So A and B have syntagmatic relation ifthey can be combined with each other in a sentence, that means these twowords are semantically related. So for example, cat and sit are relatedbecause a cat can sit somewhere. Similarly, car anddrive are related semantically and they can be combined witheach other to convey meaning. However, in general, we can notreplace cat with sit in a sentence or car with drive in the sentenceto still get a valid sentence, meaning that if we do that, the sentencewill become somewhat meaningless. So this is different fromparadigmatic relation. And these two relations are in fact sofundamental that they can be generalized to capture basic relationsbetween units in arbitrary sequences. And definitely they can begeneralized to describe relations of any items in a language. So, A and B don't have to be words andthey can be phrases, for example. And they can even be more complexphrases than just a non-phrase. If you think about the generalproblem of the sequence mining then we can think about the unitsbeing and the sequence data. Then we think of paradigmaticrelation as relations that are applied to units that tend to occurin a singular locations in a sentence, or in a sequence of dataelements in general. So they occur in similar locationsrelative to the neighbors in the sequence. Syntagmatical relation onthe other hand is related to co-occurrent elements that tendto show up in the same sequence. So these two are complimentary andare basic relations of words. And we're interested in discoveringthem automatically from text data. Discovering such wordedrelations has many applications. First, such relations can be directlyuseful for improving accuracy of many NLP tasks, and this is because this is partof our knowledge about a language. So if you know these two wordsare synonyms, for example, and then you can help a lot of tasks. And grammar learning can be alsodone by using such techniques. Because if we can learnparadigmatic relations, then we form classes of words,syntactic classes for example. And if we learn syntagmatic relations,then we would be able to know the rules for putting together a largerexpression based on component expressions. So we learn the structure andwhat can go with what else. Word relations can be also very useful for many applications in text retrieval andmining. For example, in search andtext retrieval, we can use word associations to modify a query,and this can be used to introduce additional related words intoa query and make the query more effective. It's often called a query expansion. Or you can use related words tosuggest related queries to the user to explore the information space. Another application is touse word associations to automatically construct the topof the map for browsing. We can have words as nodes andassociations as edges. A user could navigate fromone word to another to find information in the information space. Finally, such word associations can alsobe used to compare and summarize opinions. For example, we might be interestedin understanding positive and negative opinions about the iPhone 6. In order to do that, we can look at whatwords are most strongly associated with a feature word like battery inpositive versus negative reviews. Such a syntagmaticalrelations would help us show the detailed opinionsabout the product. So, how can we discover suchassociations automatically? Now, here are some intuitionsabout how to do that. Now let's first look atthe paradigmatic relation. Here we essentially can takeadvantage of similar context. So here you see some simplesentences about cat and dog. You can see they generallyoccur in similar context, and that after all is the definitionof paradigmatic relation. On the right side you can kindof see I extracted expressly the context of cat anddog from this small sample of text data. I've taken away cat anddog from these sentences, so that you can see just the context. Now, of course we can have differentperspectives to look at the context. For example, we can look atwhat words occur in the left part of this context. So we can call this left context. What words occur before we see cat or dog? So, you can see in this case, clearlydog and cat have similar left context. You generally say his cat or my cat andyou say also, my dog and his dog. So that makes them similarin the left context. Similarly, if you look at the wordsthat occur after cat and dog, which we can call right context,they are also very similar in this case. Of course, it's an extreme case,where you only see eats. And in general,you'll see many other words, of course, that can't follow cat and dog. You can also even lookat the general context. And that might include allthe words in the sentence or in sentences around this word. And even in the general context, you alsosee similarity between the two words. So this was just a suggestionthat we can discover paradigmatic relation by looking atthe similarity of context of words. So, for example,if we think about the following questions. How similar are context of cat andcontext of dog? In contrast how similar are contextof cat and context of computer? Now, intuitively,we're to imagine the context of cat and the context of dog wouldbe more similar than the context of cat andcontext of the computer. That means, in the first casethe similarity value would be high, between the context of cat anddog, where as in the second, the similarity between context of cat andcomputer would be low because they all not having a paradigmatic relationship and imagine what wordsoccur after computer in general. It would be very different fromwhat words occur after cat. So this is the basic idea of whatthis covering, paradigmatic relation. What about the syntagmatic relation? Well, here we're going to explorethe correlated occurrences, again based on the definitionof syntagmatic relation. Here you see the same sample of text. But here we're interested in knowingwhat other words are correlated with the verb eats andwhat words can go with eats. And if you look at the rightside of this slide and you see,I've taken away the two words around eats. I've taken away the word to its left and also the word to itsright in each sentence. And then we ask the question, what wordstend to occur to the left of eats? And what words tend tooccur to the right of eats? Now thinking about this questionwould help us discover syntagmatic relations because syntagmatic relationsessentially captures such correlations. So the important question to ask forsyntagmatical relation is, whenever eats occurs,what other words also tend to occur? So the question here hasto do with whether there are some other words that tendto co-occur together with each. Meaning that whenever you see eatsyou tend to see the other words. And if you don't see eats, probably,you don't see other words often either. So this intuition can helpdiscover syntagmatic relations. Now again, consider example. How helpful is occurrence of eats forpredicting occurrence of meat? Right.All right, so knowing whether eats occurs in a sentence would generally help uspredict whether meat also occurs indeed. And if we see eats occur in the sentence,and that should increase the chancethat meat would also occur. In contrast,if you look at the question in the bottom, how helpful is the occurrence of eats forpredicting of occurrence of text? Because eats andtext are not really related, so knowing whether eats occurredin the sentence doesn't really help us predict the weather,text also occurs in the sentence. So this is in contrast tothe question about eats and meat. This also helps explain that intuition behind the methods of whatdiscovering syntagmatic relations. Mainly we need to capture the correlationbetween the occurrences of two words. So to summarize the general ideas for discovering word associationsare the following. For paradigmatic relation,we present each word by its context. And then compute its context similarity. We're going to assume the wordsthat have high context similarity to have paradigmatic relation. For syntagmatic relation, we will counthow many times two words occur together in a context, which can be a sentence,a paragraph, or a document even. And we're going to compare their co-occurrences withtheir individual occurrences. We're going to assume wordswith high co-occurrences but relatively low individual occurrencesto have syntagmatic relations because they attempt to occur together andthey don't usually occur alone. Note that the paradigmatic relation andthe syntagmatic relation are actually closely relatedin that paradigmatically related words tend to have syntagmaticrelation with the same word. They tend to be associatedwith the same word, and that suggests that we can also do jointhe discovery of the two relations. So these general ideas can beimplemented in many different ways. And the course won't cover all of them,but we will cover at least some ofthe methods that are effective for discovering these relations. [MUSIC]
This lecture is about Probabilistic TopicModels for topic mining and analysis. In this lecture, we're going to continue talkingabout the topic mining and analysis. We're going to introduceprobabilistic topic models. So this is a slide thatyou have seen earlier, where we discussed the problemswith using a term as a topic. So, to solve these problemsintuitively we need to use more words to describe the topic. And this will address the problemof lack of expressive power. When we have more words that wecan use to describe the topic, that we can describe complicated topics. To address the second problem weneed to introduce weights on words. This is what allows you to distinguishsubtle differences in topics, and to introduce semanticallyrelated words in a fuzzy manner. Finally, to solve the problem ofword ambiguity, we need to split ambiguous word, sothat we can disambiguate its topic. It turns out that all these can be doneby using a probabilistic topic model. And that's why we're going to spend a lotof lectures to talk about this topic. So the basic idea here is that, improve the replantation oftopic as one distribution. So what you see now isthe older replantation. Where we replanted each topic, it was justone word, or one term, or one phrase. But now we're going to use a worddistribution to describe the topic. So here you see that for sports. We're going to usethe word distribution over theoretical speaking allthe words in our vocabulary. So for example, the highprobability words here are sports, game, basketball,football, play, star, etc. These are sports related terms. And of course it would also givea non-zero probability to some other word like Trouble which might berelated to sports in general, not so much related to topic. In general we can imagine a nonzero probability for all the words. And some words that are not read andwould have very, very small probabilities. And these probabilities will sum to one. So that it forms a distributionof all the words. Now intuitively, this distributionrepresents a topic in that if we assemble words from the distribution, we tendedto see words that are ready to dispose. You can also see, as a very special case,if the probability of the mass is concentrated in entirely onjust one word, it's sports. And this basically degeneratesto the symbol foundation of a topic was just one word. But as a distribution,this topic of representation can, in general,involve many words to describe a topic and can model several differencesin semantics of a topic. Similarly we can model Travel and Sciencewith their respective distributions. In the distribution for Travel we see topwords like attraction, trip, flight etc. Whereas in Science we see scientist,spaceship, telescope, or genomics, and, you know,science related terms. Now that doesn't mean sports related terms will necessarily have zeroprobabilities for science. In general we can imagine all of thesewords we have now zero probabilities. It's just that for a particulartopic in some words we have very, very small probabilities. Now you can also see there are somewords that are shared by these topics. When I say shared it just means evenwith some probability threshold, you can still see one wordoccurring much more topics. In this case I mark them in black. So you can see travel, for example,occurred in all the three topics here, but with different probabilities. It has the highest probability forthe Travel topic, 0.05. But with much smaller probabilities forSports and Science, which makes sense. And similarly, you can see a Staralso occurred in Sports and Science with reasonablyhigh probabilities. Because they might be actuallyrelated to the two topics. So with this replantation it addresses thethree problems that I mentioned earlier. First, it now uses multiplewords to describe a topic. So it allows us to describea fairly complicated topics. Second, it assigns weights to terms. So now we can model severaldifferences of semantics. And you can bring in relatedwords together to model a topic. Third, because we have probabilities forthe same word in different topics, we can disintegrate the sense of word. In the text to decodeit's underlying topic, to address all these three problems withthis new way of representing a topic. So now of course our problem definitionhas been refined just slightly. The slight is very similar to whatyou've seen before except we have added refinement for what our topic is. Now each topic is word distribution,and for each word distribution we know that all the probabilities should sum toone with all the words in the vocabulary. So you see a constraint here. And we still have another constrainton the topic coverage, namely pis. So all the Pi sub ij's must sum to one forthe same document. So how do we solve this problem? Well, let's look at this problemas a computation problem. So we clearly specify it's input and output andillustrate it here on this side. Input of course is our text data. C is our collection but we also generallyassume we know the number of topics, k. Or we hypothesize a number andthen try to bind k topics, even though we don't know the exacttopics that exist in the collection. And V is the vocabulary that hasa set of words that determines what units would be treated asthe basic units for analysis. In most cases we'll use wordsas the basis for analysis. And that means each word is a unique. Now the output would consist of as firsta set of topics represented by theta I's. Each theta I is a word distribution. And we also want to know the coverageof topics in each document. So that's. That the same pi ijsthat we have seen before. So given a set of text data we wouldlike compute all these distributions and all these coverages as youhave seen on this slide. Now of course there may be manydifferent ways of solving this problem. In theory, you can write the [INAUDIBLE]program to solve this problem, but here we're going to introduce a general way of solving thisproblem called a generative model. And this is, in fact,a very general idea and it's a principle way of using statisticalmodeling to solve text mining problems. And here I dimmed the picturethat you have seen before in order to show the generation process. So the idea of this approach is actuallyto first design a model for our data. So we design a probabilistic modelto model how the data are generated. Of course,this is based on our assumption. The actual data aren'tnecessarily generating this way. So that gave us a probabilitydistribution of the data that you are seeing on this slide. Given a particular model andparameters that are denoted by lambda. So this template of actually consists of all the parameters thatwe're interested in. And these parameters in generalwill control the behavior of the probability risk model. Meaning that if you set theseparameters with different values and it will give some data pointshigher probabilities than others. Now in this case of course,for our text mining problem or more precisely topic mining problemwe have the following plans. First of all we have theta i's whichis a word distribution snd then we have a set of pis for each document. And since we have n documents, so we haven sets of pis, and each set the pi up. The pi values will sum to one. So this is to say that wefirst would pretend we already have these word distributions andthe coverage numbers. And then we can see how we can generatedata by using such distributions. So how do we model the data in this way? And we assume that the dataare actual symbols drawn from such a model thatdepends on these parameters. Now one interesting question here is to think about how manyparameters are there in total? Now obviously we can already seen multiplied by K parameters. For pi's. We also see k theta i's. But each theta i is actually a setof probability values, right? It's a distribution of words. So I leave this as an exercise for you to figure out exactly howmany parameters there are here. Now once we set up the model thenwe can fit the model to our data. Meaning that we canestimate the parameters or infer the parameters based on the data. In other words we would like toadjust these parameter values. Until we give our data setthe maximum probability. I just said,depending on the parameter values, some data points will have higherprobabilities than others. What we're interested in, here, is what parameter values will giveour data set the highest probability? So I also illustrate the problemwith a picture that you see here. On the X axis I just illustrate lambda,the parameters, as a one dimensional variable. It's oversimplification, obviously,but it suffices to show the idea. And the Y axis shows the probabilityof the data, observe. This probability obviously dependson this setting of lambda. So that's why it varies as youchange the value of lambda. What we're interested hereis to find the lambda star. That would maximize the probabilityof the observed data. So this would be, then,our estimate of the parameters. And these parameters, note that are precisely what wehoped to discover from text data. So we'd treat these parametersas actually the outcome or the output of the data mining algorithm. So this is the general idea of using a generative model for text mining. First, we design a model withsome parameter values to fit the data as well as we can. After we have fit the data,we will recover some parameter value. We will use the specificparameter value And those would be the outputof the algorithm. And we'll treat those as actuallythe discovered knowledge from text data. By varying the model of course wecan discover different knowledge. So to summarize, we introduceda new way of representing topic, namely representing as word distributionand this has the advantage of using multiple words to describe a complicatedtopic.It also allow us to assign weights on words so we have more thanseveral variations of semantics. We talked about the task of topic mining,and answers. When we define a topic as distribution. So the importer is a clashing of textarticles and a number of topics and a vocabulary set andthe output is a set of topics. Each is a word distribution and also the coverage of allthe topics in each document. And these are formally representedby theta i's and pi i's. And we have two constraints here forthese parameters. The first is the constraintson the worded distributions. In each worded distributionthe probability of all the words must sum to 1,all the words in the vocabulary. The second constraint is onthe topic coverage in each document. A document is not allowed to recovera topic outside of the set of topics that we are discovering. So, the coverage of each of these ktopics would sum to one for a document. We also introduce a general idea of usinga generative model for text mining. And the idea here is, first we're designa model to model the generation of data. We simply assume that theyare generative in this way. And inside the model we embed someparameters that we're interested in denoted by lambda. And then we can infer the mostlikely parameter values lambda star, given a particular data set. And we can then take the lambda star asknowledge discovered from the text for our problem. And we can adjustthe design of the model and the parameters to discover variouskinds of knowledge from text. As you will see laterin the other lectures. [MUSIC]
[SOUND] Sonow let's talk about the exchanging of PLSA to of LDA and to motivate that, we need to talk about somedeficiencies of PLSA. First, it's not really a generative modelbecause we can compute the probability of a new document. You can see why, and that's because thepis are needed to generate the document, but the pis are tied to the documentthat we have in the training data. So we can't compute the pis forfuture document. And there's some heuristic workaround,though. Secondly, it has many parameters, and I'veasked you to compute how many parameters exactly there are in PLSA, andyou will see there are many parameters. That means that model is very complex. And this also means that thereare many local maxima and it's prone to overfitting. And that means it's very hard toalso find a good local maximum. And that we are representingglobal maximum. And in terms of explaining future data,we might find that it will overfit the training databecause of the complexity of the model. The model is so flexible to fit preciselywhat the training data looks like. And then it doesn't allow us to generalizethe model for using other data. This however is not a necessary problemfor text mining because here we're often only interested in hittingthe training documents that we have. We are not always interested in modernfuture data, but in other cases, or if we would care about the generality,we would worry about this overfitting. So LDA is proposing to improve that,and basically to make PLSA a generative model by imposinga Dirichlet prior on the model parameters. Dirichlet is just a special distributionthat we can use to specify product. So in this sense, LDA is justa Bayesian version of PLSA, and the parameters are nowmuch more regularized. You will see there are manyfew parameters and you can achieve the same goal as PLSA fortext mining. It means it can compute the top coverageand topic word distributions as in PLSA. However, there's no. Why are the parameters forPLSA here are much fewer, there are fewer parameters andin order to compute a topic coverage and word distributions,we again face a problem of influence of these variables becausethey are not parameters of the model. So the influence part againface the local maximum problem. So essentially they are doing somethingvery similar, but theoretically, LDA is a more elegant way of lookingat the top and bottom problem. So let's see how we cangeneralize the PLSA to LDA or a standard PLSA to have LDA. Now a full treatment of LDA isbeyond the scope of this course and we just don't have time to go indepth on that talking about that. But here, I just want to give youa brief idea about what's extending and what it enables, all right. So this is the picture of LDA. Now, I remove the backgroundof model just for simplicity. Now, in this model, all theseparameters are free to change and we do not impose any prior. So these word distributions are nowrepresented as theta vectors. So these are word distributions, so here. And the other set of parameters are pis. And we would present it as a vector also. And this is more convenientto introduce LDA. And we have one vector for each document. And in this case, in theta,we have one vector for each topic. Now, the difference between LDA and PLSA is that in LDA, we're not goingto allow them to free the chain. Instead, we're going to force them tobe drawn from another distribution. So more specifically, they will be drawn from two Dirichletdistributions respectively, but the Dirichlet distribution isa distribution over vectors. So it gives us a probability offour particular choice of a vector. Take, for example, pis, right. So this Dirichlet distribution tellsus which vectors of pi is more likely. And this distribution in itself iscontrolled by another vector of parameters of alphas. Depending on the alphas, we cancharacterize the distribution in different ways but with full certain choices ofpis to be more likely than others. For example, you might favor the choice of a relativelyuniform distribution of all the topics. Or you might favor generatinga skewed coverage of topics, and this is controlled by alpha. And similarly here, the topic orword distributions are drawn from another Dirichletdistribution with beta parameters. And note that here,alpha has k parameters, corresponding to our inference onthe k values of pis for our document. Whereas here, beta has n values corresponding tocontrolling the m words in our vocabulary. Now once we impose this price, thenthe generation process will be different. And we start with joined pis from the Dirichlet distribution andthis pi will tell us these probabilities. And then, we're going to use the pito further choose which topic to use, and this is of coursevery similar to the PLSA model. And similar here, we're not goingto have these distributions free. Instead, we're going to draw onefrom the Dirichlet distribution. And then from this,then we're going to further sample a word. And the rest is very similar to the. The likelihood function nowis more complicated for LDA. But there's a close connection between thelikelihood function of LDA and the PLSA. So I'm going to illustratethe difference here. So in the top, you see PLSA likelihood functionthat you have already seen before. It's copied from previous slide. Only that I dropped the background forsimplicity. So in the LDA formulas yousee very similar things. You see the first equationis essentially the same. And this is the probability of generatinga word from multiple word distributions. And this formula is a sum of allthe possibilities of generating a word. Inside a sum is a product ofthe probability of choosing a topic multiplied by the probability ofobserving the word from that topic. So this is a very important formula,as I've stressed multiple times. And this is actually the coreassumption in all the topic models. And you might see other topic modelsthat are extensions of LDA or PLSA. And they all rely on this. So it's very important to understand this. And this gives us a probability ofgetting a word from a mixture model. Now, next in the probability ofa document, we see there is a PLSA component in the LDA formula, but the LDAformula will add a sum integral here. And that's to account forthe fact that the pis are not fixed. So they are drawn from the originaldistribution, and that's shown here. That's why we have to take an integral,to consider all the possible pis that we could possibly draw fromthis Dirichlet distribution. And similarly in the likelihood forthe whole collection, we also see further components added,another integral here. Right? So basically in the area we're justadding this integrals to account for the uncertainties and we added of coursethe Dirichlet distributions to cover the choice of this parameters,pis, and theta. So this is a likelihood function for LDA. Now, next to this, let's talk about theparameter as estimation and inferences. Now the parameters can be now estimatedusing exactly the same approach maximum likelihood estimate for LDA. Now you might think about how manyparameters are there in LDA versus PLSA. You'll see there're a fewer parametersin LDA because in this case the only parameters are alphas and the betas. So we can use the maximum likelihoodestimator to compute that. Of course, it's more complicated becausethe form of likelihood function is more complicated. But what's also importantis notice that now these parameters that we are interestedin name and topics, and the coverage are nolonger parameters in LDA. In this case we have touse basic inference or posterior inference to compute them basedon the parameters of alpha and the beta. Unfortunately, thiscomputation is intractable. So we generally have to resortto approximate inference. And there are many methods available forthat and I'm sure you will see them when you use different tool kitsfor LDA, or when you read papers about these different extensions of LDA. Now here we, of course, can't givein-depth instruction to that, but just know that they are computed based in inference by usingthe parameters alphas and betas. But our math [INAUDIBLE],actually, in the end, in some of our math list,it's very similar to PLSA. And, especially when we usealgorithm called class assembly, then the algorithm looks verysimilar to the Algorithm. So in the end,they are doing something very similar. So to summarize our discussionof properties of topic models, these models providea general principle or way of mining and analyzing topicsin text with many applications. The best basic task setup isto take test data as input and we're going to output the k topics. Each topic is characterizedby word distribution. And we're going to also output proportionsof these topics covered in each document. And PLSA is the basic topic model, andin fact the most basic of the topic model. And this is often adequate formost applications. That's why we spend a lot oftime to explain PLSA in detail. Now LDA improves overPLSA by imposing priors. This has led to theoreticallymore appealing models. However, in practice, LDA andPLSA tend to give similar performance, so in practice PLSA and LDA would workequally well for most of the tasks. Now here are some suggested readings ifyou want to know more about the topic. First is a nice review ofprobabilistic topic models. The second has a discussion about howto automatically label a topic model. Now I've shown you some distributions andthey intuitively suggest a topic. But what exactly is a topic? Can we use phrases to label the topic? To make it the more easy to understand and this paper is about the techniques fordoing that. The third one is empirical comparisonof LDA and the PLSA for various tasks. The conclusion is that theytend to perform similarly. [MUSIC]
[SOUND] So let's plug in these model masses into the ranking function tosee what we will get, okay? This is a general smoothing. So a general ranking function forsmoothing with subtraction and you have seen this before. And now we have a very specific smoothingmethod, the JM smoothing method. So now let's see what what's a value foroffice of D here. And what's the value for p sub c here? Right, so we may need to decide this in order to figure out the exactform of the ranking function. And we also need to figureout of course alpha. So let's see. Well this ratio is basically this,right, so, here, this is the probabilityof c board on the top, and this is the probabilityof unseen war or, in other words basically 11times basically the alpha here, this, so it's easy to see that. This can be then rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? So it would be just lambda, right? And what would happen if we plug inthis value here, if this is lambda. What can we say about this? Does it depend on the document? No, so it can be ignored. Right? So we'll end up having thisranking function shown here. And in this case you can easy to see, this a precisely a vector spacemodel because this part is a sum over all the matched query terms,this is an element of the query map. What do you think is a elementof the document up there? Well it's this, right. So that's our document left element. And let's further examine what'sinside of this logarithm. Well one plus this. So it's going to be nonnegative,this log of this, it's going to be at least 1, right? And these, this is a parameter,so lambda is parameter. And let's look at this. Now this is a TF. Now we see very clearlythis TF weighting here. And the larger the count is,the higher the weighting will be. We also see IDF weighting,which is given by this. And we see docking the lan'srelationship here. So all these heuristicsare captured in this formula. What's interesting thatwe kind of have got this weighting function automaticallyby making various assumptions. Whereas in the vector space model, we had to go through those heuristicdesign in order to get this. And in this case note thatthere's a specific form. And when you see whether thisform actually makes sense. All right so what do you thinkis the denominator here, hm? This is a math of document. Total number of words,multiplied by the probability of the word given by the collection, right? So this actually can be interpretedas expected account over word. If we're going to draw, a word,from the connection that we model. And, we're going to draw as many asthe number of words in the document. If you do that,the expected account of a word, w, would be precisely givenby this denominator. So, this ratio basically,is comparing the actual count, here. The actual count of the word in thedocument with expected count given by this product if the word is in fact followingthe distribution in the clutch this. And if this counter is larger thanthe expected counter in this part, this ratio would be larger than one. So that's actually a veryinteresting interpretation, right? It's very natural and intuitive,it makes a lot of sense. And this is one advantage of usingthis kind of probabilistic reasoning where we have made explicit assumptions. And, we know precisely whywe have a logarithm here. And, why we have these probabilities here. And, we also have a formula thatintuitively makes a lot of sense and does TF-IDF weighting anddocumenting and some others. Let's look at the,the Dirichlet Prior Smoothing. It's very similar tothe case of JM smoothing. In this case,the smoothing parameter is mu and that's different fromlambda that we saw before. But the format looks very similar. The form of the functionlooks very similar. So we still have linear operation here. And when we compute this ratio, one will find that is thatthe ratio is equal to this. And what's interesting here is that weare doing another comparison here now. We're comparing the actual count. Which is the expected account of the worldif we sampled meal worlds according to the collection world probability. So note that it's interesting we don'teven see docking the lens here and lighter in the JMs model. All right so this of courseshould be plugged into this part. So you might wonder, sowhere is docking lens. Interestingly the docking lensis here in alpha sub d so this would be plugged into this part. As a result what we get isthe following function here and this is again a sum overall the match query words. And we're against the queer,the query, time frequency here. And you can interpret this asthe element of a document vector, but this is no longera single dot product, right? Because we have this part,I know that n is the name of the query, right? So that just means ifwe score this function, we have to take a sum overall the query words, and then do some adjustment ofthe score based on the document. But it's still, it's still clearthat it does documents lens modulation because this lensis in the denominator so a longer document willhave a lower weight here. And we can also see it has tf here andnow idf. Only that this time the form of theformula is different from the previous one in JMs one. But intuitively it still implements TFIDFwaiting and document lens rendition again, the form of the function is dictatedby the probabilistic reasoning and assumptions that we have made. Now there are alsodisadvantages of this approach. And that is, there's no guaranteethat there's such a form of the formula will actually work well. So if we look about at this geo function,all those TF-IDF waiting and document lens rendition for example it's unclear whetherwe have sub-linear transformation. Unfortunately we can see here thereis a logarithm function here. So we do have also the,so it's here right? So we do have the sublineartransformation, but we do not intentionally do that. That means there's no guarantee thatwe will end up in this, in this way. Suppose we don't have logarithm,then there's no sub-linear transformation. As we discussed before, perhapsthe formula is not going to work so well. So that's an example of the gapbetween a formal model like this and the relevance that we have to model, which is really a subjectmotion that is tied to users. So it doesn't mean we cannot fix this. For example, imagine if we didnot have this logarithm, right? So we can take a risk andwe're going to add one, or we can even add double logarithm. But then, it would mean that the functionis no longer a proper risk model. So the consequence ofthe modification is no longer as predictable aswhat we have been doing now. So, that's also why, for example,PM45 remains very competitive and still, open channel how to usepublic risk models as they arrive, better model than the PM25. In particular how do we use querylike how to derive a model and that would work consistentlybetter than DM 25. Currently we still cannot do that. Still interesting open question. So to summarize this part, we've talkedabout the two smoothing methods. Jelinek-Mercer which is doing the fixedcoefficient linear interpolation. Dirichlet Prior this is what add a pseudocounts to every word and is doing adaptive interpolation in that the coefficientwould be larger for shorter documents. In most cases we can see, by using thesesmoothing methods, we will be able to reach a retrieval function wherethe assumptions are clearly articulate. So they are less heuristic. Explaining the results also showthat these, retrieval functions. Also are very effective and they arecomparable to BM 25 or pm lens adultation. So this is a major advantageof probably smaller where we don't have to doa lot of heuristic design. Yet in the end that we naturallyimplemented TF-IDF weighting and doc length normalization. Each of these functions also hasprecise ones smoothing parameter. In this case of course we still needto set this smoothing parameter. There are also methods that can beused to estimate these parameters. So overall,this shows by using a probabilistic model, we follow very different strategiesthen the vector space model. Yet, in the end, we end up uh,withsome retrievable functions that look very similar tothe vector space model. With some advantages in havingassumptions clearly stated. And then, the form dictatedby a probabilistic model. Now, this also concludes our discussion ofthe query likelihood probabilistic model. And let's recall whatassumptions we have made in order to derive the functionsthat we have seen in this lecture. Well we basically have made fourassumptions that I listed here. The first assumption is that the relevancecan be modeled by the query likelihood. And the second assumption with med is, arequery words are generated independently that allows us to decomposethe probability of the whole query into a product of probabilitiesof old words in the query. And then,the third assumption that we have made is, if a word is not seen,the document or in the late, its probability proportional toits probability in the collection. That's a smoothing witha collection ama model. And finally, we made one of thesetwo assumptions about the smoothing. So we either used JM smoothing orDirichlet prior smoothing. If we make these four assumptionsthen we have no choice but to take the form of the retrievalfunction that we have seen earlier. Fortunately the function has a niceproperty in that it implements TF-IDF weighting and document machine andthese functions also work very well. So in that sense, these functions are less heuristiccompared with the vector space model. And there are many extensions of this,this basic model and you can find the discussion of them inthe reference at the end of this lecture. [MUSIC]
[MUSIC] So let's take a look at this in detail. So in this random surfingmodel at any page would assume random surfer would choosethe next page to visit. So this is a small graph here. That's of course, over simplificationof the complicated web. But let's say there are fourdocuments here, d1, d2, d3 and d4. And let's assume that a random surfer orrandom walker can be any of these pages. And then the randomsurfer could decide to, just randomly jumping to any page or follow a link andthen visit the next page. So if the random surfer is at d1, then there is some probability thatrandom surfer will follow the links. Now there are two outlinks here,one is pointing to d3, the other is pointing to d4. So the random surfer could pick anyof these two to reach d3 and d4. But it also assumes that the random sofar might get bore sometimes. So the random surfing which decideto ignore the actual links and simply randomly jumpinto any page in the web. So if it does that, it would be ableto reach any of the other pages even though there's no link you actually,you want from that page. So this is to assume thatrandom surfing model. Imagine a random surfer isreally doing surfing like this, then we can ask the question howlikely on average the surfer would actually reach a particularpage like a d1, a d2, or a d3. That's the average probability ofvisiting a particular page and this probability is preciselywhat a page ranker computes. So the page rank score ofthe document is the average probability that the surfervisits a particular page. Now intuitively, this would basicallycapture the inlink account, why? Because if a page has a lot of inlinks, then it would have a higherchance of being visited. Because there will be moreopportunities of having the server to follow a link to come to this page. And this is why the random surfing model actually captures the IDof counting the inlinks. Note that it also considersthe interacting links, why? Because if the page is that point thenyou have themselves a lot of inlinks. That would mean the random surfer wouldvery likely reach one of them and therefore, it increasethe chance of visiting you. So this is just a nice way to captureboth indirect and a direct links. So mathematically, how can we compute thisproblem in a day in order to see that, we need to take a look at how thisproblem there is a computing. So first of all let's take a lookat the transition metrics here. And this is just metrics withvalues indicating how likely the random surfer would gofrom one page to another. So each rule stands for a starting page. For example, rule one wouldindicate the probability of going to any of the other four pages from d1. And here we see there are only2 non 0 entries which is 1/2. So this is because if you look atthe graph d1 is pointing to d3 and d4. There is no link from d1 or d2. So we've got 0s for the first 2 columns and 0.5 for d3 and d4. In general, the M in this matrix, M sub ij is the probabilityof going from di to dj. And obviously for each rule,the values should sum to 1, because the surfer would have to go toprecisely one of these other pages. So this is a transition metric. Now how can we compute the probabilityof a surfer visiting a page? Well if you look at the surfmodel then basically, we can compute the probabilityof reaching a page as follows. So here on the left hand side,you see it's the probability visiting page dj at time plus 1,so it's the next time point. On the right hand side, you can seethe equation involves the probability of at page di at time t. So you can see the subscriptin that t here, and that indicates that's the probability thatthe server was at a document at time t. So the equation basically,captures the two possibilities of reachingat dj at the time t plus 1. What are these two possibilities? Well one is through random surfing and one is through following a link,as we just explained. So the first part captures the probability that the random surfer would reachthis page by following a link. And you can see the randomsurfer chooses this strategy with probability 1 minusalpha as we assume. And sothere is a factor of 1 minus alpha here. But the main party is realistsum over all the possible pages that the surfer could have been at time t. There are n pages soit's a sum over all possible n pages. Inside the sum is a productof two probabilities. One is the probability that the surfer was at di at time t, that's p sub t of di. The other is the transitionprobability from di to dj. And so in order to reach this dj page, the surfer must first be at di at time t. And then also, would also have tofollow the link to go from di to dj. So the probability is the probabilityof being at di at time t multiplied by the probability of going from thatpage to the target page, dj here. The second part is a similar sum, the onlydifference is that now the transition probability is a uniformtransition probability. 1 over n andthis part of captures is the probability of reaching this pagethrough random jumping. So the form is exactly the same andthis also allows us to see on why PageRank is essentially assumeda smoothing of the transition matrix. If you think about this 1 over n ascoming from another transition matrix that has all the elements being1 over n in uniform matrix. Then you can see very clearlyessentially we can merge the two parts, because they are of the same form. We can imagine there's a differentmetrics that's combination of this m and that uniform metrics whereevery m is 1 over n. And in this sense PageRank usesthis idea of smoothing and ensuring that there's no zero entryin such as transition matrix. Now of course this is the time dependentthe calculation of the probabilities. Now we can imagine, if we'll computethe average of the probabilities, the average of probabilities probablywith the sets of file this equation without considering the time index. So let's drop the time index andjust assume that they will be equal. Now this would give us any equations,because for each page we have such equation. And if you look at the whatvariables we have in these equations there are also precisely n variables. So this basically means,we now have a system of n equations with n variables andthese are linear equations. So basically, now the problem boilsdown to solve this system of equations. And here, I also showthe equations in the metric form. It's the vector p here equals a matrix or the transpose of the matrix here andmultiplied by the vector again. Now, if you still remember some knowledgethat you've learned from linear algebra and then you will realize, this isprecisely the equation for eigenvector. When multiply the metrics by this vector,you get the same value as this matter and this can be solved byusing iterative algorithm. So because the equations here on the back are basicallytaken from the previous slide. So you'll see the relation between thepage that ran sports on different pages. And this iterative approach orpower approach, we simply start with srandomly initialized vector p. And then we repeatedlyjust update this p by multiplying the metricshere by this p factor. I also show a concrete example here. So you can see this now. If we assume alpha is 0.2, then with the example thatwe show here on the slide, we have the originaltransition matrix is here. That includes the graph, the actual linksand we have this smoothing transition metrics, uniform transition metricsrepresenting random jumping. And we can combine them together witha liner interpolation to form another metric that would be like this. So essentially, we can imagine now the web looks likethis and can be captured like that. They're all virtual linksbetween all the pages now. The page we're on now would justinitialize the p vector first and then just computed the updating of this p vector by using thismetrics multiplication. Now if you rewrite thismetric multiplication in terms of individual equations,you'll see this. And this is basically,the updating formula for this particular pages and page score. So you can also see if you want to computethe value of this updated score for d1. You basically multiplythis rule by this column, and we'll take the thirdproduct of the two. And that will give us the value forthis value. So this is how we updated the vectorwe started with an initial values for these guys for this. And then we just revisethe scores which generate a new set of scores andthe updating formula is this one. So we just repeatedly apply this andhere it converges. And when the matrix is like this,where there's no 0 values and it can be guaranteed to converge. And at that point the we will just havethe PageRank scores for all the pages. We typically go to sets ofinitial values just to 1 over n. So interestingly,this updating formula can be also interpreted as propagatingscores on the graph, can you see why? Or if you look at this formula andthen compare that with this graph and can you imagine,how we might be able to interpret this as essentially propagatingscores over the graph. I hope you will see that indeed, we can imagine we have valuesinitialized on each of these pages. So we can have values here andsay, that's a 1 over 4 for each. And then we're going to use thesemetrics to update this the scores. And if you look at the equation herethis one, basically we're going to combine the scores of the pages thatpossibly would lead to reaching this page. So we'll look at all the pagesthat are pointing to this page and then combine this score and propagate thesum of the scores to this document, d1. To look at the scores that we presentthe probability that the random surfer would be visiting the otherpages before it reached d1. And then just dothe propagation to simulate the probability of reaching this page, d1. So there are two interpretations here. One is just the matrix multiplication. We repeat the multiplyingthat by these metrics. The other is to just thinkof it as a propagating these scores repeatedly on the web. So in practice, the combination ofPageRank score is actually efficient. Because the matrices is fast and thereare some, ways we transform the equation. So that you avoid actuallyliterally computing the values for all those elements. Sometimes you may also normalize theequation and that will give you a somewhat different form of the equation, butthen the ranking of pages will not change. The results of this potentialproblem of zero-outlink problem. In that case, if a page does not haveany outlink then the probability of these pages would not sum to 1. Basically, the probability of reaching thenext page from this page would not sum to 1, mainly because we have lostsome probability to mass. One would assume there's some probabilitythat the surfer would try to follow the links, butthen there is no link to follow. And one possible solution is simply to usea page that is specific damping factor, and that could easily fix this. Basically, that's to say alpha wouldbe 1.0 for a page with no outlink. In that case,the surfer would just have to randomly jump to another pageinstead of trying to follow a link. There are many extensions of PageRank, oneextension is to topic-specific PageRank. Note that PageRank doesn't merelyuse the query information. So we can make PageRank specific however. So for example,at the top of a specific page you rank, we can simply assumewhen the surfer is bored. The surfer is not randomlyjumping to any page on the web. Instead, he's going to jump to only thosepages that are relevant to our query. For example, if the query is not sportsthen we can assume that when it's doing random jumping, it's goingto randomly jump to a sports page. By doing this, then we can buya PageRank through topic and sports. And then if you know the currenttheory is about sports, and then you can use this specializedPageRank score to rank documents. That would be better than if youuse the generic PageRank score. PageRank is also a channel that can beused in many other applications for network analysis particularly forexample, social networks. You can imagine if you computethe PageRank scores for social network, where a linkmight indicate a friendship or a relation, you would get somemeaningful scores for people [MUSIC]
This lecture is aboutcollaborative filtering. In this lecture we're going to continuethe discussion of recommended systems. In particular, we're going to look atthe approach of collaborative filtering. You have seen this slide before whenwe talked about the two strategies to answer the basic question,will user U like item X? In the previous lecture, we looked at the item similarity,that's content-based filtering. In this lecture, we're going tolook at the user similarity. This is a different strategy,called a collaborative filtering. So first, what is collaborative filtering? It is to make filtering decisions for individual user based onthe judgements of other uses. And that is to say we willinfer individual's interest or preferences from thatof other similar users. So the general idea is the following. Given a user u, we're going to firstfind the similar users, U1 through. And then we're going topredict the use preferences based on the preferences ofthese similar users, U1 through. Now, the user similarity here canbe judged based their similarity, the preferences on a common set of items. Now here you can see the exactcontent of item doesn't really matter. We're going to look at the only therelation between the users and the items. So this means thisapproach is very general. It can be applied to any items,not just the text of objects. So this approach would work wellunder the following assumptions. First, users with the same interestwill have similar preferences. Second, the users with similar preferencesprobably share the same interest. So for example, if the interest ofthe user is in information retrieval, then we can infer the userprobably favor SIGIR papers. So those who are interested ininformation retrieval researching, probably all favor SIGIR papers. That's an assumption that we make. And if this assumption is true, then it would help collaborativefiltering to work well. We can also assume that if we seepeople favor See SIGIR papers, then we can infer their interestis probably information retrieval. So in these simple examples,it seems to make sense, and in many cases such assumptionactually does make sense. So another assumption we have to makeis that there are sufficiently large number of user preferencesavailable to us. So for example, if you see a lotof ratings of users for movies and those indicate theirpreferences on movies. And if you have a lot of such data,then cluttered and filtering can be very effective. If not, there will be a problem, andthat's often called a cold start problem. That means you don't have manypreferences available, so the system could not fully take advantageof collaborative filtering yet. So let's look at the filteringproblem in a more formal way. So this picture shows that we are, in general, considering a lot of users and we're showing m users here, so U1 through. And we're also consideringa number of objects. Let's say n objects inorder to O1 through On. And then we will assume thatthe users will be able to judge those objects and the user could forexample give ratings to those items. For example, those items could be movies,could be products and then the users would giveratings 1 through 5 and see. So what you see here is that we haveshown some ratings available for some combinations. So some users have watched some movies,they have rated those movies, they obviously won't be ableto watch all the movies and some users may actuallyonly watch a few movies. So this is in general a small symmetrics. So many items andmany entries have unknown values. And what's interesting here is wecould potentially infer the value of an element in this matrixbased on other values. And that's after the essential questionin collaborative filtering, and that is, we assume there's an unknownfunction here, f. That would map a pair of user andobject to a rating. And we have observed the sumvalues of this function. And we want to infer the valueof this function for other pairs that don't havethat as available here. So this is very similar to othermachinery problems where we'd know the values of the functionon some training data set. And we hope to predict the values ofthis function on some test data so this is a function approximation. And how can we pick out the functionbased on the observed ratings. So this is the setup. Now there are many approachesto solving this problem. In fact,this is a very active research area or reason that there are specialconferences dedicated to the problem, major conference devoted to the problem. [MUSIC]
##  **Overview**  There will be 3 Programming Assignments (MPs) to be completed using Python. All MPs are individual assignments. You have unlimited attempts to complete them. Links to some Python tutorials have been provided at the end of this reading.  Programming assignments will be managed via [ LiveDataLab ](http://livelab.centralus.cloudapp.azure.com/) .  **Please follow the "LiveDataLab Setup" instructions below. This setup process needs to be completed ONLY ONCE.**  1\. Please sign up on [ LiveDataLab ](http://livelab.centralus.cloudapp.azure.com/) using your **Illinois** email id. Please note down or remember the password as you will use it often to log into LiveDataLab!  2\. Next, enroll in the course using the following link: [ http://livelab.centralus.cloudapp.azure.com/course/join/V7NL069ZKL6F9N1 ](http://livelab.centralus.cloudapp.azure.com/course/join/V7NL069ZKL6F9N1)  3\. Then, follow the instructions in the pdf below to link your Github account to LiveDataLab.  **Please follow the "MP Setup" process in the pdf below. This process needs to be completed for EACH MP.**  **Python Tutorials:**  [ https://cs231n.github.io/python-numpy-tutorial/#python ](https://cs231n.github.io/python-numpy-tutorial/#python)  [ https://docs.python.org/3/tutorial/ ](https://docs.python.org/3/tutorial/)  [ https://www.tutorialspoint.com/python/index.htm ](https://www.tutorialspoint.com/python/index.htm)  
[SOUND]This lecture is aboutthe Paradigmatics Relation Discovery. In this lecture we are going to talk abouthow to discover a particular kind of word association calleda paradigmatical relation. By definition,two words are paradigmatically related if they share a similar context. Namely, they occur insimilar positions in text. So naturally our idea of discovering sucha relation is to look at the context of each word and then try to computethe similarity of those contexts. So here is an example ofcontext of a word, cat. Here I have taken the wordcat out of the context and you can see we are seeing some remainingwords in the sentences that contain cat. Now, we can do the same thing foranother word like dog. So in general we would like to capturesuch a context and then try to assess the similarity of the context of cat andthe context of a word like dog. So now the question is how can weformally represent the context and then define the similarity function. So first, we note that the contextactually contains a lot of words. So, they can be regarded asa pseudo document, a imagine document, but there are also differentways of looking at the context. For example, we can look at the wordthat occurs before the word cat. We can call this context Left1 context. All right, so in this case youwill see words like my, his, or big, a, the, et cetera. These are the words that canoccur to left of the word cat. So we say my cat, his cat,big cat, a cat, et cetera. Similarly, we can also collect the wordsthat occur right after the word cat. We can call this context Right1, and here we see words like eats,ate, is, has, et cetera. Or, more generally, we can look at all the words inthe window of text around the word cat. Here, let's say we can take a windowof 8 words around the word cat. We call this context Window8. Now, of course, you can see allthe words from left or from right, and so we'll have a bag of words ingeneral to represent the context. Now, such a word based representationwould actually give us an interesting way to define theperspective of measuring the similarity. Because if you look at justthe similarity of Left1, then we'll see words that sharejust the words in the left context, and we kind of ignored the other wordsthat are also in the general context. So that gives us one perspective tomeasure the similarity, and similarly, if we only use the Right1 context, we will capture this narrativefrom another perspective. Using both the Left1 andRight1 of course would allow us to capture the similarity with evenmore strict criteria. So in general, context may containadjacent words, like eats and my, that you see here, ornon-adjacent words, like Saturday, Tuesday, orsome other words in the context. And this flexibility also allows usto match the similarity in somewhat different ways. Sometimes this is useful, as we might want to capturesimilarity base on general content. That would give us looselyrelated paradigmatical relations. Whereas if you use only the wordsimmediately to the left and to the right of the word, then youlikely will capture words that are very much related by their syntacticalcategories and semantics. So the general idea of discoveringparadigmatical relations is to compute the similarityof context of two words. So here, for example,we can measure the similarity of cat and dog based on the similarityof their context. In general, we can combine allkinds of views of the context. And so the similarity function is,in general, a combination of similaritieson different context. And of course, we can also assignweights to these different similarities to allow us to focusmore on a particular kind of context. And this would be naturallyapplication specific, but again, here the main idea for discoveringpardigmatically related words is to computer the similarityof their context. So next let's see how we exactlycompute these similarity functions. Now to answer this question,it is useful to think of bag of words representation as vectorsin a vector space model. Now those of you who have beenfamiliar with information retrieval or textual retrieval techniques wouldrealize that vector space model has been used frequently formodeling documents and queries for search. But here we also find it convenientto model the context of a word for paradigmatic relation discovery. So the idea of thisapproach is to view each word in our vocabulary as defining onedimension in a high dimensional space. So we have N words intotal in the vocabulary, then we have N dimensions,as illustrated here. And on the bottom, you can see a frequencyvector representing a context, and here we see where eatsoccurred 5 times in this context, ate occurred 3 times, et cetera. So this vector can then be placedin this vector space model. So in general,we can represent a pseudo document or context of cat as one vector,d1, and another word, dog, might give us a different context,so d2. And then we can measurethe similarity of these two vectors. So by viewing context inthe vector space model, we convert the problem ofparadigmatical relation discovery into the problem of computingthe vectors and their similarity. So the two questions that wehave to address are first, how to compute each vector, andthat is how to compute xi or yi. And the other question is howdo you compute the similarity. Now in general, there are many approachesthat can be used to solve the problem, and most of them are developed forinformation retrieval. And they have been shown to work well for matching a query vector anda document vector. But we can adapt many ofthe ideas to compute a similarity of context documents for our purpose here. So let's first look atthe one plausible approach, where we try to matchthe similarity of context based on the expected overlap of words,and we call this EOWC. So the idea here is to representa context by a word vector where each word has a weightthat's equal to the probability that a randomly picked word fromthis document vector, is this word. So in other words,xi is defined as the normalized account of word wi in the context, and this can be interpreted asthe probability that you would actually pick this word from d1if you randomly picked a word. Now, of course these xi's would sum to onebecause they are normalized frequencies, and this means the vector is actually probability ofthe distribution over words. So, the vector d2 can be alsocomputed in the same way, and this would give us then two probabilitydistributions representing two contexts. So, that addresses the problemhow to compute the vectors, and next let's see how we can definesimilarity in this approach. Well, here, we simply definethe similarity as a dot product of two vectors, andthis is defined as a sum of the products of the correspondingelements of the two vectors. Now, it's interesting to seethat this similarity function actually has a nice interpretation,and that is this. Dot product, in fact that givesus the probability that two randomly picked words fromthe two contexts are identical. That means if we try to pick a wordfrom one context and try to pick another word from another context, we can thenask the question, are they identical? If the two contexts are very similar,then we should expect we frequently will see the two words picked fromthe two contexts are identical. If they are very different,then the chance of seeing identical words being picked fromthe two contexts would be small. So this intuitively makes sense, right,for measuring similarity of contexts. Now you might want to also takea look at the exact formulas and see why this can be interpretedas the probability that two randomly picked words are identical. So if you just stare at the formulato check what's inside this sum, then you will see basically in eachcase it gives us the probability that we will see an overlap ona particular word, wi. And where xi gives us a probability thatwe will pick this particular word from d1, and yi gives us the probabilityof picking this word from d2. And when we pick the sameword from the two contexts, then we have an identical pick, right so. That's one possible approach, EOWC,extracted overlap of words in context. Now as always, we would like to assesswhether this approach it would work well. Now of course, ultimately we have totest the approach with real data and see if it gives us reallysemantically related words. Really give us paradigmatical relations,but analytically we can also analyzethis formula a little bit. So first, as I said,it does make sense, right, because this formula will give a higher score if thereis more overlap between the two contexts. So that's exactly what we want. But if you analyzethe formula more carefully, then you also see there mightbe some potential problems, and specifically thereare two potential problems. First, it might favor matchingone frequent term very well, over matching more distinct terms. And that is because in the dot product,if one element has a high value and this element is shared by both contexts andit contributes a lot to the overall sum, it might indeed make the scorehigher than in another case, where the two vectors actually havea lot of overlap in different terms. But each term has a relatively lowfrequency, so this may not be desirable. Of course, this might bedesirable in some other cases. But in our case, we should intuitivelyprefer a case where we match more different terms in the context,so that we have more confidence in saying that the two wordsindeed occur in similar context. If you only rely on one term and that's a little bit questionable,it may not be robust. Now the second problem is that ittreats every word equally, right. So if you match a word like the and it will be the same asmatching a word like eats, but intuitively we knowmatching the isn't really surprising because the occurs everywhere. So matching the is not as suchstrong evidence as matching what a word like eats,which doesn't occur frequently. So this is anotherproblem of this approach. In the next chapter we are going to talkabout how to address these problems. [MUSIC]
[SOUND]>> This lecture is about the Overviewof Statistical Language Models, which cover propermodels as special cases. In this lecture we're going to give a overview of Statical Language Models. These models are general models that cover probabilistic topic modelsas a special cases. So first off,what is a Statistical Language Model? A Statistical Language Model isbasically a probability distribution over word sequences. So, for example,we might have a distribution that gives, today is Wednesday a probability of .001. It might give today Wednesday is, which is a non-grammatical sentence, a very,very small probability as shown here. And similarly another sentence, the eigenvalue is positive mightget the probability of .00001. So as you can see such a distributionclearly is Context Dependent. It depends on the Context of Discussion. Some Word Sequences might have higherprobabilities than others but the same Sequence of Words might have differentprobability in different context. And so this suggests that such adistribution can actually categorize topic such a model can also be regardedas Probabilistic Mechanism for generating text. And that just means we can view textdata as data observed from such a model. For this reason,we call such a model as Generating Model. So, now given a model we can thenassemble sequences of words. So, for example, based on the distributionthat I have shown here on this slide, when matter it say assemblea sequence like today is Wednesday because it has a relativehigh probability. We might often get such a sequence. We might also get the itemvalue as positive sometimes with a smaller probability andvery, very occasionally we might get today is Wednesday becauseit's probability is so small. So in general, in order to categorize sucha distribution we must specify probability values forall these different sequences of words. Obviously, it's impossibleto specify that because it's impossible to enumerate all ofthe possible sequences of words. So in practice, we will have tosimplify the model in some way. So, the simplest language model iscalled the Unigram Language Model. In such a case, it was simply a the text is generated by generatingeach word independently. But in general, the words maynot be generated independently. But after we make this assumption, we cansignificantly simplify the language more. Basically, now the probability ofa sequence of words, w1 through wn, will be just the product ofthe probability of each word. So for such a model, we have as many parameters asthe number of words in our vocabulary. So here we assume we have n words,so we have n probabilities. One for each word. And then some to 1. So, now we assume thatour text is a sample drawn according to this word distribution. That just means,we're going to draw a word each time and then eventually we'll get a text. So for example, now again, we can try to assemble wordsaccording to a distribution. We might get Wednesday often ortoday often. And some other words like eigenvaluemight have a small probability, etcetera. But with this, we actually canalso compute the probability of every sequence, even though our modelonly specify the probabilities of words. And this is because of the independence. So specifically, we can computethe probability of today is Wednesday. Because it's just a productof the probability of today, the probability of is, andprobability of Wednesday. For example,I show some fake numbers here and when you multiply these numbers together you getthe probability that today's Wednesday. So as you can see, with N probabilities,one for each word, we actually can characterize the probability situationover all kinds of sequences of words. And so, this is a very simple model. Ignore the word order. So it may not be, in fact, in someproblems, such as for speech recognition, where you may care aboutthe order of words. But it turns out to bequite sufficient for many tasks that involve topic analysis. And that's also whatwe're interested in here. So when we have a model, we generally havetwo problems that we can think about. One is, given a model, how likely are weto observe a certain kind of data points? That is,we are interested in the Sampling Process. The other is the Estimation Process. And that, is to think ofthe parameters of a model given, some observe the data and we'regoing to talk about that in a moment. Let's first talk about the sampling. So, here I show two examples of WaterDistributions or Unigram Language Models. The first one has higher probabilities for words like a text mining association,it's separate. Now this signals a topic about text miningbecause when we assemble words from such a distribution, we tend to see wordsthat often occur in text mining contest. So in this case,if we ask the question about what is the probability ofgenerating a particular document. Then, we likely will see text thatlooks like a text mining paper. Of course, the text that wegenerate by drawing words. This distribution is unlikely coherent. Although, the probabilityof generating attacks mine [INAUDIBLE] publishingin the top conference is non-zero assuming that no word hasa zero probability in the distribution. And that just means,we can essentially generate all kinds of text documents including verymeaningful text documents. Now, the second distribution show, on the bottom, has different thanwhat was high probabilities. So food [INAUDIBLE] healthy [INAUDIBLE],etcetera. So this clearly indicatesa different topic. In this case it's probably about health. So if we sample a wordfrom such a distribution, then the probability of observing a textmining paper would be very, very small. On the other hand, the probability ofobserving a text that looks like a food nutrition paper would be high,relatively higher. So that just means, given a particulardistribution, different than the text. Now let's look atthe estimation problem now. In this case, we're going to assumethat we have observed the data. I will know exactly whatthe text data looks like. In this case,let's assume we have a text mining paper. In fact, it's abstract of the paper,so the total number of words is 100. And I've shown some countsof individual words here. Now, if we ask the question,what is the most likely Language Model that has beenused to generate this text data? Assuming that the text is observedfrom some Language Model, what's our best guessof this Language Model? Okay, so the problem now is just toestimate the probabilities of these words. As I've shown here. So what do you think? What would be your guess? Would you guess text hasa very small probability, or a relatively large probability? What about query? Well, your guess probablywould be dependent on how many times we have observedthis word in the text data, right? And if you think about it for a moment. And if you are like many others,you would have guessed that, well, text has a probability of 10out of 100 because I've observed the text 10 times in the textthat has a total of 100 words. And similarly, mining has 5 out of 100. And query has a relatively smallprobability, just observed for once. So it's 1 out of 100. Right, so that, intuitively,is a reasonable guess. But the question is, is this our bestguess or best estimate of the parameters? Of course,in order to answer this question, we have to define what do we mean by best,in this case, it turns out that ourguesses are indeed the best. In some sense and this is calledMaximum Likelihood Estimate. And it's the best thing that, it will givethe observer data our maximum probability. Meaning that, if you changethe estimate somehow, even slightly, then the probability of the observedtext data will be somewhat smaller. And this is calleda Maximum Likelihood Estimate. [MUSIC]
###  **Introduction**  The course project is to give the students hands-on experience on developing some novel information retrieval and/or text mining tools. It would allow the students to potentially apply all the knowledge and skills learned in the course to solve a real-world problem. Group work is encouraged, but not required (i.e., you can have a one-person team). The maximum size of a team is 5 members to avoid challenges in efficient coordination of the work by too many team members. However, a team of a larger size is also possible subject to the approval of the instructor. A typical reason for a larger team is because the project has a very natural task division among the team members so that the need for frequent interactions and coordination of team members may be minimum despite the large size of the team. Whenever possible, collaboration of multiple project teams is strongly encouraged to minimize the amount of work of each team via expertise or resource sharing, as well as to generate “combined impact” (e.g., one team may develop a crawler that can be used by another team that develops a search engine).  More details (what to submit at each step) can be found here:  https://docs.google.com/document/d/1ubzdWekH2WLzft- IaSnkYflKrR7DqW-X7WsWiG30loI/edit#heading=h.wfalxc53x421  ###  **Grading criteria**  Your project will be graded based on the following weighting scheme, corresponding to three stages of work including 1) team search and formation; 2) proposal development; 3) project result submission. All team members will receive the same grade provided that every member has made sufficient effort (at least 20 hours of quality time to work on the project).    * **Team formation (5%):** Every student is required to form a team by the **beginning of** **Week 9 (Oct 18, 2021)** . The team shall need to designate one of the team members as the team leader who will be responsible for submitting and/or uploading the course project deliverables.     * **Project proposal (5%):** Each project team is required to submit a roughly one-page project proposal by the **end of** **Week 9 (Oct 24, 2021)** , and graded based on completion.     * **Progress report (5%):** Each project team is required to submit a short progress report by the **beginning of Week 13 (Nov 15, 2021)** listing the progress made, remaining tasks, any challenges/issues being faced, etc. It will be graded based on completion.     * **Software code submission with documentation (65%):** Due **end of Week 15 (Dec 09, 2021)** , each project team will be asked to submit the produced source code with reasonable documentation. The documentation should cover both how to use the software and how the software is implemented. The 65% of the grade would be distributed as follows: **45% for source code submission** ; **20% for documentation submission** . Both would be graded based on completion.     * **Software usage tutorial presentation (20%):** Due **end of** **Week 15 (Dec 09, 2021).** 10% of the grading is based on completion and the remaining 10% is based on result of testing the software by graders   Thus, your course project work would be graded primarily based on your effort. If you have followed our guidelines and completed all the required tasks, you should receive at least 90% of the total points for the project. This is to encourage the students to pay attention to time management and set realistic goals that can actually be completed by the end of the semester. The remaining 10% is based on how well your software works; a fully functioning software would be given the whole 10%, whereas a buggy software or a software with missing functions would result in losing some of the 10% of the grade. Completion of a functioning software is emphasized also due to the potential dependency between multiple projects when they are all contributing to a larger project (e.g., one team may produce a crawler to crawl data for use by another team to build a search engine).  _The proposal, progress report and final submission will be peer-graded by you. The TAs will review all peer reviews and make sure that (1) the reviews are fair, and (2) the submissions satisfy all requirements._  **Peer-Grading Instructions**  We'll be using peer-grading for grading project proposals, progress reports and final submissions. Later in the semester, you will receive an email from CMT inviting you as a reviewer, and we will make an announcement when the emails are sent.  Peer grading will begin soon after the submission deadlines have passed. **The proposal peer-grading should be done between Oct 24 (12:00 am CST)-Oct 29 (12:00am CST).** **The progress report peer-grading should be done between Nov 15 (12:00 am CST)-Nov 19 (11:59 pm CST). The final submissions peer-grading should be done between Dec 09 (12:00 am CST)-Dec 17 (11:59 pm CST). Please finish the gradings on time.**  Each project will be reviewed by at least 2 students and each student will review ~1-2 projects. We will be using your comments and scores as a guide to assign the final scores to the projects. Please grade them carefully and honestly. We appreciate your help with managing the grading of such a large class. We hope the process would also be a good learning experience for you. More specific instructions will be announced and sent later.  ###  **Instructions**  **1\. Form a team (5%, due beginning of Week 9, Oct 18 2021)**  By the beginning of Week 9, you should form teams. A team ideally comprises of 1 to 3 members. To form larger groups of up to 5 members, please post a private note on Piazza to get TA or instructor approval. Groups of more than 5 members are highly discouraged. The team also needs to designate a member of the team as team leader. Your team leader will be responsible for submitting and/or uploading the course project deliverables such as: 1) this form, 2) the proposal, 3) the presentation, 4) the code, etc.  **2\. Pick a topic & Write a proposal (5%, due end of Week 9, Oct 24 2021) **  **Picking a project topic**  Students will be able to choose their own topic or select a topic from a list of suggested topics which we will provide by Week 4.  **Writing a project proposal**  Each project team is required to write a one-page proposal before you actually go in depth on a topic. The proposal is due end of Week 9.  In the proposal, you should include the names and email addresses of all the team members. One member must be designated as the project coordinator/leader for the team, so please make sure to indicate that. The project coordinator would be responsible for the coordination of the project work by the team and also communication with the instructor or TA when the team needs help.  Detailed instructions about the required content of the proposal will be provided by Week 4, including a short set of questions that need to be answered in the proposal. As long as those questions are addressed (wherever applicable), the proposal does not have to be very long. A couple of sentences for each question would be sufficient.  **3.** **Peer-review project proposals (due Middle of Week 10, Oct 29 2021)**  Each student will review the progress reports of ~1-2 groups and provide feedback/suggestions. TAs will go through the peer assessments and make sure that the proposals satisfies all requirements listed in the Google Doc.  **4\. Work on the project**  You should try to reuse any existing tools as much as possible so as to minimize the amount of work without sacrificing your goal. Discuss any problems or issues with your teammates or classmates on Campuswire and leverage Campuswire to collaborate with each other. Consistent with our course policy, we strongly encourage you to help each other in all the course work so as to maximize your gain of new knowledge and skills while minimizing your work as much as possible. We will do our best to help you as well. Consider documenting your work regularly. This way, you will already have a lot of things written down by the end of the semester.  **5\. Submit progress report (5%, due beginning of Week 13, Nov 15 2021)**  Each team must submit a short report detailing 1) Progress made thus far, 2) Remaining tasks, 3) Any challenges/issues being faced. The graders, TAs and instructor may provide help and suggestions based on the report. Continue working on the project until the final submission.  **6\. Peer-review progress reports (due end of Week 13, Nov 19 2021)**  Each student will review the progress reports of ~1-2 groups and provide feedback/suggestions.  **7\. Software code submission with documentation (65%, due end of Week 15, Dec 09 2021)**  Each team must submit the software code produced for the project along with a written documentation. The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. 4) Brief description of contribution of each team member in case of a multi-person team. Note that **if you are in a team, it is your responsibility to figure out how to contribute to your group project, so you will need to act proactively and in a timely manner if your group coordinator has not assigned a task to you.** There will be no opportunity to make up for any task that you failed to accomplish. **In general, all the members of a team will get the same grade for the project unless the documentation submission indicates that some member(s) only superficially participated in the project without doing much actual work; in that case, we will discount the grade.** Everyone is expected to spend at least 20 hours to seriously work on your course project as a minimum, not including the time spent for preparing the documentation.  The 65% of the grade would be distributed as follows: **45% for source code submission** ; **20% for documentation submission.** The 20% for the documentation submission includes 5% for overview of functions, 10% for implementation documentation, 5% for usage documentation. There is no strict length requirement for the documentation.  **8\. Software usage tutorial presentation (20%, due end of Week 15, Dec 09 2021)**  At the end of the semester, every project team will be asked to submit a short tutorial presentation (e.g., a voiced ppt presentation) to explain how the developed software is to be used. The presentation must include (1) sufficient instructions on how to install the software if applicable, (2) sufficient instructions on how to use the software, and (3) at least one example of use case so as to allow a grader to use the provided use case to test the software. There is no strict length requirement for this video submission, but you should target at 5~10 minutes. A presentation shorter than 5 minutes is unlikely detailed enough to help users understand how to use the software, whereas a longer video than 10 minutes might be too long for impatient users. However, feel free to produce a longer presentation if needed.  The tutorial presentation would be graded based on  1) completion of the presentation (10%); and  2) result of testing the software by graders (10%).  If the software passes the test (i.e., is working as expected), full points will be given; otherwise, points will be deducted from the 10% allocated to the “result of testing the software by graders.”  **9\. Peer-review project code, documentation, and presentations (due Dec 17 2021)**  Each student will review the final project submissions of ~1-2 groups and provide feedback/suggestions.  
[SOUND]So we talked about PageRank asa way to capture the assault. Now, we also looked at some other exampleswhere a hub might be interesting. So there is another algorithm called HITS,and that going to compute the scores forauthorities and hubs. The intuitions are pages that are widelycited are good authorities and whereas pages that cite manyother pages are good hubs. I think that the most interestingidea of this algorithm HITS, is it's going to usea reinforcement mechanism to kind of help improve the scoring forhubs and the authorities. And so here's the idea, it was assumed that goodauthorities are cited by good hubs. That means if you are cited by manypages with good hub scores then that inquiry says, you're an authority. And similarly, good hubs are thosethat point at good authorities. So if you pointed to a lotof good authority pages, then your hubs score would be increased. So then you will have literally reinforcedeach other, because you have pointed so some good hubs. And so you have pointed to some goodauthorities to get a good hubs score, whereas those authorityscores would be also improved because theyare pointing to by a good hub. And this is algorithms is also general itcan have many applications in graph and network analysis. So just briefly, here's how it works. We first also construct a matrix, but thistime we're going to construct an adjacent matrix andwe're not going to normalize the values. So if there's a link there's a 1,if there's no link that's 0. Again, it's the same graph. And then we're going todefine the hubs score of page as the sum of the authority scores ofall the pages that it appoints to. So whether you are hub, really depends on whether you are pointingto a lot of good authority pages. That's what it says in the first equation. In the second equation,we define the authorities of a page as a sum of the hub scores of allthose pages that appoint to you. So whether you are good authoritywould depend on whether those pages that are pointingto you are good hubs. So you can see this formsiterative reinforcement mechanism. Now, these three questions can bealso written in the metrics format. So what we get here is then the hubvector is equal to the product of the adjacency matrix andthe authority vector, and this is basically the first equation. And similarly, the second equationcan be returned as the authority vector is equal to the product ofa transpose multiplied by the hub vector. Now, these are just different waysof expressing these equations. But what's interesting is thatif you look at the matrix form, you can also plug in the authorityequation into the first one. So if you do that, you have actuallyeliminated the authority vector completely and you get the equationsof only hubs scores. The hubs score vector isequal to a multiplied by a transpose multipliedby the hub score again. Similarly, we can do a transformationto have equation for just the authorities also. So although we frame the problemas computing hubs and authorities, we can actually eliminate one of them toobtain equation just for one of them. Now, the difference between this and pagerandom is that now the matrix is actually a multiplication of the adjacencymatrix and it's transpose. So this is different from page rank. But mathematically, then we willbe computing the same problem. So in HITS,we typically would initialize the values. Let's say, 1 for all these values, and then we would iteratively applythese equations, essentially. And this is equivalent to multiplythat by the metrics a and a transpose. So the arrows of these is exactlythe same in the PageRank. But here because the adjacencymatrix is not normalized. So what we have to do is after eachiteration we're going to normalize, and this would allow us tocontrol the growth of value. Otherwise they would grow larger andlarger. And if we do that, andthat will basically get HITS. That was the computer, the hubs scores,and authority scores for all the pages. And these scores can then be used inbranching just like the PageRank scores. So to summarize in this lecture, we haveseen that link information's very useful. In particular,the anchor text is very useful to increase the textrepresentation of a page. And we also talk about the PageRank and page anchor as two majorlink analysis algorithms. Both can generate scores for web pagesthat can be used in the ranking function. Note that PageRank andthe HITS are also very general algorithms. So they have many applications inanalyzing other graphs or networks. [MUSIC]
[SOUND]And here we're going to talkabout basic strategy. And that would be based onsimilarity of users and then predicting the rating of and object by an active user using the ratingsof similar users to this active user. This is called a memory based approachbecause it's a little bit similar to storing all the user information and when we are considering a particularuser we going to try to retrieve the rating users orthe similar users to this user case. And then try to use thisinformation about those users to predict the preference of this user. So here is the general idea andwe use some notations here, so x sub i j denotes the ratingof object o j by user u i and n sub i is average ratingof object by this user. So this n i is needed because we would like to normalizethe ratings of objects by this user. So how do you do normalization? Well, we're going to just subtractthe average rating from all the ratings. Now, this is to normalize these ratings so that the ratings from differentusers would be comparable. Because some users might be more generous,and they generally give more high ratings but some others might bemore critical so their ratings cannot be directly compared with eachother or aggregate them together. So we need to do this normalization. Another prediction ofthe rating on the item by another user oractive user, u sub a here can be based on the averageratings of similar users. So the user u sub a is the user that weare interested in recommending items to. And we now are interested inrecommending this o sub j. So we're interested in knowing howlikely this user will like this object. How do we know that? Where the idea here is to look atwhether similar users to this user have liked this object. So mathematically this is to saywell the predicted the rating of this user on this app object,user a on object o j is basically combination of the normalizedratings of different users, and in fact here,we're taking a sum over all the users. But not all users contributeequally to the average, and this is conjured by the weights. So this weight controls the inference of the user on the prediction. And of course,naturally this weight should be related to the similarity between ua andthis particular user, ui. The more similar they are,then the more contribution user ui can make in predictingthe preference of ua. So, the formula is extremely simple. You can see,it's a sum of all the possible users. And inside the sum we have their ratings,well, their normalized ratingsas I just explained. The ratings need to be normalized inorder to be comparable with each other. And then these ratingsare weighted by their similarity. So you can imagine w of a and i is justa similarity of user a and user i. Now what's k here? Well k is simply a normalizer. It's just one over the sum of allthe weights, over all the users. So this means, basically, if you considerthe weight here together with k, and we have coefficients of weight thatwill sum to one for all the users. And it's just a normalization strategy sothat you get this predictor rating in the same range as these ratingsthat we used to make the prediction. Right? So this is basically the main ideaof memory-based approaches for collaborative filtering. Once we make this prediction,we also would like to map back through the rating that the user would actually make, and this is to furtheradd the mean rating or average rating of this user usub a to the predicted value. This would recover a meaningful rating forthis user. So if this user is generous, thenthe average it would be is somewhat high, and when we add that the rating will beadjusted to our relatively high rate. Now when you recommend an item to a userthis actually doesn't really matter, because you are interested inbasically the normalized reading, that's more meaningful. But when they evaluate theserather than filter approaches, they typically assume thatactual ratings of the user on these objects to be unknown andthen you do the prediction and then you compare the predictedratings with their actual ratings. So, you do have accessto the actual ratings. But, then you pretend that you don't know,and then you compare your systemspredictions with the actual ratings. In that case, obviously, the systemsprediction would be adjusted to match the actual ratings of the user andthis is what's happening here basically. Okay so this is the memory based approach. Now, of course,if you look at the formula, if you want to writethe program to implement it, you still face the problem ofdetermining what is this w function? Once you know the w function, thenthe formula is very easy to implement. So, indeed, there are many different waysto compute this function or this weight, w, and specific approaches generallydiffer in how this is computed. So here are some possibilities and you can imagine thereare many other possibilities. One popular approach is we usethe Pearson correlation coefficient. This would be a sum overcommonly rated items. And the formula is a standardappears in correlation coefficient formula as shown here. So this basically measureswhether the two users tended to all give higher ratings to similaritems or lower ratings to similar items. Another measure is the cosine measure,and this is going to treat the rating vectors as vectors in the vector space. And then,we're going to measure the angle and compute the cosine ofthe angle of the two vectors. And this measure has been using the vectorspace model for retrieval, as well. So as you can imagine there are justas many different ways of doing that. In all these cases, note that the user'ssimilarity is based on their preferences on items and we did not actually useany content information of these items. It didn't matter these items are,they can be movies, they can be books, they can be products, they can be text documents whichhas been cabled the content and so this allows such approach to beapplied to a wide range of problems. Now in some newer approaches of course, we would like to use moreinformation about the user. Clearly, we know more about the user,not just these preferences on these items. So in the actual filtering system,is in collaborative filtering, we could also combine thatwith content based filtering. We could use more context information,and those are all interesting approaches that people are just starting, andthere are new approaches proposed. But, this memory based approach hasbeen shown to work reasonably well, and it's easy to implement inpractical applications this could be a starting point to see if the strategyworks well for your application. So, there are some obvious waysto also improve this approach and mainly we would like to improvethe user similarity measure. And there are some practicalissues we deal with here as well. So for example,there will be a lot of missing values. What do you do with them? Well, you can set them to default valuesor the average ratings of the user. And that would be a simple solution. But there are advanced approaches thatcan actually try to predict those missing values, and then use predictivevalues to improve the similarity. So in fact that the memory based apologycan predict those missing values, right? So you get you have iterative approachwhere you first use some preliminary prediction and then you can use the predictive values tofurther improve the similarity function. So this is a heuristicway to solve the problem. And the strategy obviously would affectthe performance of claritative filtering just like any other heuristics wouldimprove these similarity functions. Another idea which is actually verysimilar to the idea of IDF that we have seen in text search is calleda Inverse User Frequency or IUF. Now here the idea is to look at wherethe two users share similar ratings. If the item is a popular item thathas been viewed by many people and seen [INAUDIBLE] to people interestedin this item may not be so interesting but if it's a rare item,it has not been viewed by many users. But these two users deal with thisitem and they give similar ratings. And, that says moreabout their similarity. It's kind of to emphasizemore on similarity on items that are notviewed by many users. [MUSIC]
In this lecture, we continue discussing ParadigmaticalRelation Discovery. Earlier we introduceda method called Expected Overlap ofWords in Context. In this method, werepresent each context by a word vector that represents the probability of aword in the context. We measure the similarityby using the.product, which can be interpreted asthe probability that two randomly picked words from the two contexts are identical. We also discussedthe two problems of this method. The first is thatit favors matching one frequent term very well over matching more distinct terms. It put too much emphasis onmatching one term very well. The second is that ittreats every word equally. Even a common word likethe will contribute equally as contentword like eats. So now we are going to talk about howto solve these problems. More specifically, we'regoing to introduce some retrieval heuristicsused in text retrieval. These heuristics can effectivelysolve these problems, as these problems alsooccur in text retrieval when we match a query thatthough with a document vector. So to address the first problem, we can use a sublineartransformation of tone frequency. That is, we don't have to use the raw frequency count of a term to represent the context. We can transformit into some form that wouldn't emphasize somuch on the raw frequency. To address thesynchronous problem, we can put more weighton rare terms. That is we can rewardmatching a real-world. This heuristic is called the IDF term weighting in text retrieval. IDF stands forInverse Document Frequency. So now, we're going to talk about the two heuristicsin more detail. First let's talk aboutthe TF Transformation. That is to convertthe raw count of a word in the documentinto some weight that reflects our belief about how importantthis word in the document. So that will bedenoted by TF of w,d. That's shown in the y-axis. Now, in general, there aremany ways to map that. Let's first look atthe simple way of mapping. In this case, we'regoing to say, well, any non-zero countswill be mapped to one and the zero countwill be mapped to zero. So with this mapping all the frequencies will be mapped to only twovalues; zero or one. The mapping function is shownhere as a flat line here. Now, this is naive because it's notthe frequency of words. However, this actuallyhas the advantage of emphasizing matching allthe words in the context. So it does not allow a frequency of word todominate the matching. Now, the approachthat we have taken earlier in the expectedoverlap count approach, is a linear transformation. We basically, takey as the same as x. So we use the raw countas a representation. That created the problem that we just talked about namely; it emphasize too much on justmatching one frequent term. Matching one frequent termcan contribute a lot. So we can have a lot of other interestingtransformations in between the two extremes, and they generally forma sublinear transformation. So for example,one possibility is to take logarithm of the raw count, and this will give us curvethat looks like this, that you are seeing here. In this case, you can seethe high frequency counts. The high counts arepenalize a little bit, so the curve is a sublinearcurve and it brings down the weight ofthose really high counts. This is what we want,because it prevents that terms from dominatingthe scoring function. Now, there is also another interestingtransformation called a BM25 transformation which has been shown to be veryeffective for retrieval. In this transformation, we have a form that looks like this. So it's k plus one multipliedby x divided by x plus k, where k is a parameter, x is the count, the raw count of a word. Now, the transformationis very interesting in that it can actually go from one extreme to the otherextreme by varying k. It also interestingthat it has upper bound, k plus one in this case. So this putsa very strict constraint on high frequency terms, because their weight wouldnever exceed k plus one. As we vary k, if we cansimulate the two extremes. So when k is set to zero, we roughly have the 0,1 vector. Whereas when we set kto a very large value, it will behave more likethe linear transformation. So this transformationfunction is by far the most effectivetransformation function for text retrieval and it also makes sense for our problem setup. So we just talked about howto solve the problem of overemphasizing a frequency term Now let's look atthe second problem, and that is how we canpenalize popular terms. Matching "the" is not surprising, because "the" occurs everywhere. But matching "eats"would count a lot. So how can we addressthat problem? Now in this case, we canuse the IDF weighting. That's commonlyused in retrieval. IDF stands forInverse Document Frequency. Document frequencymeans the count of the total number of documents that containa particular word. So here we show that the IDFmeasure is defined as a logarithm functionof the number of documents that match aterm or document frequency. So K is the number ofdocuments containing word or document frequency and M here is the total number ofdocuments in the collection. The IDF function is givinga higher value for a lower K, meaning that itrewards rare term. The maximum value islog of M plus one. That's when the word occurredjust once in a context. So that's a very rare term, the rare is term inthe whole collection. The lowest value you cansee here is when K reaches its maximum which would be M. So that would bea very low value, close to zero in fact. So this of course measure is used in search where wenaturally have a collection. In our case, what wouldbe our collection? Well, we can alsouse the context that we can collect all the wordsas our collection. That is to say, a word that's popular inthe collection in general, would also have a low IDF. Because depending on the dataset, we can construct the contextvectors in different ways. But in the end if a term is very frequent inthe original dataset, then it will still be frequent in the collectivecontext documents. So how can we addthese heuristics to improve our similarity function? Well, here's one wayand there are many other waysthat are possible. But this is a reasonable way, where we can adaptthe BM25 retrieval model for paradigmaticalrelation mining. In this case, we define thedocument vector as containing elements representingnormalized BM25 values. So in thisnormalization function, we take sum over allthe words and we normalize the weight ofeach word by the sum of the weights of all the words. This is to again ensure all the xi's will sum toone in this vector. So this would be very similarto what we had before, in that this vector is actually something similarto a word distribution, all the xi's will sum to one. Now, the weight of BM25 foreach word is defined here. If you compare this withour old definition where we just have a normalized countof this one, right? So we only have this oneand the document lens or the total counts of words inthat context to document, and that's what we had before. But now with the BM25transformation, we introduced something else. First, of course,this extra occurrence of this count is just to achieve the sub-linearnormalization. But we also see we introducedthe parameter, k, here, and this parameter isgenerally a non-active number, although zero is also possible. But this controlsthe upper bound, and also controls to what extent it simulates thelinear transformation. So this is one parameter, but we also see there isanother parameter here, b, and this would bewithin zero and one. This is a parameter tocontrol lens normalization. In this case, the normalization formula has a average document lens here. This is computed upby taking the average of the lenses of all thedocuments in the collection. In this case, all the lenses of all the context of documentsthat we're considering. So this average documents will be a constant forany given collection. So it actually is only affecting the effectof the parameter, b, here becausethis is a constant. But I kept it here becauseit's a constant that's used for in retrieval where it would give us a stabilizedinterpretation of parameter, b. But for our purpose, this will be a constant soit would only be affecting the lens normalizationtogether with parameter, b. Now, with this definition then, we have a new way to defineour document of vectors, and we can computethe vector d2 in the same way. The difference is that the high-frequency terms will now have a somewhat lower weights. This would help us control the inference ofthese high-frequency terms. Now, the idea can be addedhere in the scoring function. That means we'llintroduce a weight for matching each term. So you may recallthis sum indicates all the possible wordsthat can be overlap between the two contexts. The x_i and the y_iare probabilities of picking the wordfrom both contexts. Therefore, itindicates how likely we'll see a match on this word. Now, IDF would give us the importance ofmatching this word. A common word will be worthless than a rare word. So we emphasize more onmatching rare words now. So with this modification, then the new function will likely addressthose two problems. Now, interestinglywe can also use this approach to discoversyntagmatic relations. In general, when we re-brand a context with a term vector, we would likely see some terms have high weights and other terms have low weights. Depending on how we assignweights to these terms, we might be able touse these weights to discover the words that are strongly associated with the candidate wordin the context. So let's take a look at the term vector inmore detail here. We have each x_i defined as the normalizedweight of BM25. Now, this weight alone only reflects how frequent the wordoccurs in the context. But we can't just say any frequent term in the context that wouldbe correlated with the candidate word because many common words like 'the' will occur frequently inall the context. But if we apply IDFweighting as you see here, we can then re-weightthese terms based on IDF. That means the words that are common like 'the'will get penalized. So now the highestweighted terms will not be those common terms becausethey have lower IDFs. Instead, those terms would be the terms that arefrequent in the context, but not frequentin the collection. So those are clearly the wordsthat tend to occur in the context of the candidateword, for example, cat. So for this reason, the highly weighted terms inthis idea of weighted vector can also be assumed to be candidates forsyntagmatic relations. Now, of course, this isonly a by-product of our approach for discoveringparadigmatic relations. In the next lecture, we'regoing to talk more about how to discoversyntagmatic relations. But it clearly shows the relation between discoveringthe two relations. Indeed they can be discovered in a joint manner by leveragingsuch associations. So to summarize,the main idea for discovering paradigmatic relations is to collect the context of a candidate word toform a pseudo document. This is typically representedas a bag of words. Then compute the similarity of the correspondingcontext documents of two candidate words. Then we can takethe highly similar word pairs, and treat them as havingparadigmatic relations. These are the words thatshare similar contexts. There are many different ways to implement this general idea. We just talked aboutsome of the approaches. More specifically, wetalked about using text retrieval models to help us design effectivesimilarity function to compute theparadigmatic relations. More specifically, we have used the BM25 and IDF weighting to discoverparadigmatic relation. These approaches also represent the state of the art intext retrieval techniques. Finally, syntagmatic relationscan also be discovered as a by-product when we discoverparadigmatic relations.
[MUSIC] So now let's talk about the problema little bit more, and specifically let's talk about the two different waysof estimating the parameters. One is called the Maximum Likelihoodestimate that I already just mentioned. The other is Bayesian estimation. So in maximum likelihood estimation,we define best as meaning the data likelihoodhas reached the maximum. So formally it's givenby this expression here, where we define the estimate as a argmax of the probability of x given theta. So, arg max here just means itsactually a function that will turn. The argument that gives the functionmaximum value, adds the value. So the value of arg max is notthe value of this function. But rather, the argument that hasmade it the function reaches maximum. So in this case the valueof arg max is theta. It's the theta that makes the probabilityof X, given theta, reach it's maximum. So this estimate that in due it alsomakes sense and it's often very useful, and it seeks the premisethat best explains the data. But it has a problem, when the datais too small because when the data points are too small,there are very few data points. The sample is small,then if we trust data in entirely and try to fit the data andthen we'll be biased. So in the case of text data,let's say, all observed 100 words did not contain anotherword related to text mining. Now, our maximum likelihood estimatorwill give that word a zero probability. Because giving the non-zero probability would take away probabilitymass from some observer word. Which obviously is not optimal interms of maximizing the likelihood of the observer data. But this zero probability for all the unseen words may notbe reasonable sometimes. Especially, if we want the distributionto characterize the topic of text mining. So one way to address this problem isactually to use Bayesian estimation, where we actually would lookat the both the data, and our prior knowledge about the parameters. We assume that we have some priorbelief about the parameters. Now in this case of course, so we are not going to look at just the data,but also look at the prior. So the prior here isdefined by P of theta, and this means, we will impose somepreference on certain theta's of others. And by using Bayes Rule,that I have shown here, we can then combinethe likelihood function. With the prior to give us this posterior probability of the parameter. Now, a full explanation of Bayes rule,and some of these things related to Bayesian reasoning,would be outside the scope of this course. But I just gave a briefintroduction because this is general knowledge thatmight be useful to you. The Bayes Rule is basically defined here,and allows us to write down oneconditional probability of X given Y in terms of the conditionalprobability of Y given X. And you can see the two probabilities are different in the orderof the two variables. But often the rule is used formaking inferences of the variable, solet's take a look at it again. We can assume that p(X) Encodesour prior belief about X. That means before we observe any otherdata, that's our belief about X, what we believe some X values havehigher probability than others. And this probability of X given Y is a conditional probability, andthis is our posterior belief about X. Because this is our belief about Xvalues after we have observed the Y. Given that we have observed the Y,now what do we believe about X? Now, do we believe some values havehigher probabilities than others? Now the two probabilitiesare related through this one, this can be regarded as the probability of the observed evidence Y,given a particular X. So you can think about Xas our hypothesis, and we have some prior belief aboutwhich hypothesis to choose. And after we have observed Y,we will update our belief and this updating formula is basedon the combination of our prior. And the likelihood of observingthis Y if X is indeed true, so much for detour about Bayes Rule. In our case, what we are interestedin is inferring the theta values. So, we have a prior here that includesour prior knowledge about the parameters. And then we have the data likelihood here, that would tell us which parametervalue can explain the data well. The posterior probabilitycombines both of them, so it represents a compromiseof the the two preferences. And in such a case, we can maximizethis posterior probability. To find this theta that wouldmaximize this posterior probability, and this estimator is called a Maximuma Posteriori, or MAP estimate. And this estimator is a more general estimator thanthe maximum likelihood estimator. Because if we define our prioras a noninformative prior, meaning that it's uniformover all the theta values. No preference, then we basically would goback to the maximum likelihood estimated. Because in such a case,it's mainly going to be determined by this likelihood value, the same as here. But if we have some not informative prior,some bias towards the different values then map estimatorcan allow us to incorporate that. But the problem here of course,is how to define the prior. There is no free lunch and if you want tosolve the problem with more knowledge, we have to have that knowledge. And that knowledge,ideally, should be reliable. Otherwise, your estimate may notnecessarily be more accurate than that maximum likelihood estimate. So, now let's look at the Bayesianestimation in more detail. So, I show the theta values as just a one dimension value andthat's a simplification of course. And so, we're interested in whichvariable of theta is optimal. So now, first we have the Prior. The Prior tells us thatsome of the variables are more likely the others would believe. For example, these values are morelikely than the values over here, or here, or other places. So this is our Prior, and then we have our theta likelihood. And in this case, the theta also tells uswhich values of theta are more likely. And that just means loose syllablescan best expand our theta. And then when we combine the twowe get the posterior distribution, and that's just a compromise of the two. It would say that it'ssomewhere in-between. So, we can now look at someinteresting point that is made of. This point represents the mode of prior,that means the most likely parameter value according to our prior,before we observe any data. This point is the maximumlikelihood estimator, it represents the theta that givesthe theta of maximum probability. Now this point is interesting,it's the posterior mode. It's the most likely value of the thetagiven by the posterior of this. And it represents a goodcompromise of the prior mode and the maximum likelihood estimate. Now in general in Bayesian inference,we are interested in the distribution of all theseparameter additives as you see here. If there's a distribution oversee how values that you can see. Here, P of theta given X. So the problem of Bayesian inference is to infer this posterior, this regime, and also to infer other interestingquantities that might depend on theta. So, I show f of theta here as an interesting variablethat we want to compute. But in order to compute this value,we need to know the value of theta. In Bayesian inference,we treat theta as an uncertain variable. So we think about allthe possible variables of theta. Therefore, we can estimate the value ofthis function f as extracted value of f, according to the posterior distributionof theta, given the observed evidence X. As a special case, we can assume fof theta is just equal to theta. In this case,we get the expected value of the theta, that's basically the posterior mean. That gives us also one point of theta, and it's sometimes the same as posterior mode,but it's not always the same. So, it gives us another wayto estimate the parameter. So, this is a general illustration ofBayesian estimation and its an influence. And later,you will see this can be useful for topic mining where we want to injectthe sum prior knowledge about the topics. So to summarize,we've used the language model which is basically probabilitydistribution over text. It's also called a generative model fortext data. The simplest language modelis Unigram Language Model, it's basically a word distribution. We introduced the conceptof likelihood function, which is the probability ofthe a data given some model. And this function is very important, given a particular set of parametervalues this function can tell us which X, which data point has a higher likelihood,higher probability. Given a data sample X,we can use this function to determine which parameter values would maximizethe probability of the observed data, and this is the maximumlivelihood estimate. We also talk about the Bayesianestimation or inference. In this case we, must define a prioron the parameters p of theta. And then we're interested in computing theposterior distribution of the parameters, which is proportional to the prior andthe likelihood. And this distribution would allow us thento infer any derive that is from theta. [MUSIC]
[SOUND]So to summarize our discussion ofrecommender systems, in some sense, the filtering task forrecommender task is easy, and in some other sense,the task is actually difficult. So it's easy becausethe user's expectation is low. In this case the system takes initiativeto push information to the user. The user doesn't really make any effort,so any recommendation is better than nothing. All right.So, unless you recommend the noise items or useless documents. If you can recommendsome useful information users generally will appreciate it,so that's, in that sense that's easy. However, filtering is actually much hardertask than retrieval because you have to make a binary decision and you can'tafford waiting for a lot of items and then you're going to see whetherone item is better than others. You have to make a decisionwhen you see this item. Think about news filtering. As soon as you see the news enoughto decide whether the news would be interesting to the user. If you wait for a few days, well, even ifyou can make accurate recommendation of the most relevant news, the utility isgoing to be significantly decreased. Another reason why it's hardis because of data sparseness if you think of thisas a learning problem. Collaborative filtering, for example, is purely based onlearning from the past ratings. So if you don't have many ratings there'sreally not that much you can do, right? And yeah I just mentionedthis cold start problem. This is actually a very serious,serious problem. But of course there are strategies thathave been proposed for the soft problem, and there are different strategies thatyou can use to alleviate the problem. You can use, for example, more userinformation to asses their similarity, instead of using the preferencesof these users on these items give me additional informationavailable about the user, etc. And we also talk about two strategies forfiltering task. One is content-based wherewe look at items there is collaborative filtering wherewe look at Use a similarity. And they obviously can becombined in a practical system. You can imagine they generallywould have to be combined. So that would give us a hybridstrategy for filtering. And we also could recall that we talked about push versus pull as two strategiesfor getting access to the text data. And recommender system easy tohelp users in the push mode, and search engines are servingusers in the pull mode. Obviously the two should be combined,and they can be combined. The two have a systemthat can support user with multiple mode information access. So in the future we could anticipate sucha system to be more useful the user. And either,this is an active research area so there are a lot of new algorithmsbeing proposed all the time. In particular those new algorithms tendto use a lot of context information. Now the context here could bethe context of the user and could also be the context of the user. Items. The items are not the isolated. They're connected in many ways. The users might formsocial network as well, so there's a rich context therethat we can leverage in order to really solve the problem well andthen that's active research area where also machinelearning algorithms have been applied. Here are some additional readings in the handbook calledRecommender Systems and has a collection of a lotof good articles that can give you an overviewof a number of specific approaches through recommender systems. [MUSIC]
##  **CS410 Technology Review (4-credit students only)**  **CS410 Technology Review**  The Technology Review assignment is designed to provide students with an opportunity to go beyond the materials covered in the course lectures to learn about an interesting course-related cutting-edge technology topic not covered in any lecture. In this assignment, a student is required to write a short review article on a chosen topic by the student from a list of suggested topics by the instructor and TAs. A student can also propose a topic not on the list subject to the approval of the instructor.  The topic of a technology review can be one of the three broad topic categories related to the general topic of “text data retrieval and analysis”:  1) Useful software toolkits for processing text data or building text data applications.  2) Emerging new applications of text retrieval or analysis.  3) New techniques for text retrieval or analysis.  A list of specific topics will be provided for students to choose, but students may also propose additional topics interesting to them subject to the approval of the instructor. A review may cover one toolkit/application/technique in-depth or compare multiple toolkits/applications/techniques. The former is only allowed if the toolkit/application/technique is sufficiently complex to justify devoting an entire review to it. In any case, your review must have novel content that does not exist in any existing literature or webpages so that it would offer unique information/knowledge that others can learn from your review. So please make sure to check whether there is already a review on the topic before you devote time to complete a review. If you find an existing review on the topic, you may still write about the topic, but just need to make sure that you take a somewhat different perspective than the existing review or add new content on top of the existing review (i.e., extending it in some way).  The Technology Review should be completed individually. It will be graded based on completion of the following two tasks  1\. **Topic proposal** : Every student is required to select a topic from a provided topic list or propose a topic by the **end of Week 8 (Oct 17, 2021)** in the signup sheet (access with illinois.edu email address): [ https://docs.google.com/spreadsheets/d/1v-RYD- E_KgqFnAdt7IvYHb9svNtlTla5HcOEfYSZyQM/edit?usp=sharing ](https://docs.google.com/spreadsheets/d/1v-RYD- E_KgqFnAdt7IvYHb9svNtlTla5HcOEfYSZyQM/edit?usp=sharing "https://docs.google.com/spreadsheets/d/1v-RYD- E_KgqFnAdt7IvYHb9svNtlTla5HcOEfYSZyQM/edit?usp=sharing")  Some sample topics are provided here: [ https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing ](https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing "https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci- VPOs/edit?usp=sharing")  2\. **Review submission** : Each student is required to submit a complete Technology Review by the **end of Week 11 (Nov 7, 2021)** . The review must have a coherent storyline (Intro, Body, Conclusion) and cite relevant references. It must be at least ~2 pages  The deadline is set to an earlier time than that of course project code submission so as to give the students an opportunity to read some relevant reviews (especially those on toolkits) before finishing their projects if they want to.  
[SOUND] This lecture is a continued discussion of probabilistic topic models. In this lecture, we're going to continuediscussing probabilistic models. We're going to talk abouta very simple case where we are interested in just miningone topic from one document. So in this simple setup,we are interested in analyzing one document andtrying to discover just one topic. So this is the simplestcase of topic model. The input now no longer has k,which is the number of topics because we know there is only one topic and thecollection has only one document, also. In the output,we also no longer have coverage because we assumed that the documentcovers this topic 100%. So the main goal is just to discoverthe world of probabilities for this single topic, as shown here. As always, when we think about using agenerating model to solve such a problem, we start with thinking about whatkind of data we are going to model or from what perspective we're going tomodel the data or data representation. And then we're going todesign a specific model for the generating of the data,from our perspective. Where our perspective just means we wantto take a particular angle of looking at the data, so that the model willhave the right parameters for discovering the knowledge that we want. And then we'll be thinkingabout the microfunction or write down the microfunction tocapture more formally how likely a data point will beobtained from this model. And the likelihood function will havesome parameters in the function. And then we argue our interest inestimating those parameters for example, by maximizing the likelihood which willlead to maximum likelihood estimated. These estimator parameterswill then become the output of the mining hours,which means we'll take the estimating parameters as the knowledgethat we discover from the text. So let's look at these steps forthis very simple case. Later we'll look at this procedure forsome more complicated cases. So our data, in this case is, justa document which is a sequence of words. Each word here is denoted by x sub i. Our model is a Unigram language model. A word distribution that we hope todenote a topic and that's our goal. So we will have as many parameters as manywords in our vocabulary, in this case M. And for convenience we'regoing to use theta sub i to denote the probability of word w sub i. And obviously these thetasub i's will sum to 1. Now what does a likelihoodfunction look like? Well, this is just the probabilityof generating this whole document, that given such a model. Because we assume the independence ingenerating each word so the probability of the document will be just a productof the probability of each word. And since some word mighthave repeated occurrences. So we can also rewrite thisproduct in a different form. So in this line, we have rewrittenthe formula into a product over all the unique words inthe vocabulary, w sub 1 through w sub M. Now this is differentfrom the previous line. Well, the product is over differentpositions of words in the document. Now when we do this transformation,we then would need to introduce a counter function here. This denotes the count ofword one in document and similarly this is the countof words of n in the document because these words mighthave repeated occurrences. You can also see if a word didnot occur in the document. It will have a zero count, thereforethat corresponding term will disappear. So this is a very useful form of writing down the likelihood functionthat we will often use later. So I want you to pay attention to this,just get familiar with this notation. It's just to change the product over allthe different words in the vocabulary. So in the end, of course, we'll usetheta sub i to express this likelihood function and it would look like this. Next, we're going to findthe theta values or probabilities of these words that would maximizethis likelihood function. So now lets take a look at the maximumlikelihood estimate problem more closely. This line is copied fromthe previous slide. It's just our likelihood function. So our goal is to maximizethis likelihood function. We will find it often easy to maximize the local likelihoodinstead of the original likelihood. And this is purely formathematical convenience because after the logarithm transformation our functionwill becomes a sum instead of product. And we also have constraintsover these these probabilities. The sum makes it easier to takederivative, which is often needed for finding the optimalsolution of this function. So please take a look at this sum again,here. And this is a form ofa function that you will often see later also,the more general topic models. So it's a sum over allthe words in the vocabulary. And inside the sum there isa count of a word in the document. And this is macroed bythe logarithm of a probability. So let's see how we cansolve this problem. Now at this point the problem is purely amathematical problem because we are going to just the find the optimal solutionof a constrained maximization problem. The objective function isthe likelihood function and the constraint is that all theseprobabilities must sum to one. So, one way to solve the problem isto use Lagrange multiplier approace. Now this command is beyondthe scope of this course but since Lagrange multiplier is a veryuseful approach, I also would like to just give a brief introduction to this,for those of you who are interested. So in this approach we willconstruct a Lagrange function, here. And this function will combineour objective function with another term thatencodes our constraint and we introduce Lagrange multiplier here, lambda, so it's an additional parameter. Now, the idea of this approach is just toturn the constraint optimization into, in some sense,an unconstrained optimizing problem. Now we are just interested inoptimizing this Lagrange function. As you may recall from calculus,an optimal point would be achieved whenthe derivative is set to zero. This is a necessary condition. It's not sufficient, though. So if we do that you willsee the partial derivative, with respect to theta ihere ,is equal to this. And this part comes from the derivativeof the logarithm function and this lambda is simply taken from here. And when we set it to zero we can easily see theta sub i isrelated to lambda in this way. Since we know all the thetai's must a sum to one we can plug this into this constraint,here. And this will allow us to solve forlambda. And this is just a netsum of all the counts. And this further allows us to thensolve the optimization problem, eventually, to find the optimalsetting for theta sub i. And if you look at this formula it turnsout that it's actually very intuitive because this is just the normalizedcount of these words by the document ns, which is also a sum of allthe counts of words in the document. So, after all this mess, after all, we have just obtained somethingthat's very intuitive and this will be just ourintuition where we want to maximize the data byassigning as much probability mass as possible to allthe observed the words here. And you might also notice that this isthe general result of maximum likelihood raised estimator. In general, the estimator would be tonormalize counts and it's just sometimes the counts have to be done in a particularway, as you will also see later. So this is basically an analyticalsolution to our optimization problem. In general though, when the likelihoodfunction is very complicated, we're not going to be able to solve the optimizationproblem by having a closed form formula. Instead we have to use somenumerical algorithms and we're going to see such cases later, also. So if you imagine what would weget if we use such a maximum likelihood estimator to estimate onetopic for a single document d here? Let's imagine this documentis a text mining paper. Now, what you might see issomething that looks like this. On the top, you will see the highprobability words tend to be those very common words,often functional words in English. And this will be followed bysome content words that really characterize the topic well like text,mining, etc. And then in the end,you also see there is more probability of words that are not reallyrelated to the topic but they might be extraneouslymentioned in the document. As a topic representation,you will see this is not ideal, right? That because the high probabilitywords are functional words, they are not reallycharacterizing the topic. So my question is how can weget rid of such common words? Now this is the topic of the next module. We're going to talk about how to useprobabilistic models to somehow get rid of these common words. [MUSIC]
##  SCHEDULE AN EXAM WITH ProctorU  **Step 1: Test your equipment** 1\. Check the Technical Requirements for Online Proctored Exam to make sure you have the right computer equipment. 2\. Test your system’s equipment using the ProctorU System Test. **Step 2: Create Your ProctorU User Account** 1\. If this is your first time scheduling an exam with ProctorU, click the Sign Up link on the University of Illinois ProctorU portal page to create a new user account and ProctorU log-in/password. 2\. Your browser will ask you to allow ProctorU to use your camera to take a snapshot of you. Note: this step can be skipped if you don’t wish to take a snapshot at this time. In that case, the proctor will upload a picture of you during your first testing session. 3\. Fill out all the other fields on the account creation page. Your Username can be any name you choose. Choose a name you will easily remember (perhaps the first part of your email address before the @ symbol). 4\. Accept the Terms of Service. 5\. Click the Create Account button to create your account. 6\. On the My Preferences page, select from the drop-downs to choose the optimal time to take your exams (e.g., weekdays, weekends, mornings, evenings, etc.) IMPORTANT: depending on the specific exam window for your course, your preferred time may or may not be available. 7\. Click the Save & Update button. **Step 3: Schedule Your Exam** 1\. From the My Exams page, click the Schedule New Exam button to schedule your exam date and time. 2\. Fill in the institution, term, and exam from the dropdown menus, and click the Find Reservations button. 3\. On the Schedule Exam page, select a reservation time from the calendar at the bottom of the screen. IMPORTANT: If you do not see any reservations listed, please select the "View All" radio button next to "Filter Results" to display exam times that are outside your specified preference. 4\. Click the "BOOK IT" button next to your desired exam appointment time. 5\. Confirm your selection on the next page and click the Proceed to Cart button when you are ready to continue. 6\. In your cart, click the Proceed to Checkout button. 7\. Enter the appropriate credit card information and click Make Payment. {note that some sections have the exams paid by the department} 8\. You will see an exam confirmation page and will receive an email message with your scheduled exam information. **Step 4: Take Your Exam** 1\. At the date and time of your appointment, login to the University of Illinois ProctorU portal page. 2\. After logging in, you will see a countdown to your exam time at the top of the page. Prior to your exam appointment, you may reschedule using the Reschedule button. 3\. At the appointment time, a Start button will appear next to the appointment. Click the Start button, and you will be connected to a proctor who will guide you through the rest of your proctored exam process.  
[NOISE] This lecture is a summary of this course. This map shows the major topicswe have covered in this course. And here are some keyhigh-level take-away messages. First, we talked about naturallanguage content analysis. Here the main take-away messagesis natural language processing is a foundation for text retrieval, butcurrently the NLP isn't robust enough so the battle of wars is generally the mainmethod used in modern search engines. And it's often sufficient beforemost of the search tasks, but obviously formore complex search tasks then we need a deeper natural languageprocessing techniques. We then talked about the highlevel strategies for text access andwe talked about push versus pull. In pull we talked aboutquerying versus browsing. Now in general in future search engines,we should integrate all these techniques to provide a math involvedinformation access. And now we'll talk about a number ofissues related to search engines. We talked about the search problem. And we framed that as a ranking problem. And we talked about a numberof retrieval methods. We start with the overviewof vector space model and the probabilistic model and then we talkedabout the vector space model in depth. We also later talked aboutthe language modeling approach, and that's probabilistic model. And here, many take-away message is thatthe modeling retrieval function tend to look similar, andthey generally use various heuristics. Most important ones are TF-IDF weighting,document length normalization. And the TF is often transformed througha sub media transformation function. And then we talked about how toimplement a retrieval system, and here, the main techniques that we talked about,how to construct an inverted index so that we can prepare the systemto answer a query quickly. And we talked about how to do a fastersearch by using the inverted index. And we then talked about how toevaluate the text retrieval system, mainly introduced tothe Cranfield Evaluation Methodology. This was a very importantevaluation methodology that can be applied to many tasks. We talked about the majorevaluation measures. So, the most important measures fora search engine are MAP, mean average precision,and nDCG Summarize the discount or accumulative gain and also precision andrecall are the two basic measures. And we then talked aboutfeedback techniques. And we talked about the Rocchioin the Vector Space Model and the mixture model andthe language modeling approach. Feedback is a very importanttechnique especially considering the opportunity of learning froma lot of pixels on the Web. We then talked about Web search. And here we talked about howto use parallel in that scene to solve the scalability issue in thatscene we're going to use the net reduce. Then we talked about how to use linkingpermission model app to improve search. We talked about page rank and hits as the major hours is toanalyzing links on the Web. We then talked aboutlearning through rank. This is the use of machine learningto combine multiple features for improvement scoring. Not only that the effectiveness can beimproved in using this approach, but we can also improve the robustness of the. The ranking function so that it'snot easy to expand a search engine. It just some features to promote the page. And finally we talked aboutthe future of Web search. About the some major reactionsthat we might to see in the future in improving the countof regeneration of such engines. And then finally we talked aboutthe recommended systems and, these are systems toincrement the push mode. And we'll talk about the two approaches,one is content-based, one is collaborative filtering andthey can be combined together. Now, an obvious missing piece in this picture is the user, so user interface is also an importantcomponent in any search engine. Even though the current search interfaceis relatively simple they actually have done a lot of studies of user interfaceswhere we do visualization for example. And this is the topic to that, you can learn more by reading this book. It's an excellent book about all kindsof studies of search using the face. If you want to know more aboutthe topics that we talked about, you can also read some additionalreadings that are listed here. In this short course we onlymanage to cover some basic topics in text retrievals andsearch engines. And these resources provide additionalinformation about more advanced topics and they give a more thorough treatment ofsome of the topics that we talked about. And a main source isthe Synthesis Digital Library that you can see a lot of shortto textbook or textbooks, or long tutorials. They tend to provide a lot ofinformation to explain a topic. And there a lot of series thatare related to this cause. One is information concepts,retrieval, and services. One is human langauge technology. And yet another is artificialintelligence and machine learning. There are also some major journals andconferences listed here that tend to have a lot of research paperswe need to and topic of this course. And finally, for more informationabout resources Including readings, tool kits, etc you can check out his URL. So, if you have not taken the textmining course in this data mining specialization series then naturallythe next step is to take that course. As this picture shows,to mine big text data, we generally need two kinds of techniques. One is text retrieval,which is covered in this course. And these techniques will help usconvert raw big text data into small relevant text data, which are actuallyneeded in the specific application. Now human plays important role in miningany text data because text data is written for humans to consume. So involving humans in the processof data mining is very important and in this course we have coveredthe various strategies to help users get access to the most relevant data. These techniques are always soessential in any text mining system to help provide prominence andto help users interpret the inner patterns that the user willdefine through text data mining. So, in general, the user would haveto go back to the original data to better understand the patterns. So the text mining cause, or rather,text mining and analytics course will be dealing with what to do oncethe user has a following information. So this is a second step in thispicture where we would convert the text data into actionable knowledge. And this has to do with helping users tofurther digest the found information or to find the patterns andto reveal knowledge. In text and such knowledge canthen be used in application systems to help decision making orto help a user finish a task. So, if you have not taken that course,the natural step and that natural next step wouldbe to take that course. Thank you for taking this course. I hope you had fun andfound this course to be useful to you. And I look forward to interactingwith you at a future opportunity. [MUSIC]
