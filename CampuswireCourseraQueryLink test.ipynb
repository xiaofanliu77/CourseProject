{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campuswire and Coursera Query Link\n",
    "## Data Collection and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for Data Collection and Cleaning\n",
    "import os\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coursera-DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "os.remove(\"documents.txt\")\n",
    "c = open(\"documents.txt\",'a+',encoding = \"utf-8\")\n",
    "for filename in os.listdir('Data'):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join('Data', filename)) as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('\\n',' ')\n",
    "            content = content.replace('[NOISE]',' ')\n",
    "            content = content.replace('[MUSIC]',' ')\n",
    "            content = content.replace('[SOUND]',' ')\n",
    "            content = content.replace('\\u2011',' ')\n",
    "            c.write(content + '\\n')\n",
    "            \n",
    "    if filename.endswith('.html'):\n",
    "        with open(os.path.join('Data', filename), encoding='utf8') as d:\n",
    "            content = d.read()\n",
    "            h = html2text.HTML2Text()\n",
    "            content = h.handle(content)\n",
    "            content = content.replace('\\n',' ')\n",
    "            content = content.replace('\\u2011',' ')\n",
    "            string = str(content)\n",
    "            c.write(string + '\\n')\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query relevance: query id, doc id, binary relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print with highlight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_highlight(content, query, doc_id=None):\n",
    "    highlight_list = []\n",
    "    for w in query.split(\" \"):\n",
    "        highlight_list.append(w.capitalize())\n",
    "        highlight_list.append(w.lower())\n",
    "\n",
    "\n",
    "    highlight_str = r\"\\b(?:\" + '|'.join(highlight_list) + r\")\\b\"\n",
    "    text_highlight = re.sub(highlight_str, '\\033[36;40m\\g<0>\\033[m', content)\n",
    "    if doc_id:\n",
    "        print(\"{}. {}...\\n\".format(d_id, text_highlight))\n",
    "    else:\n",
    "        print(text_highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Okapi BM25 Ranking Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for Ranking Function\n",
    "import metapy\n",
    "import math\n",
    "import pytoml\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = \"config.toml\"\n",
    "idx = metapy.index.make_inverted_index(cfg)\n",
    "ranker = metapy.index.OkapiBM25()\n",
    "\n",
    "with open(cfg, 'r') as fin:\n",
    "    cfg_d = pytoml.load(fin)\n",
    "\n",
    "query_cfg = cfg_d['query-runner']\n",
    "if query_cfg is None:\n",
    "    print(\"query-runner table needed in {}\".format(cfg))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the result (top 10 document ids and their scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 5.817067623138428), (28, 5.3951215744018555), (63, 5.162073612213135), (93, 5.086323261260986), (23, 4.743767738342285), (53, 4.711088180541992), (40, 4.505471706390381), (56, 4.466672897338867), (86, 4.31912088394165), (106, 4.309500217437744)]\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "query_path = query_cfg.get('query-path', 'CampusWireHeading_Queries.txt')\n",
    "query_start = query_cfg.get('query-id-start', 0)\n",
    "\n",
    "query = metapy.index.Document()\n",
    "query_text = \"Question on Probabilistic retrieval model\"\n",
    "\n",
    "query.content(query_text.lower())\n",
    "\n",
    "top_docs = ranker.score(idx, query, num_results=10)\n",
    "print(top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the document content with keywords highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.   This lecture is about the \u001b[36;40mProbabilistic\u001b[m \u001b[36;40mRetrieval\u001b[m \u001b[36;40mModel\u001b[m. In this lecture, we're going to continue the discussion of the Text \u001b[36;40mRetrieval\u001b[m Methods. We're going to look at another kind of very different way to design ranking functions than the Vector Space \u001b[36;40mModel\u001b[m that we discussed before. In \u001b[36;40mprobabilistic\u001b[m models, we define the ranking function, based \u001b[36;40mon\u001b[m the probability that this document is relevant to this query. In other words, we introduce a binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observations from random variables. Note that in the vector-based models, we assume they are vectors, but here we assume they are the data observed from random variables. And so, the problem of \u001b[36;40mretrieval\u001b[m becomes to estimate the probability of relevance. In this category of models, there are different variants. The classic \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m has led to the BM25 \u001b[36;40mretrieval\u001b[m function, which we discussed in in the vectors-based \u001b[36;40mmodel\u001b[m because its a form is actually similar to a backwards space \u001b[36;40mmodel\u001b[m. In this lecture, we will discuss another sub class in this P class called a language modeling approaches to \u001b[36;40mretrieval\u001b[m. In particular, we're going to discuss the query likelihood \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m, which is one of the most effective models in \u001b[36;40mprobabilistic\u001b[m models. There was also another line called the divergence from randomness \u001b[36;40mmodel\u001b[m which has led to the PL2 function, it's also one of the most effective state of the art \u001b[36;40mretrieval\u001b[m functions. In query likelihood, our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. So intuitively, this probability just captures the following probability. And that is if a user likes document d, how likely would the user enter query q ,in order to retrieve document d? So we assume that the user likes d, because we have a relevance value here. And then we ask the \u001b[36;40mquestion\u001b[m about how likely we'll see this particular query from this user? So this is the basic idea. Now, to understand this idea, let's take a look at the general idea or the basic idea of \u001b[36;40mProbabilistic\u001b[m \u001b[36;40mRetrieval\u001b[m Models. So here, I listed some imagined relevance status values or relevance judgments of queries and documents. For example, in this line, it shows that q1 is a query that the user typed in. And d1 is a document that the user has seen. And 1 means the user thinks d1 is relevant to q1. So this R here can be also approximated by the click-through data that a search engine can collect by watching how you interacted with the search results. So in this case, let's say the user clicked \u001b[36;40mon\u001b[m this document. So there's a 1 here. Similarly, the user clicked \u001b[36;40mon\u001b[m d2 also, so there is a 1 here. In other words, d2 is assumed to be relevant to q1. \u001b[36;40mOn\u001b[m the other hand, d3 is non-relevant, there's a 0 here. And d4 is non-relevant and then d5 is again, relevant, and so \u001b[36;40mon\u001b[m and so forth. And this part, maybe, data collected from a different user. So this user typed in q1 and then found that the d1 is actually not useful, so d1 is actually non-relevant. In contrast, here we see it's relevant. Or this could be the same query typed in by the same user at different times. But d2 is also relevant, etc. And then here, we can see more data about other queries. Now, we can imagine we have a lot of such data. Now we can ask the \u001b[36;40mquestion\u001b[m, how can we then estimate the probability of relevance? So how can we compute this probability of relevance? Well, intuitively that just means if we look at all the entries where we see this particular d and this particular q, how likely we'll see a one \u001b[36;40mon\u001b[m this other column. So basically that just means that we can just collect the counts. We can first count how many times we have seen q and d as a pair in this table and then count how many times we actually have also seen 1 in the third column. And then, we just compute the ratio. So let's take a look at some specific examples. Suppose we are trying to compute this probability for d1, d2 and d3 for q1. What is the estimated probability? Now, think about that. You can pause the video if needed. Try to take a look at the table. And try to give your estimate of the probability. Have you seen that, if we are interested in q1 and d1, we'll be looking at these two pairs? And in both cases, well, actually, in one of the cases, the user has said this is 1, this is relevant. So R = 1 in only one of the two cases. In the other case, it's 0. So that's one out of two. What about the d1 and the d2? Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1. So it's a two out of two and so \u001b[36;40mon\u001b[m and so forth. So you can see with this approach, we can actually score these documents for the query, right? We now have a score for d1, d2 and d3 for this query. And we can simply rank them based \u001b[36;40mon\u001b[m these probabilities and so that's the basic idea \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m. And you can see it makes a lot of sense, in this case, it's going to rank d2 above all the other documents. Because in all the cases, when you have c and q1 and d2, R = 1. The user clicked \u001b[36;40mon\u001b[m this document. So this also should show that with a lot of click-through data, a search engine can learn a lot from the data to improve their search engine. This is a simple example that shows that with even with small amount of entries here we can already estimate some probabilities. These probabilities would give us some sense about which document might be more relevant or more useful to a user for typing this query. Now, of course, the problems that we don't observe all the queries and all the documents and all the relevance values, right? There would be a lot of unseen documents, in general, we have only collected the data from the documents that we have shown to the users. And there are even more unseen queries because you cannot predict what queries will be typed in by users. So obviously, this approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic idea of \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m and it makes sense intuitively. So what do we do in such a case when we have a lot of unseen documents and unseen queries? Well, the solutions that we have to approximate in some way. So in this particular case called a query likelihood \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m, we just approximate this by another conditional probability. p(q given d, R=1). So in the condition part, we assume that the user likes the document because we have seen that the user clicked \u001b[36;40mon\u001b[m this document. And this part shows that we're interested in how likely the user would actually enter this query. How likely we will see this query in the same row. So note that here, we have made an interesting assumption here. Basically, we're going to do, assume that whether the user types in this query has something to do with whether user likes the document. In other words, we actually make the following assumption. And that is a user formulates a query based \u001b[36;40mon\u001b[m an imaginary relevant document. Where if you just look at this as conditional probability, it's not obvious we are making this assumption. So what I really meant is that to use this new conditional probability to help us score, then this new conditional probability will have to somehow be able to estimate this conditional probability without relying \u001b[36;40mon\u001b[m this big table. Otherwise we would be having similar problems as before, and by making this assumption, we have some way to bypass this big table, and try to just \u001b[36;40mmodel\u001b[m how the user formulates the query, okay? So this is how you can simplify the general \u001b[36;40mmodel\u001b[m so that we can derive a specific relevant function later. So let's look at how this \u001b[36;40mmodel\u001b[m work for our example. And basically, what we are going to do in this case is to ask the following \u001b[36;40mquestion\u001b[m. Which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query? So we ask this \u001b[36;40mquestion\u001b[m and we quantify the probability and this probability is a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the user's mind. Here you can see we've computed all these query likelihood probabilities. The likelihood of queries given each document. Once we have these values, we can then rank these documents based \u001b[36;40mon\u001b[m these values. So to summarize, the general idea of modern relevance in the proper risk \u001b[36;40mmodel\u001b[m is to assume the we introduce a binary random variable R, here. And then, let the scoring function be defined based \u001b[36;40mon\u001b[m this conditional probability. We also talked about approximating this by using the query likelihood. And in this case we have a ranking function that's basically based \u001b[36;40mon\u001b[m the probability of a query given the document. And this probability should be interpreted as the probability that a user who likes document d, would pose query q. Now, the \u001b[36;40mquestion\u001b[m of course is, how do we compute this conditional probability? At this in general has to do with how you compute the probability of text, because q is a text. And this has to do with a \u001b[36;40mmodel\u001b[m called a Language \u001b[36;40mModel\u001b[m. And these kind of models are proposed to \u001b[36;40mmodel\u001b[m text. So more specifically, we will be very interested in the following conditional probability as is shown in this here. If the user liked this document, how likely the user would pose this query. And in the next lecture we're going to do, giving introduction to language models that we can see how we can \u001b[36;40mmodel\u001b[m text that was a probable risk \u001b[36;40mmodel\u001b[m, in general.  ...\n",
      "\n",
      "28. #  Week 9 Overview  During this week's lessons, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture \u001b[36;40mmodel\u001b[m, the basic topic \u001b[36;40mmodel\u001b[m, \u001b[36;40mProbabilistic\u001b[m Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA.  ##  Time  This module should take **approximately 3.5 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with required assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 9 Video Lectures  |  2 hours      **Week 9 Graded Quiz**  |  1 hour      **Project Proposal and Team Formation Submission for Grading**  |  30 mins      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain what a mixture of unigram language \u001b[36;40mmodel\u001b[m is and why using a background language in a mixture can help “absorb” common words in English.     * Explain what PLSA is and how it can be used to mine and analyze topics in text.     * Explain the general idea of using a generative \u001b[36;40mmodel\u001b[m for text mining.     * Explain how to compute the probability of observing a word from a mixture \u001b[36;40mmodel\u001b[m like PLSA.     * Explain the basic idea of the EM algorithm and how it works.     * Explain the main difference between LDA and PLSA.   ##  Guiding Questions  Develop your answers to the following guiding questions while watching the video lectures throughout the week.    * What is a mixture \u001b[36;40mmodel\u001b[m? In general, how do you compute the probability of observing a particular word from a mixture \u001b[36;40mmodel\u001b[m? What is the general form of the expression for this probability?     * What does the maximum likelihood estimate of the component word distributions of a mixture \u001b[36;40mmodel\u001b[m behave like? In what sense do they “collaborate” and/or “compete”? Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability \u001b[36;40mon\u001b[m the common (often non-content) words?     * What is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?     * What is PLSA? How many parameters does a PLSA \u001b[36;40mmodel\u001b[m have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior \u001b[36;40mon\u001b[m a topic word distribution?     * How is LDA different from PLSA? What is shared by the two models?   ##  Additional Readings and Resources  The following readings are optional:    * C. Zhai and S. Massung, _Text Data Management and Analysis: A Practical Introduction to Information \u001b[36;40mRetrieval\u001b[m and Text Mining_ . ACM and Morgan & Claypool Publishers, 2016. Chapter 17.     * Blei, D. 2012. _Probabilistic Topic Models_ . Communications of the ACM 55 (4): 77–84. doi: 10.1145/2133806.2133826.     * Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. _Automatic Labeling of Multinomial Topic Models_ . Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246.     * Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011. _Investigating task performance of \u001b[36;40mprobabilistic\u001b[m topic models: an empirical study of PLSA and LDA_ . Information \u001b[36;40mRetrieval\u001b[m, 14, 2 (April 2011), 178-203. doi: 10.1007/s10791-010-9141-9.   ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * Mixture \u001b[36;40mmodel\u001b[m     * Component \u001b[36;40mmodel\u001b[m     * Constraints \u001b[36;40mon\u001b[m probabilities     * \u001b[36;40mProbabilistic\u001b[m Latent Semantic Analysis (PLSA)     * Expectation-Maximization (EM) algorithm     * E-step and M-step     * Hidden variables     * Hill climbing     * Local maximum     * Latent Dirichlet Allocation (LDA)   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking \u001b[36;40mon\u001b[m the **Contact Us!** link available \u001b[36;40mon\u001b[m each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links \u001b[36;40mon\u001b[m course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues.   ...\n",
      "\n",
      "63.   This lecture is a overview of text \u001b[36;40mretrieval\u001b[m methods. In the previous lecture, we introduced the problem of text \u001b[36;40mretrieval\u001b[m. We explained that the main problem is the design of ranking function to rank documents for a query. In this lecture, we will give an overview of different ways of designing this ranking function. So the problem is the following. We have a query that has a sequence of words and the document that's also a sequence of words. And we hope to define a function f that can compute a score based \u001b[36;40mon\u001b[m the query and document. So the main challenge you hear is with design a good ranking function that can rank all the relevant documents \u001b[36;40mon\u001b[m top of all the non-relevant ones. Clearly, this means our function must be able to measure the likelihood that a document d is relevant to a query q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m, which gives us a formalization of relevance. Now, over many decades, researchers have designed many different kinds of \u001b[36;40mretrieval\u001b[m models. And they fall into different categories. First, one family of the models are based \u001b[36;40mon\u001b[m the similarity idea. Basically, we assume that if a document is more similar to the query than another document is, then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vector space \u001b[36;40mmodel\u001b[m, which we will cover more in detail later in the lecture. A second kind of models are called \u001b[36;40mprobabilistic\u001b[m models. In this family of models, we follow a very different strategy, where we assume that queries and documents are all observations from random variables. And we assume there is a binary random variable called R here to indicate whether a document is relevant to a query. We then define the score of document with respect to a query as a probability that this random variable R is equal to 1, given a particular document query. There are different cases of such a general idea. One is classic \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m, another is language \u001b[36;40mmodel\u001b[m, yet another is divergence from randomness \u001b[36;40mmodel\u001b[m. In a later lecture, we will talk more about one case, which is language \u001b[36;40mmodel\u001b[m. A third kind of \u001b[36;40mmodel\u001b[m are based \u001b[36;40mon\u001b[m \u001b[36;40mprobabilistic\u001b[m inference. So here the idea is to associate uncertainty to inference rules, and we can then quantify the probability that we can show that the query follows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to define a set of constraints that we hope a good \u001b[36;40mretrieval\u001b[m function to satisfy. So in this case, the problem is to seek a good ranking function that can satisfy all the desired constraints. Interestingly, although these different models are based \u001b[36;40mon\u001b[m different thinking, in the end, the \u001b[36;40mretrieval\u001b[m function tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the art \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m and to examine some of the common ideas used in all these models. First, these models are all based \u001b[36;40mon\u001b[m the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation used in all the search engines. So with this assumption, the score of a query, like a presidential campaign news with respect to a document of d here, would be based \u001b[36;40mon\u001b[m scores computed based \u001b[36;40mon\u001b[m each individual word. And that means the score would depend \u001b[36;40mon\u001b[m the score of each word, such as presidential, campaign, and news. Here, we can see there are three different components, each corresponding to how well the document matches each of the query words. Inside of these functions, we see a number of heuristics used. So for example, one factor that affects the function d here is how many times does the word presidential occur in the document? This is called a term frequency, or TF. We might also denote as c of presidential and d. In general, if the word occurs more frequently in the document, then the value of this function would be larger. Another factor is, how long is the document? And this is to use the document length for scoring. In general, if a term occurs in a long document many times, it's not as significant as if it occurred the same number of times in a short document. Because in a long document, any term is expected to occur more frequently. Finally, there is this factor called document frequency. That is, we also want to look at how often presidential occurs in the entire collection, and we call this document frequency, or df of presidential. And in some other models, we might also use a probability to characterize this information. So here, I show the probability of presidential in the collection. So all these are trying to characterize the popularity of the term in the collection. In general, matching a rare term in the collection is contributing more to the overall score than matching up common term. So this captures some of the main ideas used in pretty much older state of the art original models. So now, a natural \u001b[36;40mquestion\u001b[m is, which \u001b[36;40mmodel\u001b[m works the best? Now it turns out that many models work equally well. So here are a list of the four major models that are generally regarded as a state of the art original models, pivoted length normalization, BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these, BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers. And we'll talk more about this method later in some other lectures. So, to summarize, the main points made in this lecture are first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing appropriate \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m. Second, many models are equally effective, but we don't have a single winner yet. Researchers are still active and working \u001b[36;40mon\u001b[m this problem, trying to find a truly optimal \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m. Finally, the state of the art ranking functions tend to rely \u001b[36;40mon\u001b[m the following ideas. First, bag of words representation. Second, TF and document frequency of words. Such information is used in the weighting function to determine the overall contribution of matching a word and document length. These are often combined in interesting ways, and we'll discuss how exactly they are combined to rank documents in the lectures later. There are two suggested additional readings if you have time. The first is a paper where you can find the detailed discussion and comparison of multiple state of the art models. The second is a book with a chapter that gives a broad review of different \u001b[36;40mretrieval\u001b[m models.  ...\n",
      "\n",
      "93.   So let's plug in these \u001b[36;40mmodel\u001b[m masses into the ranking function to see what we will get, okay? This is a general smoothing. So a general ranking function for smoothing with subtraction and you have seen this before. And now we have a very specific smoothing method, the JM smoothing method. So now let's see what what's a value for office of D here. And what's the value for p sub c here? Right, so we may need to decide this in order to figure out the exact form of the ranking function. And we also need to figure out of course alpha. So let's see. Well this ratio is basically this, right, so, here, this is the probability of c board \u001b[36;40mon\u001b[m the top, and this is the probability of unseen war or, in other words basically 11 times basically the alpha here, this, so it's easy to see that. This can be then rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? So it would be just lambda, right? And what would happen if we plug in this value here, if this is lambda. What can we say about this? Does it depend \u001b[36;40mon\u001b[m the document? No, so it can be ignored. Right? So we'll end up having this ranking function shown here. And in this case you can easy to see, this a precisely a vector space \u001b[36;40mmodel\u001b[m because this part is a sum over all the matched query terms, this is an element of the query map. What do you think is a element of the document up there? Well it's this, right. So that's our document left element. And let's further examine what's inside of this logarithm. Well one plus this. So it's going to be nonnegative, this log of this, it's going to be at least 1, right? And these, this is a parameter, so lambda is parameter. And let's look at this. Now this is a TF. Now we see very clearly this TF weighting here. And the larger the count is, the higher the weighting will be. We also see IDF weighting, which is given by this. And we see docking the lan's relationship here. So all these heuristics are captured in this formula. What's interesting that we kind of have got this weighting function automatically by making various assumptions. Whereas in the vector space \u001b[36;40mmodel\u001b[m, we had to go through those heuristic design in order to get this. And in this case note that there's a specific form. And when you see whether this form actually makes sense. All right so what do you think is the denominator here, hm? This is a math of document. Total number of words, multiplied by the probability of the word given by the collection, right? So this actually can be interpreted as expected account over word. If we're going to draw, a word, from the connection that we \u001b[36;40mmodel\u001b[m. And, we're going to draw as many as the number of words in the document. If you do that, the expected account of a word, w, would be precisely given by this denominator. So, this ratio basically, is comparing the actual count, here. The actual count of the word in the document with expected count given by this product if the word is in fact following the distribution in the clutch this. And if this counter is larger than the expected counter in this part, this ratio would be larger than one. So that's actually a very interesting interpretation, right? It's very natural and intuitive, it makes a lot of sense. And this is one advantage of using this kind of \u001b[36;40mprobabilistic\u001b[m reasoning where we have made explicit assumptions. And, we know precisely why we have a logarithm here. And, why we have these probabilities here. And, we also have a formula that intuitively makes a lot of sense and does TF-IDF weighting and documenting and some others. Let's look at the, the Dirichlet Prior Smoothing. It's very similar to the case of JM smoothing. In this case, the smoothing parameter is mu and that's different from lambda that we saw before. But the format looks very similar. The form of the function looks very similar. So we still have linear operation here. And when we compute this ratio, one will find that is that the ratio is equal to this. And what's interesting here is that we are doing another comparison here now. We're comparing the actual count. Which is the expected account of the world if we sampled meal worlds according to the collection world probability. So note that it's interesting we don't even see docking the lens here and lighter in the JMs \u001b[36;40mmodel\u001b[m. All right so this of course should be plugged into this part. So you might wonder, so where is docking lens. Interestingly the docking lens is here in alpha sub d so this would be plugged into this part. As a result what we get is the following function here and this is again a sum over all the match query words. And we're against the queer, the query, time frequency here. And you can interpret this as the element of a document vector, but this is no longer a single dot product, right? Because we have this part, I know that n is the name of the query, right? So that just means if we score this function, we have to take a sum over all the query words, and then do some adjustment of the score based \u001b[36;40mon\u001b[m the document. But it's still, it's still clear that it does documents lens modulation because this lens is in the denominator so a longer document will have a lower weight here. And we can also see it has tf here and now idf. Only that this time the form of the formula is different from the previous one in JMs one. But intuitively it still implements TFIDF waiting and document lens rendition again, the form of the function is dictated by the \u001b[36;40mprobabilistic\u001b[m reasoning and assumptions that we have made. Now there are also disadvantages of this approach. And that is, there's no guarantee that there's such a form of the formula will actually work well. So if we look about at this geo function, all those TF-IDF waiting and document lens rendition for example it's unclear whether we have sub-linear transformation. Unfortunately we can see here there is a logarithm function here. So we do have also the, so it's here right? So we do have the sublinear transformation, but we do not intentionally do that. That means there's no guarantee that we will end up in this, in this way. Suppose we don't have logarithm, then there's no sub-linear transformation. As we discussed before, perhaps the formula is not going to work so well. So that's an example of the gap between a formal \u001b[36;40mmodel\u001b[m like this and the relevance that we have to \u001b[36;40mmodel\u001b[m, which is really a subject motion that is tied to users. So it doesn't mean we cannot fix this. For example, imagine if we did not have this logarithm, right? So we can take a risk and we're going to add one, or we can even add double logarithm. But then, it would mean that the function is no longer a proper risk \u001b[36;40mmodel\u001b[m. So the consequence of the modification is no longer as predictable as what we have been doing now. So, that's also why, for example, PM45 remains very competitive and still, open channel how to use public risk models as they arrive, better \u001b[36;40mmodel\u001b[m than the PM25. In particular how do we use query like how to derive a \u001b[36;40mmodel\u001b[m and that would work consistently better than DM 25. Currently we still cannot do that. Still interesting open \u001b[36;40mquestion\u001b[m. So to summarize this part, we've talked about the two smoothing methods. Jelinek-Mercer which is doing the fixed coefficient linear interpolation. Dirichlet Prior this is what add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. In most cases we can see, by using these smoothing methods, we will be able to reach a \u001b[36;40mretrieval\u001b[m function where the assumptions are clearly articulate. So they are less heuristic. Explaining the results also show that these, \u001b[36;40mretrieval\u001b[m functions. Also are very effective and they are comparable to BM 25 or pm lens adultation. So this is a major advantage of probably smaller where we don't have to do a lot of heuristic design. Yet in the end that we naturally implemented TF-IDF weighting and doc length normalization. Each of these functions also has precise ones smoothing parameter. In this case of course we still need to set this smoothing parameter. There are also methods that can be used to estimate these parameters. So overall, this shows by using a \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m, we follow very different strategies then the vector space \u001b[36;40mmodel\u001b[m. Yet, in the end, we end up uh,with some retrievable functions that look very similar to the vector space \u001b[36;40mmodel\u001b[m. With some advantages in having assumptions clearly stated. And then, the form dictated by a \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m. Now, this also concludes our discussion of the query likelihood \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m. And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture. Well we basically have made four assumptions that I listed here. The first assumption is that the relevance can be modeled by the query likelihood. And the second assumption with med is, are query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of old words in the query. And then, the third assumption that we have made is, if a word is not seen, the document or in the late, its probability proportional to its probability in the collection. That's a smoothing with a collection ama \u001b[36;40mmodel\u001b[m. And finally, we made one of these two assumptions about the smoothing. So we either used JM smoothing or Dirichlet prior smoothing. If we make these four assumptions then we have no choice but to take the form of the \u001b[36;40mretrieval\u001b[m function that we have seen earlier. Fortunately the function has a nice property in that it implements TF-IDF weighting and document machine and these functions also work very well. So in that sense, these functions are less heuristic compared with the vector space \u001b[36;40mmodel\u001b[m. And there are many extensions of this, this basic \u001b[36;40mmodel\u001b[m and you can find the discussion of them in the reference at the end of this lecture.  ...\n",
      "\n",
      "23. #  Week 4 Overview  During this week's lessons, you will learn \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mretrieval\u001b[m models and statistical language models, particularly the detail of the query likelihood \u001b[36;40mretrieval\u001b[m function with two specific smoothing methods, and how the query likelihood \u001b[36;40mretrieval\u001b[m function is connected with the \u001b[36;40mretrieval\u001b[m heuristics used in the vector space \u001b[36;40mmodel\u001b[m.  ##  Time  This module should take **approximately 6 hours** of dedicated time to complete, with its videos and assignments.  ##  Activities  The activities for this module are listed below (with assignments in bold):  **Activity**  |  **Estimated Time Required**      ---|---      Week 4 Video Lectures  |  2 hours      **Week 4 Graded Quiz**  |  1 hour      **Programming Assignment 2.2**  |  3 hours      ##  Goals and Objectives  After you actively engage in the learning experiences in this module, you should be able to:    * Explain how to interpret p(R=1|q,d) and estimate it based \u001b[36;40mon\u001b[m a large set of collected relevance judgments (or clickthrough information) about query q and document d.     * Explain how to interpret the conditional probability p(q|d) used for scoring documents in the query likelihood \u001b[36;40mretrieval\u001b[m function.     * Explain what a statistical language \u001b[36;40mmodel\u001b[m and a unigram language \u001b[36;40mmodel\u001b[m are.     * Explain how to compute the maximum likelihood estimate of a unigram language \u001b[36;40mmodel\u001b[m.     * Explain how to use unigram language models to discover semantically related words.     * Compute p(q|d) based \u001b[36;40mon\u001b[m a given document language \u001b[36;40mmodel\u001b[m p(w|d).     * Explain what smoothing does.     * Show that query likelihood \u001b[36;40mretrieval\u001b[m function implements TF-IDF weighting if we smooth the document language \u001b[36;40mmodel\u001b[m p(w|d) using the collection language \u001b[36;40mmodel\u001b[m p(w|C) as a reference language \u001b[36;40mmodel\u001b[m.     * Compute the estimate of p(w|d) using Jelinek-Mercer (JM) smoothing and Dirichlet Prior smoothing, respectively.   ##  Guiding Questions  Develop your answers to the following guiding questions while completing the readings and working \u001b[36;40mon\u001b[m assignments throughout the week.    * Given a table of relevance judgments in the form of three columns (query, document, and binary relevance judgments), how can we estimate p(R=1|q,d)?     * How should we interpret the query likelihood conditional probability p(q|d)?     * What is a statistical language \u001b[36;40mmodel\u001b[m? What is a unigram language \u001b[36;40mmodel\u001b[m? How many parameters are there in a unigram language \u001b[36;40mmodel\u001b[m?     * How do we compute the maximum likelihood estimate of the unigram language \u001b[36;40mmodel\u001b[m (based \u001b[36;40mon\u001b[m a text sample)?     * What is a background language \u001b[36;40mmodel\u001b[m? What is a collection language \u001b[36;40mmodel\u001b[m? What is a document language \u001b[36;40mmodel\u001b[m?     * Why do we need to smooth a document language \u001b[36;40mmodel\u001b[m in the query likelihood \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m? What would happen if we don’t do smoothing?     * When we smooth a document language \u001b[36;40mmodel\u001b[m using a collection language \u001b[36;40mmodel\u001b[m as a reference language \u001b[36;40mmodel\u001b[m, what is the probability assigned to an unseen word in a document?     * How can we prove that the query likelihood \u001b[36;40mretrieval\u001b[m function implements TF-IDF weighting if we use a collection language \u001b[36;40mmodel\u001b[m smoothing?     * How does linear interpolation (Jelinek-Mercer) smoothing work? What is the formula?     * How does Dirichlet prior smoothing work? What is the formula?     * What are the similarities and differences between Jelinek-Mercer smoothing and Dirichlet prior smoothing?   ##  Additional Readings and Resources    * C. Zhai and S. Massung. _Text Data Management and Analysis: A Practical Introduction to Information \u001b[36;40mRetrieval\u001b[m and Text Mining_ , ACM Book Series, Morgan & Claypool Publishers, 2016. **Chapter 6 - Section 6.4**  ##  Key Phrases and Concepts  Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.    * p(R=1|q,d) ; query likelihood, p(q|d)     * Statistical and unigram language models     * Maximum likelihood estimate     * Background, collection, and document language models     * Smoothing of unigram language models     * Relation between query likelihood and TF-IDF weighting     * Linear interpolation (i.e., Jelinek-Mercer) smoothing     * Dirichlet Prior smoothing   ##  Tips for Success  To do well this week, I recommend that you do the following:    * Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.     * When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.     * It’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.     * Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!   ##  Getting and Giving Help  You can get/give help via the following means:    * Use the [ **Learner Help Center** ](https://courserahelp.zendesk.com/hc/en-us/) to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking \u001b[36;40mon\u001b[m the **Contact Us!** link available \u001b[36;40mon\u001b[m each topic's page within the Learner Help Center.     * Use the [ **Content Issues** ](https://www.coursera.org/learn/text-\u001b[36;40mretrieval\u001b[m/forum/VNWXSgylEeaZrBJIefqa4w/discussions?sort=lastActivityAtDesc&page=1) forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links \u001b[36;40mon\u001b[m course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues   ...\n",
      "\n",
      "53.   This lecture is about query likelihood, \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m. In this lecture, we continue the discussion of \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m. In particular, we're going to talk about the query light holder \u001b[36;40mretrieval\u001b[m function. In the query light holder \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m, our idea is \u001b[36;40mmodel\u001b[m. How like their user who likes a document with pose a particular query? So in this case, you can imagine if a user likes this particular document about a presidential campaign news. Now we assume, the user would use this a document as a basis to impose a query to try and retrieve this document. So again, imagine use a process that works as follows. Where we assume that the query is generated by assembling words from the document. So for example, a user might pick a word like presidential, from this document and then use this as a query word. And then the user would pick another word like campaign, and that would be the second query word. Now this of course is an assumption that we have made about how a user would pose a query. Whether a user actually followed this process may be a different \u001b[36;40mquestion\u001b[m, but this assumption has allowed us to formerly characterize this conditional probability. And this allows us to also not rely \u001b[36;40mon\u001b[m the big table that I showed you earlier to use empirical data to estimate this probability. And this is why we can use this idea then to further derive \u001b[36;40mretrieval\u001b[m function that we can implement with the program language. So as you see the assumption that we made here is each query word is independent of the sample. And also each word is basically obtained from the document. So now let's see how this works exactly. Well, since we are completing a query likelihood then the probability here is just the probability of this particular query, which is a sequence of words. And we make the assumption that each word is generated independently. So as a result, the probability of the query is just a product of the probability of each query word. Now how do we compute the probability of each query word? Well, based \u001b[36;40mon\u001b[m the assumption that a word is picked from the document that the user has in mind. Now we know the probability of each word is just the relative frequency of each word in the document. So for example, the probability of presidential given the document. Would be just the count of presidential document divided by the total number of words in the document or document s. So with these assumptions we now have actually a simple formula for \u001b[36;40mretrieval\u001b[m. We can use this to rank our documents. So does this \u001b[36;40mmodel\u001b[m work? Let's take a look. Here are some example documents that you have seen before. Suppose now the query is presidential campaign and we see the formula here \u001b[36;40mon\u001b[m the top. So how do we score this document? Well, it's very simple. We just count how many times do we have seen presidential or how many times do we have seen campaigns, etc. And we see here 44, and we've seen presidential twice. So that's 2 over the length of document 4 multiplied by 1 over the length of document 4 for the probability of campaign. And similarly, we can get probabilities for the other two documents. Now if you look at these numbers or these formulas for scoring all these documents, it seems to make sense. Because if we assume d3 and d4 have about the same length, then looks like a nominal rank d4 above d3 and which is above d2. And as we would expect, looks like it did captures a TF query state, and so this seems to work well. However, if we try a different query like this one, presidential campaign update then we might see a problem. Well what problem? Well think about the update. Now none of these documents has mentioned update. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining the word update would be what? Would be 0. So that causes a problem, because it would cause all these documents to have zero probability of generating this query. Now why it's fine to have zero probability for d2, which is non-relevant? It's not okay to have 0 for d3 and d4 because now we no longer can distinguish them. What's worse? We can't even distinguish them from d2. So that's obviously not desirable. Now when a [INAUDIBLE] has such result, we should think about what has caused this problem? So we have to examine what assumptions have been made, as we derive this ranking function. Now is you examine those assumptions carefully you will realize, what has caused this problem? So take a moment to think about it. What do you think is the reason why update has zero probability and how do we fix it? So if you think about this from the moment you realize that that's because we have made an assumption that every query word must be drawn from the document in the user's mind. So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document. So that's the improved \u001b[36;40mmodel\u001b[m. An improvement here is to say that, well instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document \u001b[36;40mmodel\u001b[m. And so I show a \u001b[36;40mmodel\u001b[m here. And we assume that this document is generated using this unigram language \u001b[36;40mmodel\u001b[m. Now, this \u001b[36;40mmodel\u001b[m doesn't necessarily assign zero probability for update in fact, we can assume this \u001b[36;40mmodel\u001b[m does not assign zero probability for any word. Now if we're thinking this way then the generation process is a little bit different. Now the user has this \u001b[36;40mmodel\u001b[m in mind instead of this particular document. Although the \u001b[36;40mmodel\u001b[m has to be estimated based \u001b[36;40mon\u001b[m the document. So the user can again generate the query using a singular process. Namely, pick a word for example, presidential and another word campaign. Now the difference is that this time we can also pick a word like update, even though update doesn't occur in the document to potentially generate a query word like update. So that a query was updated 1 times 0 probabilities. So this would fix our problem. And it's also reasonable because when our thinking of what the user is looking for in a more general way, that is unique language \u001b[36;40mmodel\u001b[m instead of fixed document. So how do we compute this query likelihood? If we make this sum wide involved two steps. The first one is compute this \u001b[36;40mmodel\u001b[m, and we call it document language \u001b[36;40mmodel\u001b[m here. For example, I've shown two pulse models here, it's major based \u001b[36;40mon\u001b[m two documents. And then given a query like a data mining algorithms the thinking is that we'll just compute the likelihood of this query. And by making independence assumptions we could then have this probability as a product of the probability of each query word. We do this for both documents, and then we can score these two documents and then rank them. So that's the basic idea of this query likelihood \u001b[36;40mretrieval\u001b[m function. So more generally this ranking function would look like in the following. Here we assume that the query has n words, w1 through wn, and then the scoring function. The ranking function is the probability that we observe this query, given that the user is thinking of this document. And this is assume it will be product of probabilities of all individual words. This is based \u001b[36;40mon\u001b[m independent assumption. Now we actually often score the document before this query by using log of the query likelihood as shown \u001b[36;40mon\u001b[m the second line. Now we do this to avoid having a lot of small probabilities, mean multiply together. And this could cause under flow and we might loose the precision by transforming the value in our algorithm function. We maintain the order of these documents yet we can avoid the under flow problem. And so if we take longer than transformation of course, the product would become a sum as you \u001b[36;40mon\u001b[m the second line here. So the sum of all the query words inside of the sum that is one of the probability of this word given by the document. And then we can further rewrite the sum to a different form. So in the first sum here, in this sum, we have it over all the query words and query word. And in this sum we have a sum of all the possible words. But we put a counter here of each word in the query. Essentially we are only considering the words in the query, because if a word is not in the query, the count will be 0. So we're still considering only these n words. But we're using a different form as if we were going to take a sample of all the words in the vocabulary. And of course, a word might occur multiple times in the query. That's why we have a count here. And then this part is log of the probability of the word, given by the document language \u001b[36;40mmodel\u001b[m. So you can see in this \u001b[36;40mretrieval\u001b[m function, we actually know the count of the word in the query. So the only thing that we don't know is this document language \u001b[36;40mmodel\u001b[m. Therefore, we have converted the \u001b[36;40mretrieval\u001b[m problem include the problem of estimating this document language \u001b[36;40mmodel\u001b[m. So that we can compute the probability of each query word given by this document. And different estimation methods would lead to different ranking functions. This is just like a different way to place document in the vector space which leads to a different ranking function in the vector space \u001b[36;40mmodel\u001b[m. Here different ways to estimate will lead to a different ranking function for query likelihood.  ...\n",
      "\n",
      "40.   This lecture is about the statistical language \u001b[36;40mmodel\u001b[m. In this lecture, we're going to give an introduction to statistical language \u001b[36;40mmodel\u001b[m. This has to do with how do you \u001b[36;40mmodel\u001b[m text data with \u001b[36;40mprobabilistic\u001b[m models. So it's related to how we \u001b[36;40mmodel\u001b[m query based \u001b[36;40mon\u001b[m a document. We're going to talk about what is a language \u001b[36;40mmodel\u001b[m. And then we're going to talk about the simplest language \u001b[36;40mmodel\u001b[m called the unigram language \u001b[36;40mmodel\u001b[m, which also happens to be the most useful \u001b[36;40mmodel\u001b[m for text \u001b[36;40mretrieval\u001b[m. And finally, what this class will use is a language \u001b[36;40mmodel\u001b[m. What is a language \u001b[36;40mmodel\u001b[m? Well, it's just a probability distribution over word sequences. So here, I'll show one. This \u001b[36;40mmodel\u001b[m gives the sequence Today is Wednesday a probability of 0.001. It give Today Wednesday is a very, very small probability because it's non-grammatical. You can see the probabilities given to these sentences or sequences of words can vary a lot depending \u001b[36;40mon\u001b[m the \u001b[36;40mmodel\u001b[m. Therefore, it's clearly context dependent. In ordinary conversation, probably Today is Wednesday is most popular among these sentences. Imagine in the context of discussing apply the math, maybe the eigenvalue is positive, would have a higher probability. This means it can be used to represent the topic of a text. The \u001b[36;40mmodel\u001b[m can also be regarded as a \u001b[36;40mprobabilistic\u001b[m mechanism for generating text. And this is why it's also often called a generating \u001b[36;40mmodel\u001b[m. So what does that mean? We can imagine this is a mechanism that's visualised here as a stochastic system that can generate sequences of words. So, we can ask for a sequence, and it's to send for a sequence from the device if you want, and it might generate, for example, Today is Wednesday, but it could have generated any other sequences. So for example, there are many possibilities, right? So in this sense, we can view our data as basically a sample observed from such a generating \u001b[36;40mmodel\u001b[m. So, why is such a \u001b[36;40mmodel\u001b[m useful? Well, it's mainly because it can quantify the uncertainties in natural language. Where do uncertainties come from? Well, one source is simply the ambiguity in natural language that we discussed earlier in the lecture. Another source is because we don't have complete understanding, we lack all the knowledge to understand the language. In that case, there will be uncertainties as well. So let me show some examples of questions that we can answer with a language \u001b[36;40mmodel\u001b[m that would have interesting applications in different ways. Given that we see John and feels, how likely will we see happy as opposed to habit as the next word in a sequence of words? Now, obviously, this would be very useful for speech recognition because happy and habit would have similar acoustic sound, acoustic signals. But, if we look at the language \u001b[36;40mmodel\u001b[m, we know that John feels happy would be far more likely than John feels habit. Another example, given that we observe baseball three times and game once in a news article, how likely is it about sports? This obviously is related to text categorization and information \u001b[36;40mretrieval\u001b[m. Also, given that a user is interested in sports news, how likely would the user use baseball in a query? Now, this is clearly related to the query likelihood that we discussed in the previous lecture. So now, let's look at the simplest language \u001b[36;40mmodel\u001b[m, called a unigram language \u001b[36;40mmodel\u001b[m. In such a case, we assume that we generate a text by generating each word independently. So this means the probability of a sequence of words would be then the product of the probability of each word. Now normally, they're not independent, right? So if you have single word in like a language, that would make it far more likely to observe \u001b[36;40mmodel\u001b[m than if you haven't seen the language. So this assumption is not necessarily true, but we make this assumption to simplify the \u001b[36;40mmodel\u001b[m. So now the \u001b[36;40mmodel\u001b[m has precisely N parameters, where N is vocabulary size. We have one probability for each word, and all these probabilities must sum to 1. So strictly speaking, we actually have N-1 parameters. As I said, text can then be assumed to be assembled, drawn from this word distribution. So for example, now we can ask the device or the \u001b[36;40mmodel\u001b[m to stochastically generate the words for us, instead of sequences. So instead of giving a whole sequence, like Today is Wednesday, it now gives us just one word. And we can get all kinds of words. And we can assemble these words in a sequence. So that will still allow you to compute the probability of Today is Wednesday as the product of the three probabilities. As you can see, even though we have not asked the \u001b[36;40mmodel\u001b[m to generate the sequences, it actually allows us to compute the probability for all the sequences, but this \u001b[36;40mmodel\u001b[m now only needs N parameters to characterize. That means if we specify all the probabilities for all the words, then the \u001b[36;40mmodel\u001b[m's behavior is completely specified. Whereas if we don't make this assumption, we would have to specify probabilities for all kinds of combinations of words in sequences. So by making this assumption, it makes it much easier to estimate these parameters. So let's see a specific example here. Here I show two unigram language models with some probabilities. And these are high probability words that are shown \u001b[36;40mon\u001b[m top. The first one clearly suggests a topic of text mining, because the high probability was all related to this topic. The second one is more related to health. Now we can ask the \u001b[36;40mquestion\u001b[m, how likely were observe a particular text from each of these two models? Now suppose we sample words to form a document. Let's say we take the first distribution, would you like to sample words? What words do you think would be generated while making a text or maybe mining maybe another word? Even food, which has a very small probability, might still be able to show up. But in general, high probability words will likely show up more often. So we can imagine what general text of that looks like in text mining. In fact, with small probability, you might be able to actually generate the actual text mining paper. Now, it will actually be meaningful, although the probability will be very, very small. In an extreme case, you might imagine we might be able to generate a text mining paper that would be accepted by a major conference. And in that case, the probability would be even smaller. But it's a non-zero probability, if we assume none of the words have non-zero probability. Similarly from the second topic, we can imagine we can generate a food nutrition paper. That doesn't mean we cannot generate this paper from text mining distribution. We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference \u001b[36;40mon\u001b[m text mining. So the point is that the keeping distribution, we can talk about the probability of observing a certain kind of text. Some texts will have higher probabilities than others. Now let's look at the problem in a different way. Suppose we now have available a particular document. In this case, many of the abstract or the text mining table, and we see these word counts here. The total number of words is 100. Now the \u001b[36;40mquestion\u001b[m you ask here is an estimation \u001b[36;40mquestion\u001b[m. We can ask the \u001b[36;40mquestion\u001b[m which \u001b[36;40mmodel\u001b[m, which one of these distribution has been used to generate this text, assuming that the text has been generated by assembling words from the distribution. So what would be your guess? What we have to decide are what probabilities text mining, etc., would have. Suppose the view for a second, and try to think about your best guess. If you're like a lot of people, you would have guessed that well, my best guess is text has a probability of 10 out of 100 because I've seen text 10 times, and there are in total 100 words. So we simply normalize these counts. And that's in fact the word justified, and your intuition is consistent with mathematical derivation. And this is called the maximum likelihood estimator. In this estimator, we assume that the parameter settings of those that would give our observe the data the maximum probability. That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller. So you can see, this has a very simple formula. Basically, we just need to look at the count of a word in a document, and then divide it by the total number of words in the document or document lens. Normalize the frequency. A consequence of this is, of course, we're going to assign zero probabilities to unseen words. If we have an observed word, there will be no incentive to assign a non-zero probability using this approach. Why? Because that would take away probability mass for these observed words. And that obviously wouldn't maximize the probability of this particular observed text data. But one has still \u001b[36;40mquestion\u001b[m whether this is our best estimate. Well, the answer depends \u001b[36;40mon\u001b[m what kind of \u001b[36;40mmodel\u001b[m you want to find, right? This estimator gives a best \u001b[36;40mmodel\u001b[m based \u001b[36;40mon\u001b[m this particular data. But if you are interested in a \u001b[36;40mmodel\u001b[m that can explain the content of the full paper for this abstract, then you might have a second thought, right? So for thing, there should be other words in the body of that article, so they should not have zero probabilities, even though they're not observed in the abstract. So we're going to cover this a little bit more later in this class in the query likelihood \u001b[36;40mmodel\u001b[m. So let's take a look at some possible uses of these language models. One use is simply to use it to represent the topics. So here I show some general English background texts. We can use this text to estimate a language \u001b[36;40mmodel\u001b[m, and the \u001b[36;40mmodel\u001b[m might look like this. Right, so \u001b[36;40mon\u001b[m the top, we have those all common words, the, a, is, we, etc., and then we'll see some common words like these, and then some very, very rare words in the bottom. This is a background language \u001b[36;40mmodel\u001b[m. It represents the frequency of words in English in general. This is the background \u001b[36;40mmodel\u001b[m. Now let's look at another text, maybe this time, we'll look at the computer science research papers. So we have a collection of computer science research papers, we do as mentioned again, we can just use the maximum likelihood estimator, where we simply normalize the frequencies. Now in this case, we'll get the distribution that looks like this. \u001b[36;40mOn\u001b[m the top, it looks similar because these words occur everywhere, they are very common. But as we go down, we'll see words that are more related to computer science, computer software, text, etc. And so although here, we might also see these words, for example, computer, but we can imagine the probability here is much smaller than the probability here. And we will see many other words here that would be more common in general English. So you can see this distribution characterizes a topic of the corresponding text. We can look at even the smaller text. So in this case, let's look at the text mining paper. Now if we do the same, we have another distribution, again the can be expected to occur in the top. The sooner we see text, mining, association, clustering, these words have relatively high probabilities. In contrast, in this distribution, the text has a relatively small probability. So this means, again, based \u001b[36;40mon\u001b[m different text data, we can have a different \u001b[36;40mmodel\u001b[m, and the \u001b[36;40mmodel\u001b[m captures the topic. So we call this document the language \u001b[36;40mmodel\u001b[m, and we call this collection language \u001b[36;40mmodel\u001b[m. And later, you will see how they're used in the \u001b[36;40mretrieval\u001b[m function. But now, let's look at another use of this \u001b[36;40mmodel\u001b[m. Can we statistically find what words are semantically related to computer? Now how do we find such words? Well, our first thought is that let's take a look at the text that match computer. So we can take a look at all the documents that contain the word computer. Let's build a language \u001b[36;40mmodel\u001b[m. We can see what words we see there. Well, not surprisingly, we see these common words \u001b[36;40mon\u001b[m top as we always do. So in this case, this language \u001b[36;40mmodel\u001b[m gives us the conditional probability of seeing the word in the context of computer. And these common words will naturally have high probabilities. But we also see the computer itself and software will have relatively high probabilities. But if we just use this \u001b[36;40mmodel\u001b[m, we cannot just say all these words are semantically related to computer. So ultimately, what we'd like to get rid of is these common words. How can we do that? It turns out that it's possible to use language \u001b[36;40mmodel\u001b[m to do that. But I suggest you think about that. So how can we know what words are very common, so that we want to kind of get rid of them? What \u001b[36;40mmodel\u001b[m will tell us that? Well, maybe you can think about that. So the background language \u001b[36;40mmodel\u001b[m precisely tells us this information. It tells us what was our common in general. So if we use this background \u001b[36;40mmodel\u001b[m, we would know that these words are common words in general. So it's not surprising to observe them in the context of computer. Whereas computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software. So then we can use these two models to somehow figure out the words that are related to computer. For example, we can simply take the ratio of these group probabilities and normalize the topic of language \u001b[36;40mmodel\u001b[m by the probability of the word in the background language \u001b[36;40mmodel\u001b[m. So if we do that, we take the ratio, we'll see that then \u001b[36;40mon\u001b[m the top, computer is ranked, and then followed by software, program, all these words related to computer. Because they occur very frequently in the context of computer, but not frequently in the whole collection, whereas these common words will not have a high probability. In fact, they have a ratio about 1 down there because they are not really related to computer. By taking the sample of text that contains the computer, we don't really see more occurrences of that than in general. So this shows that even with these simple language models, we can do some limited analysis of semantics. So in this lecture, we talked about language \u001b[36;40mmodel\u001b[m, which is basically a probability distribution over text. We talked about the simplest language \u001b[36;40mmodel\u001b[m called unigram language \u001b[36;40mmodel\u001b[m, which is also just a word distribution. We talked about the two uses of a language \u001b[36;40mmodel\u001b[m. One is we represent the topic in a document, in a collection, or in general. The other is we discover word associations. In the next lecture, we're going to talk about how language \u001b[36;40mmodel\u001b[m can be used to design a \u001b[36;40mretrieval\u001b[m function. Here are two additional readings. The first is a textbook \u001b[36;40mon\u001b[m statistical natural language processing. The second is an article that has a survey of statistical language models with a lot of pointers to research work.  ...\n",
      "\n",
      "56. #  CS 410: Text Information Systems  ##  Course Description  The growth of “big data” created unprecedented opportunities to leverage computational and statistical approaches, which turn raw data into actionable knowledge that can support various application tasks. This is especially true for the optimization of decision making in virtually all application domains, such as health and medicine, security and safety, learning and education, scientific discovery, and business intelligence. This course covers general computational techniques for building intelligent text information systems to help users manage and make use of large amounts of text data in all kinds of applications.  Text data include all data in the form of natural language text (e.g., English text or Chinese text), including all web pages, social media data such as tweets, news, scientific literature, emails, government documents, and many other kinds of enterprise data. Text data play an essential role in our lives. Since we communicate using natural languages, we produce and consume a large amount of text data every day covering all kinds of topics. The explosive growth of text data makes it impossible for people to consume all the relevant text data in a timely manner.  The two main techniques to assist people in consuming, digesting, and making use of the text data are:    1. Text \u001b[36;40mretrieval\u001b[m, which helps identify the most relevant text data to a particular problem from a large collection of text documents, thus avoiding processing a large number of non-relevant documents     2. Text mining, which helps users further analyze and digest the found relevant text data and extract actionable knowledge for finishing a task   This course covers both text \u001b[36;40mretrieval\u001b[m and text mining, so as to provide you with the opportunity to see the complete spectrum of techniques used in building an intelligent text information system. Building \u001b[36;40mon\u001b[m two MOOCs covering the same topic and including a course project, this course enables you to learn the basic concepts, principles, and general techniques in text \u001b[36;40mretrieval\u001b[m and mining, as well as gain hands-\u001b[36;40mon\u001b[m experience with using software tools to develop interesting text data applications.  ##  Course Goals and Objectives  Upon successful completion of this course, you will be able to:    * Explain all the basic concepts in text \u001b[36;40mretrieval\u001b[m and text mining.     * Explain the main ideas behind the major models and algorithms for text \u001b[36;40mretrieval\u001b[m and text mining.     * Explain how the major models and algorithms for text \u001b[36;40mretrieval\u001b[m and text mining work.     * Explain how to implement some of the commonly used algorithms for text \u001b[36;40mretrieval\u001b[m and text mining.     * Explain how to evaluate applications of text \u001b[36;40mretrieval\u001b[m and text mining.   ##  Textbook  There is not a required textbook for this course, but there are several optional readings suggested in each week's overview page. All readings listed in the weekly overview pages are optional and are primarily from the following textbook:  Zhai, C. & Massung, S. (2016). _Text data management and analysis: A practical introduction to information \u001b[36;40mretrieval\u001b[m and text mining._ ACM Book Series. Morgan & Claypool Publishers.  ##  Course Outline  **Week**  |  **Dates**  |  **Topics**      ---|---|---      **Week 1**  |  August 23 - August 29  |  Part of Speech tagging, syntactic analysis, semantic analysis, ambiguity, “bag of words” representation, push, pull, querying, browsing, probability ranking principle, relevance, vector space \u001b[36;40mmodel\u001b[m, dot product, bit vector representation      **Week 2**  |  August 30 -September 5  |  Term frequency (TF), document frequency (DF), and inverse document frequency (IDF), TF transformation, pivoted length normalization, BM25, inverted index and postings, binary coding, unary coding, gamma-coding, d-gap, Zipf’s law      **Week 3**  |  September 6 -12  |  Cranfield evaluation methodology, precision and recall, average precision, mean average precision (MAP), geometric mean average precision (gMAP), reciprocal rank, mean reciprocal rank, F-measure , Normalized Discounted Cumulative Gain (nDCG), statistical significance test      **Week 4**  |  September 13 - 19  |  p(R=1|q,d), query likelihood, p(q|d), statistical and unigram language models, maximum likelihood estimate, background, collection, and document language models, smoothing of unigram language models, relation between query likelihood and TF-IDF weighting, linear interpolation (i.e., Jelinek-Mercer) smoothing, Dirichlet Prior smoothing      **Week 5**  |  September 20 - 26  |  Relevance feedback, pseudo-relevance feedback, implicit feedback, Rocchio feedback, Kullback-Leiber divergence (KL-divergence) \u001b[36;40mretrieval\u001b[m function, mixture language \u001b[36;40mmodel\u001b[m, scalability and efficiency, spams, crawler, focused crawling, and incremental crawling, Google File System (GFS), MapReduce, link analysis and anchor text, PageRank      **Week 6**  |  September 27 - October 3  |  Content-based filtering, collaborative filtering, Beta-Gamma threshold learning, linear utility , user profile, exploration-exploitation tradeoff, memory-based collaborative filtering, cold start      **Week 7**  |  October 4 - 10  |  Text representation (especially bag-of-words representation), context of a word, context similarity, paradigmatic relation, syntagmatic relation      **Week 8**  |  October 11 - 17  |  Entropy, conditional entropy, mutual information, topics, coverage of topic , language \u001b[36;40mmodel\u001b[m, generative \u001b[36;40mmodel\u001b[m, unigram language \u001b[36;40mmodel\u001b[m, word distribution, background language \u001b[36;40mmodel\u001b[m, parameters of a \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m, likelihood, Bayes rule, maximum likelihood estimation, prior and posterior distributions, Bayesian estimation & inference, maximum a posteriori (MAP) estimate, prior \u001b[36;40mmodel\u001b[m, posterior mode. **Exam 1**      **Week 9**  |  October 18 - 24  |  Mixture \u001b[36;40mmodel\u001b[m, component \u001b[36;40mmodel\u001b[m, constraints \u001b[36;40mon\u001b[m probabilities, \u001b[36;40mProbabilistic\u001b[m Latent Semantic Analysis (PLSA), Expectation-Maximization (EM) algorithm, E-step and M-step, hidden variables, hill climbing, local maximum, Latent Dirichlet Allocation (LDA)      **Week 10**  |  October 25 - October 31  |  Clustering, document clustering, term clustering, clustering bias, perspective of similarity, Hierarchical Agglomerative Clustering, k-Means, direction evaluation (of clustering), indirect evaluation (of clustering), text categorization, topic categorization, sentiment categorization, email routing , spam filtering, naïve Bayes classifier, smoothing      **Week 11**  |  November 1 - 7  |  Generative classifier vs. discriminative classifier, training data, logistic regression, K-Nearest Neighbor classifier, classification accuracy, precision, recall, F measure, macro-averaging, micro-averaging, opinion holder, opinion target, sentiment, opinion representation, sentiment classification, features, n-grams, frequent patterns, overfitting      **Week 12**  |  November 8 - 14  |  Text-based prediction, the “data mining loop”, context (of text data), contextual text mining, contextual \u001b[36;40mprobabilistic\u001b[m latent semantic analysis (CPLSA), views of a topic, coverage of topics, spatiotemporal trends of topics, event impact analysis, network-regularized topic modeling, NetPLSA, causal topics, iterative topic modeling with time series supervision      **Week 13**  |  November 15 - 21  |  Project work week.      **Week 14**  |  November 22 - 28  |  _Thanksgiving Break_      **Week 15**  |  November 29 -December 5  |  **Exam 2;** No new content - work \u001b[36;40mon\u001b[m your final project!      **Week 16**  |  December 6 - 12  |  Final Project Presentation and Report Due Start of Finals Week      ##  Elements of This Course    * **Lecture videos.** Each week your instructor will teach you the concepts you need to know through a collection of short video lectures. You may either stream these videos for playback within the browser by clicking \u001b[36;40mon\u001b[m their titles, or you can download each video for later offline playback by clicking the download icon. **The videos usually total 1.5 to 2 hours each week** , but you generally need to spend at least the same amount of time digesting the content in the videos. The actual amount of time needed to digest the content will naturally vary according to your background.     * **Quizzes.** Most weeks will include one for-credit quiz. You will have two attempts for each quiz, with your highest score used toward your final grade. Your top 10 quiz scores will be used to calculate your final grade (i.e., we will drop the two lowest quiz scores).     * **Exams** . This course will have two 1-hour exams. The exams are intended to test your understanding of the material you learn in the course and will contain questions similar to those seen in the weekly quizzes.     * **Programming Assignments.** The programming assignments for this course provide an opportunity for you to practice your programming skills and apply what you've learned in the course. Set aside about 2-4 hours each week to work \u001b[36;40mon\u001b[m the programming assignment if you plan to finish it.     * **Tech Review (for 4-credit students).** It will require you to generate a short 1-2 page review \u001b[36;40mon\u001b[m an interesting course-related cutting-edge technology topic not covered in any lecture.     * **Final Course Project.** There will also be one culminating project due at the end of course. It will require you to work in groups of at most three. You will build a tool using methods from the course to perform data analysis and also generate documentations and presentation.   ##  Grading  Your final grade will be calculated based \u001b[36;40mon\u001b[m the activities listed in the table below. As a note, the grade \u001b[36;40mon\u001b[m Coursera will NOT accurately reflect your grade in the course.  **Activity**  |  **Percent of Final Grade**      ---|---      Quizzes  |  25%      Programming Assignments  |  25%      Course Project  |  20%      Exam 1  |  15%      Exam 2  |  15%      **Letter Grade**  |  **Percent Needed**  |  **Letter Grade**  |  **Percent Needed**  |  **Letter Grade**  |  **Percent Needed**      ---|---|---|---|---|---      **A+**  |  95  |  **B+**  |  80  |  **C**  |  60      **A**  |  90  |  **B**  |  75  |  **D**  |  55      **A-**  |  85  |  **B-**  |  70  |  **F**  |  <55      Your final grade will be calculated based \u001b[36;40mon\u001b[m the activities listed in the table below. Your official final course grade will be listed in [ Enterprise ](https://apps.uillinois.edu/selfservice/) . The course grade you see displayed in Coursera may not match your official final course grade.  ##  Additional Course Policies  ###  Student Code and Policies  A student at the University of Illinois at the Urbana Champaign campus is a member of a University community of which all members have at least the rights and responsibilities common to all citizens, free from institutional censorship; affiliation with the University as a student does not diminish the rights or responsibilities held by a student or any other community member as a citizen of larger communities of the state, the nation, and the world. See the [ University of Illinois Student Code ](http://studentcode.illinois.edu/index.html) for more information.  The CS department also maintains a policies handbook for graduate student. For more information, see the [ Graduate Student Handbook ](https://cs.illinois.edu/sites/default/files/images/CSGraduateStudentHandbook_web.15-16.pdf) .  Additionally, all Coursera learners are required to follow an [ Honor Code ](https://learner.coursera.help/hc/en-us/articles/209818863-Coursera-Honor- Code) and a [ Code of Conduct ](https://learner.coursera.help/hc/en- us/articles/208280036-Coursera-Code-of-Conduct) . Please review both of these items before commencing your studies.  ###  Academic Integrity  All students are expected to abide by [ the campus regulations \u001b[36;40mon\u001b[m academic integrity found in the Student Code of Conduct ](http://admin.illinois.edu/policy/code/article1_part4_1-401.html) . These standards will be enforced and infractions of these rules will not be tolerated in this course. Sharing, copying, or providing any part of a homework solution or code is an infraction of the University’s rules \u001b[36;40mon\u001b[m academic integrity. We will be actively looking for violations of this policy in homework and project submissions. Any violation will be punished as severely as possible with sanctions and penalties typically ranging from a failing grade \u001b[36;40mon\u001b[m this assignment up to a failing grade in the course, including a letter of the offending infraction kept in the student's permanent university record.  Again, a good rule of thumb: _Keep every typed word and piece of code your own_ . If you think you are operating in a gray area, you probably are. If you would like clarification \u001b[36;40mon\u001b[m specifics, please contact the course staff.  ###  Disability Accommodations  Students with learning, physical, or other disabilities requiring assistance should contact the instructor as soon as possible. If you’re unsure if this applies to you or think it may, please contact the instructor and [ Disability Resources and Educational Services (DRES) ](http://disability.illinois.edu/) as soon as possible. You can contact DRES at 1207 S. Oak Street, Champaign, via phone at (217) 333-1970, or via email at [ disability@illinois.edu ](mailto:disability@illinois.edu) .  ###  Late Policy  Late homework and homework by email will not be accepted by the TA or the instructors without prior instructor approval.  ...\n",
      "\n",
      "86.   This lecture is about the specific smoothing methods for language models used in \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mretrieval\u001b[m \u001b[36;40mmodel\u001b[m. In this lecture, we will continue the discussion of language models for information \u001b[36;40mretrieval\u001b[m, particularly the query likelihood \u001b[36;40mretrieval\u001b[m method. And we're going to talk about specifically the smoothing methods used for such a \u001b[36;40mretrieval\u001b[m function. So this is a slide from a previous lecture where we show that with a query likelihood ranking and smoothing with the collection language \u001b[36;40mmodel\u001b[m, we add up having a \u001b[36;40mretrieval\u001b[m function that looks like the following. So this is the \u001b[36;40mretrieval\u001b[m function based \u001b[36;40mon\u001b[m these assumptions that we have discussed. You can see it's a sum of all the matching query terms, here. And inside its sum is the count of the term in the query and some weight for the term in the document. We have t of i, the f weight here, and then we have another constant here in n. So clearly if we want to implement this function using programming language, we still need to figure out a few variables. In particular, we're going to need to know how to estimate the probability of a word exactly and how do we set alpha. So in order to answer this \u001b[36;40mquestion\u001b[m, we have to think about very specific smoothing methods, and that is main topic of this lecture. We're going to talk about two smoothing methods. The first is simple linear interpolation with a fixed coefficient. And this is also called a Jelinek-Mercer smoothing. So the idea is actually very simple. This picture shows how we estimate a document language \u001b[36;40mmodel\u001b[m by using maximum likelihood estimate. That gives us word counts normalized by the total number of words in the text. The idea of using this method is to maximize the probability of the observed text. As a result, if a word like network is not observed in the text, it's going to get 0 probability, as shown here. So the idea of smoothing, then, is to rely \u001b[36;40mon\u001b[m collection language \u001b[36;40mmodel\u001b[m where this word is not going to have a zero probability to help us decide what nonzero probability should be assigned to such a word. So we can note that network has a nonzero probability here. So in this approach what we do is we do a linear interpolation between the maximum likelihood placement here and the collection language \u001b[36;40mmodel\u001b[m, and this is computed by the smoothing parameter lambda, which is between 0 and 1. So this is a smoothing parameter. The larger lambda is, the more smoothing we will have. So by mixing them together, we achieve the goal of assigning nonzero probabilities to a word like network. So let's see how it works for some of the words here. For example, if we compute the smooth probability for text. Now the maximum likelihood estimated gives us 10 over 100, and that's going to be here. But the collection probability is this. So we'll just combine them together with this simple formula. We can also see the word network, which used to have a zero probability, now is getting a non-zero probability of this value. And that's because the count is going to be zero for network here. But this part is nonzero, and that's basically how this method works. Now if you think about this and you can easily see now the alpha sub d in this smoothing method is basically lambda. Because that's remember the coefficient in front of the probability of the word given by the collection language \u001b[36;40mmodel\u001b[m here. Okay, so this is the first smoothing method. The second one is similar but it has a tie-in into the coefficient for linear interpolation. It's often called Dirichlet Prior, or Bayesian, Smoothing. So again here we face problem of zero probability for an unseen word like network. Again we will use the collection language \u001b[36;40mmodel\u001b[m, but in this case, we're going to combine them in somewhat different ways. The formula first can be seen as a interpolation of the maximum likelihood estimate and the collection language \u001b[36;40mmodel\u001b[m as before, as in the J-M smoothing method. Only that the coefficient now is not lambda, a fixed number, but a dynamic coefficient in this form, where mu is a parameter, it's a non-negative value. And you can see if we set mu to a constant, the effect is that a long document would actually get a smaller coefficient here. Because a long document will have longer lengths, therefore the coefficient is actually smaller. And so a long document would have less smoothing, as we would expect. So this seems to make more sense than a fixed coefficient smoothing. Of course, this part would be of this form so that the two coefficients would sum to 1. Now this is one way to understand this smoothing. Basically, it means it's a dynamic coefficient interpolation. There is another way to understand this formula which is even easier to remember, and that's \u001b[36;40mon\u001b[m this side. So it's easier to see how we can rewrite the smoothing method in this form. Now in this form we can easily see what change we have made to the maximum likelihood estimate, which would be this part. So normalize the count by the document length. So in this form we can see what we did is we add this to the count of every word. So what does this mean? Well, this is basically something related to the probability of the word in the collection. And we multiply that by the parameter mu. And when we combine this with the count here, essentially we are adding pseudocounts to the observed text. We pretend every word has got this many pseudocount. So the total count would be the sum of these pseudocounts and the actual count of the word in the document. As a result, in total we would have added this many pseudocounts. Why? Because if you take somewhat this one over all the words, then we'll see the probability of the words would sum to 1, and that gives us just mu. So this is the total number of pseudocounts that we added. And so these probabilities would still sum to 1. So in this case, we can easily see the method is essentially to add this as a pseudocount to this data. Pretend we actually augment the data by including some pseudo data defined by the collection language \u001b[36;40mmodel\u001b[m. As a result, we have more counts is that the total counts for a word would be like this. And as a result, even if a word has zero count here, let's say if we have zero count here, then it would still have nonzero count because of this part. So this is how this method works. Let's also take a look at some specific example here. So for text again we will have 10 as the original count that we actually observe, but we also add some pseudocount. And so the probability of text would be of this form. Naturally, the probability of network would be just this part. And so here you can also see what's alpha sub d here. Can you see it? If you want to think about it, you can pause the video. But you'll notice that this part is basically alpha sub d. So we can see, in this case, alpha sub d does depend \u001b[36;40mon\u001b[m the document, because this length depends \u001b[36;40mon\u001b[m the document, whereas in the linear interpolation, the J-M smoothing method, this is a constant.  ...\n",
      "\n",
      "106.   This lecture is a summary of this course. This map shows the major topics we have covered in this course. And here are some key high-level take-away messages. First, we talked about natural language content analysis. Here the main take-away messages is natural language processing is a foundation for text \u001b[36;40mretrieval\u001b[m, but currently the NLP isn't robust enough so the battle of wars is generally the main method used in modern search engines. And it's often sufficient before most of the search tasks, but obviously for more complex search tasks then we need a deeper natural language processing techniques. We then talked about the high level strategies for text access and we talked about push versus pull. In pull we talked about querying versus browsing. Now in general in future search engines, we should integrate all these techniques to provide a math involved information access. And now we'll talk about a number of issues related to search engines. We talked about the search problem. And we framed that as a ranking problem. And we talked about a number of \u001b[36;40mretrieval\u001b[m methods. We start with the overview of vector space \u001b[36;40mmodel\u001b[m and the \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m and then we talked about the vector space \u001b[36;40mmodel\u001b[m in depth. We also later talked about the language modeling approach, and that's \u001b[36;40mprobabilistic\u001b[m \u001b[36;40mmodel\u001b[m. And here, many take-away message is that the modeling \u001b[36;40mretrieval\u001b[m function tend to look similar, and they generally use various heuristics. Most important ones are TF-IDF weighting, document length normalization. And the TF is often transformed through a sub media transformation function. And then we talked about how to implement a \u001b[36;40mretrieval\u001b[m system, and here, the main techniques that we talked about, how to construct an inverted index so that we can prepare the system to answer a query quickly. And we talked about how to do a faster search by using the inverted index. And we then talked about how to evaluate the text \u001b[36;40mretrieval\u001b[m system, mainly introduced to the Cranfield Evaluation Methodology. This was a very important evaluation methodology that can be applied to many tasks. We talked about the major evaluation measures. So, the most important measures for a search engine are MAP, mean average precision, and nDCG Summarize the discount or accumulative gain and also precision and recall are the two basic measures. And we then talked about feedback techniques. And we talked about the Rocchio in the Vector Space \u001b[36;40mModel\u001b[m and the mixture \u001b[36;40mmodel\u001b[m and the language modeling approach. Feedback is a very important technique especially considering the opportunity of learning from a lot of pixels \u001b[36;40mon\u001b[m the Web. We then talked about Web search. And here we talked about how to use parallel in that scene to solve the scalability issue in that scene we're going to use the net reduce. Then we talked about how to use linking permission \u001b[36;40mmodel\u001b[m app to improve search. We talked about page rank and hits as the major hours is to analyzing links \u001b[36;40mon\u001b[m the Web. We then talked about learning through rank. This is the use of machine learning to combine multiple features for improvement scoring. Not only that the effectiveness can be improved in using this approach, but we can also improve the robustness of the. The ranking function so that it's not easy to expand a search engine. It just some features to promote the page. And finally we talked about the future of Web search. About the some major reactions that we might to see in the future in improving the count of regeneration of such engines. And then finally we talked about the recommended systems and, these are systems to increment the push mode. And we'll talk about the two approaches, one is content-based, one is collaborative filtering and they can be combined together. Now, an obvious missing piece in this picture is the user, so user interface is also an important component in any search engine. Even though the current search interface is relatively simple they actually have done a lot of studies of user interfaces where we do visualization for example. And this is the topic to that, you can learn more by reading this book. It's an excellent book about all kinds of studies of search using the face. If you want to know more about the topics that we talked about, you can also read some additional readings that are listed here. In this short course we only manage to cover some basic topics in text retrievals and search engines. And these resources provide additional information about more advanced topics and they give a more thorough treatment of some of the topics that we talked about. And a main source is the Synthesis Digital Library that you can see a lot of short to textbook or textbooks, or long tutorials. They tend to provide a lot of information to explain a topic. And there a lot of series that are related to this cause. One is information concepts, \u001b[36;40mretrieval\u001b[m, and services. One is human langauge technology. And yet another is artificial intelligence and machine learning. There are also some major journals and conferences listed here that tend to have a lot of research papers we need to and topic of this course. And finally, for more information about resources Including readings, tool kits, etc you can check out his URL. So, if you have not taken the text mining course in this data mining specialization series then naturally the next step is to take that course. As this picture shows, to mine big text data, we generally need two kinds of techniques. One is text \u001b[36;40mretrieval\u001b[m, which is covered in this course. And these techniques will help us convert raw big text data into small relevant text data, which are actually needed in the specific application. Now human plays important role in mining any text data because text data is written for humans to consume. So involving humans in the process of data mining is very important and in this course we have covered the various strategies to help users get access to the most relevant data. These techniques are always so essential in any text mining system to help provide prominence and to help users interpret the inner patterns that the user will define through text data mining. So, in general, the user would have to go back to the original data to better understand the patterns. So the text mining cause, or rather, text mining and analytics course will be dealing with what to do once the user has a following information. So this is a second step in this picture where we would convert the text data into actionable knowledge. And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge. In text and such knowledge can then be used in application systems to help decision making or to help a user finish a task. So, if you have not taken that course, the natural step and that natural next step would be to take that course. Thank you for taking this course. I hope you had fun and found this course to be useful to you. And I look forward to interacting with you at a future opportunity.  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for num, (d_id, _) in enumerate(top_docs):\n",
    "    content = idx.metadata(d_id).get('content')\n",
    "    if content is not None:\n",
    "        print_with_highlight(content, query_text, d_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test parameters and gather MAP results in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Output Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 5 Results of query, grab string around first matching key word?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
